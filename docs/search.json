[
  {
    "objectID": "SAT Segmentation_Oh.html",
    "href": "SAT Segmentation_Oh.html",
    "title": "",
    "section": "",
    "text": "import os\nimport  cv2\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom patchify import patchify\nfrom PIL import Image\nimport segmentation_models as sm\nfrom tensorflow.keras.metrics import MeanIoU\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nscaler = MinMaxScaler()\n\nroot_directory = 'Semantic segmentation dataset/'\n\npatch_size = 256\n\nModuleNotFoundError: No module named 'cv2'\n\n\n\nimport opendatasets as od\nimport pandas\n\n\nod.download(\"https://www.kaggle.com/datasets/balraj98/massachusetts-roads-dataset\")\n\nModuleNotFoundError: No module named 'opendatasets'\n\n\n\ndef patches\n    image_dataset = []  \n    for path, subdirs, files in os.walk(root_directory):\n        #print(path)  \n        dirname = path.split(os.path.sep)[-1]\n        if dirname == 'images':   #Find all 'images' directories\n            images = os.listdir(path)  #List of all image names in this subdirectory\n            for i, image_name in enumerate(images):  \n                if image_name.endswith(\".jpg\"):   #Only read jpg images...\n\n                    image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n                    SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n                    SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n                    image = Image.fromarray(image)\n                    image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n                    #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n                    image = np.array(image)             \n\n                    #Extract patches from each image\n                    print(\"Now patchifying image:\", path+\"/\"+image_name)\n                    patches_img = patchify(image, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n\n                    for i in range(patches_img.shape[0]):\n                        for j in range(patches_img.shape[1]):\n\n                            single_patch_img = patches_img[i,j,:,:]\n\n                            #Use minmaxscaler instead of just dividing by 255. \n                            single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape)\n\n                            #single_patch_img = (single_patch_img.astype('float32')) / 255. \n                            single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.                               \n                            image_dataset.append(single_patch_img)"
  },
  {
    "objectID": "segmentation_tutorial/segmentation.html",
    "href": "segmentation_tutorial/segmentation.html",
    "title": "",
    "section": "",
    "text": "From https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/?utm_source=blog&utm_medium=computer-vision-implementing-mask-r-cnn-image-segmentation#2"
  },
  {
    "objectID": "segmentation_tutorial/segmentation.html#threshold-segmentation",
    "href": "segmentation_tutorial/segmentation.html#threshold-segmentation",
    "title": "",
    "section": "Threshold Segmentation",
    "text": "Threshold Segmentation\nMost basic form of segmentation: threshold segmentation\nIf we have two classes, we define a single global threshold. If we have multiple classes, we define several local thresholds.\n\n# Import packages\nfrom skimage.color import rgb2gray\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy import ndimage\n\n\n# Read and display image\nimage = plt.imread('1.jpeg')\nplt.imshow(image);\nprint(image.shape)\n\n(192, 263, 3)\n\n\n\n\n\n\n\n\n\n\n# Convert to grayscale to illustrate thresholding\ngray = rgb2gray(image)\n# cv2.imshow('img',gray)\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()\nplt.imshow(gray, cmap = \"gray\");\n\n\n\n\n\n\n\n\n\n# Show shape\ngray.shape\n\n(192, 263)\n\n\n\n# Threshold using mean value\n    # no need for a loop...\ngray_r = gray.reshape(gray.shape[0]*gray.shape[1])\nfor i in range(gray_r.shape[0]):\n    if gray_r[i] &gt; gray_r.mean():\n        gray_r[i] = 1\n    else:\n        gray_r[i] = 0\ngray = gray_r.reshape(gray.shape[0],gray.shape[1])\nplt.imshow(gray, cmap='gray')\n\n\n\n\n\n\n\n\n\n# Threshold with several values\n    # no need for a loop...\ngray = rgb2gray(image)\ngray_r = gray.reshape(gray.shape[0]*gray.shape[1])\nfor i in range(gray_r.shape[0]):\n    if gray_r[i] &gt; gray_r.mean():\n        gray_r[i] = 3\n    elif gray_r[i] &gt; 0.5:\n        gray_r[i] = 2\n    elif gray_r[i] &gt; 0.25:\n        gray_r[i] = 1\n    else:\n        gray_r[i] = 0\ngray = gray_r.reshape(gray.shape[0],gray.shape[1])\nplt.imshow(gray, cmap='gray')\n\n\n\n\n\n\n\n\nAdvantages: - Easy to implement - Runs fast - Works well if contrast is high\nLimitations: - Only uses grayscale - Performs bad if low grayscale contrast or grayscale value overlap"
  },
  {
    "objectID": "segmentation_tutorial/segmentation.html#edge-detection-segmentation",
    "href": "segmentation_tutorial/segmentation.html#edge-detection-segmentation",
    "title": "",
    "section": "Edge detection segmentation",
    "text": "Edge detection segmentation\nFirst convolve the image with a kernel that detects edges. Then threshold (presumably).\n\n# Read and display image\nimage = plt.imread('index.png')\nplt.imshow(image);\n\n\n\n\n\n\n\n\n\n# Convert to grayscale\ngray = rgb2gray(image[:,:,:3])\n\n# Defining the sobel filters (specific kernels for extracting edges)\nsobel_horizontal = np.array([[1, 2, 1],[0, 0, 0],[-1, -2, -1]])\nprint(sobel_horizontal, 'is a kernel for detecting horizontal edges')\n \nsobel_vertical = np.array([[-1, 0, 1],[-2, 0, 2],[-1, 0, 1]])\nprint(sobel_vertical, 'is a kernel for detecting vertical edges')\n\n[[ 1  2  1]\n [ 0  0  0]\n [-1 -2 -1]] is a kernel for detecting horizontal edges\n[[-1  0  1]\n [-2  0  2]\n [-1  0  1]] is a kernel for detecting vertical edges\n\n\n\n# Convolve\nout_h = ndimage.convolve(gray, sobel_horizontal, mode='reflect')\nout_v = ndimage.convolve(gray, sobel_vertical, mode='reflect')\n# here mode determines how the input array is extended when the filter overlaps a border.\n\n# Show results\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].imshow(out_h, cmap='gray')\nax[1].imshow(out_v, cmap='gray');\n\n\n\n\n\n\n\n\n\n# The laplacer operator/kernel can detect both horizontal and vertical edges\nkernel_laplace = np.array([np.array([1, 1, 1]), np.array([1, -8, 1]), np.array([1, 1, 1])])\nprint(kernel_laplace, 'is a laplacian kernel')\n\n[[ 1  1  1]\n [ 1 -8  1]\n [ 1  1  1]] is a laplacian kernel\n\n\n\n# Convolve, show results\nout_l = ndimage.convolve(gray, kernel_laplace, mode='reflect')\nplt.imshow(out_l, cmap='gray');"
  },
  {
    "objectID": "segmentation_tutorial/segmentation.html#clustering-segmentation",
    "href": "segmentation_tutorial/segmentation.html#clustering-segmentation",
    "title": "",
    "section": "Clustering segmentation",
    "text": "Clustering segmentation\nLet’s try segmentation with k-means clustering\n\n# Read and display\npic = plt.imread('1.jpeg')/255  # dividing by 255 to bring the pixel values between 0 and 1\nprint(pic.shape)\nplt.imshow(pic);\n\n(192, 263, 3)\n\n\n\n\n\n\n\n\n\n\n# Reshape to have row for each pixel, column for each channel (we will not be doing anything spatial)\npic_n = pic.reshape(pic.shape[0]*pic.shape[1], pic.shape[2])\npic_n.shape\n\n(50496, 3)\n\n\n\n# Apply k-means clustering\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=5, random_state=0).fit(pic_n) \npic2show = kmeans.cluster_centers_[kmeans.labels_] # returns clusters centers\n\n\n# Display results\ncluster_pic = pic2show.reshape(pic.shape[0], pic.shape[1], pic.shape[2])\nplt.imshow(cluster_pic);\n\n\n\n\n\n\n\n\nK-means often performs well on small datasets!\nBut not very well on large numbers of images."
  },
  {
    "objectID": "segmentation_tutorial/segmentation.html#mask-r-cnn-segmentation",
    "href": "segmentation_tutorial/segmentation.html#mask-r-cnn-segmentation",
    "title": "",
    "section": "Mask R-CNN segmentation",
    "text": "Mask R-CNN segmentation\nThis is an extension of the Faster R-CNN method, which identifies the class and a bounding box for each object. The benefit of Mask R-CNN is that it adds a pixel-wise mask as well.\nThe first tutorial briefly discusses how this works, but does not include Python code for doing it ourselves.\nI think the implementation is included in the next tutorial! https://www.analyticsvidhya.com/blog/2019/07/computer-vision-implementing-mask-r-cnn-image-segmentation/\nLet’s work through the second tutorial now.\nIncludes a helpful graphic for distinguishing between Faster R-CNN and Mask R-CNN.\nBriefly explains how Faster R-CNN works (I didn’t understand this).\nExplains more in-depth how Mask R-CNN works (I also didn’t understand this).\nTraining time for CNNs is quite high. It took the author of the tutorial 1-2 days to train their model. Hence we use a pre-trained model below.\n\n# Clone repository with the architecture for RCNN\n!git clone https://github.com/matterport/Mask_RCNN.git\n\nCloning into 'Mask_RCNN'...\nremote: Enumerating objects: 956, done.\nremote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\nReceiving objects: 100% (956/956), 137.67 MiB | 16.08 MiB/s, done.\nResolving deltas: 100% (558/558), done.\n\n\n\n# Install required packages\n!pip install cython tensorflow keras opencv-python h5py imgaug\n\nCollecting cython\n  Downloading Cython-3.0.10-cp312-cp312-macosx_10_9_x86_64.whl.metadata (3.2 kB)\nDownloading Cython-3.0.10-cp312-cp312-macosx_10_9_x86_64.whl (3.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 12.9 MB/s eta 0:00:0000:0100:01\nInstalling collected packages: cython\nSuccessfully installed cython-3.0.10\n\n\nDownload pre-trained weights: https://github.com/matterport/Mask_RCNN/releases\nI went with the 2.0 version, downloading just mask_rcnn_coco.h5.\nThen place the download in the samples folder of the cloned GitHub repo.\nNow look in the samples folder for part 2.\nLook at level sets and active contours?"
  },
  {
    "objectID": "code/k_means.html",
    "href": "code/k_means.html",
    "title": "Testing K-means Clustering",
    "section": "",
    "text": "Imports\n\nimport skimage.io as skio\nimport skimage.util as sku\nimport skimage.color as skol\nfrom skimage import filters, feature, transform\nimport skimage.morphology as skimor\nimport skimage.draw as draw\nfrom scipy.signal import convolve2d\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d\nimport numpy as np\nimport cv2\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.ndimage import gaussian_filter, gaussian_laplace\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import datasets\nimport tensorflow.keras.backend as K\n\n\n\n\n\nimage_filepath = \"../../data/massRoads/tiff\"\n# /train/... for images and /train_labels/... for labels\n\n\nimage = skio.imread(image_filepath + \"/train/10528735_15.tiff\")\nimage_label = skio.imread(image_filepath + \"/train_labels/10528735_15.tif\")\n\n\nfig, axes = plt.subplots(1,2, figsize = (10,15))\n\naxes[0].imshow(image)\naxes[1].imshow(image_label, cmap='gray')\n\nplt.show()\n\n\n\n\n\n\n\n\nCropping Image for testing\n\nx1, x2 = 450, 550\ny1, y2 = 675, 750\nimage = image[y1:y2, x1:x2, :]\nimage_label = image_label[y1:y2, x1:x2]\n\n\nfig, axes = plt.subplots(1,2, figsize = (10,15))\n\naxes[0].imshow(image)\naxes[1].imshow(image_label, cmap='gray')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = np.sum(y_true * y_pred)\n    return (2. * intersection + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)\n\n\n# Function to print several accuracy metrics\ndef accuracy_metrics(y_true, y_pred):\n    # Create confusion matrix\n    C = confusion_matrix(y_true, y_pred, labels=(True, False))\n\n    # Overall accuracy rate\n    acc = (C[0,0] + C[1,1])/C.sum()\n\n    # Recall\n    recall = (C[0,0])/(C[0,0] + C[1,0])\n    \n    # Precision\n    prec = (C[0,0])/(C[0,0] + C[0,1])\n\n    dice = dice_coef(y_true=y_true, y_pred=y_pred)\n\n    # Print results\n    print(\"Confusion matrix:\\n\", C)\n    print(\"Overall accuracy:\", np.round(acc, 3), \"\\nPrecision:\", np.round(recall, 3), \"\\nRecall\", np.round(prec, 3), \"\\nDice Coefficient\", np.round(dice, 3)) \n\n\n#what values are used in label image\nnp.unique(image_label)\n\narray([  0, 255], dtype=uint8)\n\n\n\n\n\n\npixel_vals = image.reshape((-1,3))\npixel_vals = np.float32(pixel_vals)\n\n\n\n\n\nsk_kmeans = KMeans(n_clusters=3, verbose=1).fit(pixel_vals)\n\nInitialization complete\nIteration 0, inertia 9151501.0.\nIteration 1, inertia 6988855.0.\nIteration 2, inertia 6884783.0.\nIteration 3, inertia 6869388.5.\nIteration 4, inertia 6865800.5.\nIteration 5, inertia 6864909.0.\nConverged at iteration 5: center shift 0.06919966638088226 within tolerance 0.18099222412109375.\n\n\n\nsk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n\nskio.imshow(sk_segmented_image_pca, cmap='gray')\n\n/Users/aidenpape/miniconda3/lib/python3.11/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Low image data range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s try to put a horizontal and vertically run edge detection using Sobel kernels\n\nimage_gray = skol.rgb2gray(image)\n\n# Define Sobel kernels\nsobel_horizontal = np.array([[-1, -2, -1],\n                             [ 0,  0,  0],\n                             [ 1,  2,  1]])\n\nsobel_vertical = np.array([[-1, 0, 1],\n                           [-2, 0, 2],\n                           [-1, 0, 1]])\n\n# Perform convolutions\nedges_horizontal = convolve2d(image_gray, sobel_horizontal, mode='same', boundary='symm')\nedges_vertical = convolve2d(image_gray, sobel_vertical, mode='same', boundary='symm')\n\n# Calculate the magnitude of the gradient\nedges = np.sqrt(np.square(edges_horizontal) + np.square(edges_vertical))\n\n\nskio.imshow(edges, cmap='gray')\n\n/Users/aidenpape/miniconda3/lib/python3.11/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Float image out of standard range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Calculation Canny gradient\ncanny_edges = feature.canny(image_gray, sigma=3)\n\n\n\n\n# Create disk\ndisk = skimor.disk(1)\n\n# Area closing\nclosed_edges = skimor.dilation(canny_edges, footprint = disk)\nclosed_edges = closed_edges * 255\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\nax = axes.ravel()\n\nax[0].imshow(canny_edges, cmap='gray')\nax[0].set_title('Canny Edges')\n\nax[1].imshow(closed_edges, cmap='gray')\nax[1].set_title('Closed Canny edges')\n\nax[2].imshow(image)\nax[2].set_title('Original Image')\n\nfor a in ax:\n    a.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNow lets add this new channel to the image and see what happens\n\nimage_with_canny = np.dstack((image, closed_edges.astype(np.uint8) * 255))\n\n\nfeature_vals = image_with_canny.reshape((-1,4))\nfeature_vals = np.float32(feature_vals)\n\n\nsk_kmeans = KMeans(n_clusters=3, verbose=1).fit(feature_vals)\n\nInitialization complete\nIteration 0, inertia 7957098.0.\nIteration 1, inertia 6929594.5.\nIteration 2, inertia 6871233.0.\nIteration 3, inertia 6866522.5.\nIteration 4, inertia 6865844.0.\nConverged at iteration 4: center shift 0.07451267540454865 within tolerance 0.13574781494140625.\n\n\n\nsk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n\nskio.imshow(sk_segmented_image_pca, cmap='gray')\n\n/Users/aidenpape/miniconda3/lib/python3.11/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Low image data range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)\n\n\n\n\n\n\n\n\n\n\nnp.unique(sk_segmented_image_pca)\n\narray([0, 1, 2], dtype=int32)\n\n\n\n# Generating figure 2\nfig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n\ncluster_one = (sk_segmented_image_pca==0)\naxes[0][0].imshow(cluster_one, cmap='gray')\naxes[0][0].set_title('Cluster One')\n\ncluster_two = (sk_segmented_image_pca==1)\naxes[0][1].imshow(cluster_two, cmap='gray')\naxes[0][1].set_title('Cluster Two')\n\ncluster_three = (sk_segmented_image_pca==2)\naxes[1][0].imshow(cluster_three, cmap='gray')\naxes[1][0].set_title('Cluster Three')\n\naxes[1][1].imshow(image)\naxes[1][1].set_title('Original Image')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nlines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\nax = axes.ravel()\n\nax[0].imshow(image_gray, cmap='gray')\nax[0].set_title('Input image')\n\nax[1].imshow(canny_edges, cmap='gray')\nax[1].set_title('Canny edges')\n\nax[2].imshow(canny_edges * 0)\nfor line in lines:\n    p0, p1 = line\n    ax[2].plot((p0[0], p1[0]), (p0[1], p1[1]))\nax[2].set_xlim((0, image_gray.shape[1]))\nax[2].set_ylim((image_gray.shape[0], 0))\nax[2].set_title('Probabilistic Hough')\n\nfor a in ax:\n    a.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create a blank canvas to draw lines on\nhough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n\n# Draw the detected lines on the canvas\nfor line in lines:\n    p0, p1 = line\n    # Draw line segment\n    rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n    hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n\n# # Display the canvas with the detected lines\n# plt.imshow(hough_lines, cmap='gray')\n# plt.title('Image with Detected Lines')\n# plt.axis('off')\n# plt.show()\n\n\n\n\n\n\ndef classify_gray(image):\n\n    # Convert the image to Lab color space\n\n    # Compute the standard deviation of the r, g, and b channels\n    # std_dev = np.std(image[:,:,0], image[:,:,1], image[:,:,2])\n    std_dev = np.std(image, axis = 2)\n\n    # Define a threshold for classifying gray pixels\n    diff_threshold = 6 # Adjust as needed\n\n    # Classify pixels as gray or not gray based on the standard deviation\n    gray_mask = std_dev &lt; diff_threshold\n        \n    return gray_mask\n\n\ngray_mask = classify_gray(image)\ngray_mask = gray_mask.reshape((image.shape[0], image.shape[1]))\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 2, figsize=(15, 10), sharex=True, sharey=True)\n\naxes[0].imshow(gray_mask, cmap='gray')\naxes[0].set_title('Gray Mask')\n\naxes[1].imshow(image)\naxes[1].set_title('Original Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# Create additional features\n\n# Range of values (gray pixels will have low range)\nr = image.max(axis = 2) - image.min(axis = 2)\n\n# Canny edge detection\ncanny_edges_r = feature.canny(image[:,:,0], sigma=4);\ncanny_edges_g = feature.canny(image[:,:,1], sigma=4);\ncanny_edges_b = feature.canny(image[:,:,2], sigma=4);\n\n# Gaussian blur sigma = 1\ngaus_r_1 = gaussian_filter(image[:,:,0], sigma = 1)\ngaus_g_1 = gaussian_filter(image[:,:,1], sigma = 1)\ngaus_b_1 = gaussian_filter(image[:,:,2], sigma = 1)\n\n# Gaussian blur sigma = 3\ngaus_r_3 = gaussian_filter(image[:,:,0], sigma = 3)\ngaus_g_3 = gaussian_filter(image[:,:,1], sigma = 3)\ngaus_b_3 = gaussian_filter(image[:,:,2], sigma = 3)\n\n# Gaussian blur sigma = 5\ngaus_r_5 = gaussian_filter(image[:,:,0], sigma = 5)\ngaus_g_5 = gaussian_filter(image[:,:,1], sigma = 5)\ngaus_b_5 = gaussian_filter(image[:,:,2], sigma = 5)\n\n# LoG blur sigma = .5\nlog_r_5 = gaussian_laplace(image[:,:,0], sigma = .5)\nlog_g_5 = gaussian_laplace(image[:,:,1], sigma = .5)\nlog_b_5 = gaussian_laplace(image[:,:,2], sigma = .5)\n\n# LoG blur sigma = .6\nlog_r_6 = gaussian_laplace(image[:,:,0], sigma = .6)\nlog_g_6 = gaussian_laplace(image[:,:,1], sigma = .6)\nlog_b_6 = gaussian_laplace(image[:,:,2], sigma = .6)\n\n# LoG blur sigma = .8\nlog_r_8 = gaussian_laplace(image[:,:,0], sigma = .8)\nlog_g_8 = gaussian_laplace(image[:,:,1], sigma = .8)\nlog_b_8 = gaussian_laplace(image[:,:,2], sigma = .8)\n\n# Add layers to model\nimage_layers = np.dstack([image, r, canny_edges_r, canny_edges_g, canny_edges_b,\n                             gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                             gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                             log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8, hough_lines, gray_mask])\n\n\n\n\n\n\nimage_layers.shape\n\n(75, 100, 27)\n\n\n\nimage_layers = image_layers.reshape(image_layers.shape[0] * image_layers.shape[1], image_layers.shape[2])\nimage_layers.shape\n\n(7500, 27)\n\n\n\n# Standardize the features\nscaler = StandardScaler()\nimage_layers_scaled = scaler.fit_transform(image_layers)\n\n\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=5)\nlayers_pca = pca.fit_transform(image_layers_scaled)\n\n# Explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Plotting the explained variance ratio\nplt.figure(figsize=(8, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\n\nlayers_pca.shape\n\n(7500, 5)\n\n\nTransform to create new features in less dimensions\n\nnew_features = pca.transform(image_layers_scaled)\n\n\nnew_features.shape\n\n(7500, 5)\n\n\n\n\n\nsk_kmeans_pca = KMeans(n_clusters=3, verbose=1).fit(new_features)\n\nInitialization complete\nIteration 0, inertia 123716.19268453149.\nIteration 1, inertia 81206.56131639544.\nIteration 2, inertia 80379.63761496074.\nIteration 3, inertia 80248.56084677183.\nIteration 4, inertia 80221.05356165496.\nIteration 5, inertia 80216.4490686662.\nConverged at iteration 5: center shift 0.00034726557264135494 within tolerance 0.00039411965957254487.\n\n\n\nsk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n\n# Generating figure 2\nfig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n\ncluster_one = (sk_segmented_image_pca==0)\naxes[0][0].imshow(cluster_one, cmap='gray')\naxes[0][0].set_title('Cluster One')\n\ncluster_two = (sk_segmented_image_pca==1)\naxes[0][1].imshow(cluster_two, cmap='gray')\naxes[0][1].set_title('Cluster Two')\n\ncluster_three = (sk_segmented_image_pca==2)\naxes[1][0].imshow(cluster_three, cmap='gray')\naxes[1][0].set_title('Cluster Three')\n\naxes[1][1].imshow(image)\naxes[1][1].set_title('Original Image')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhich cluster best captures the roads? Let’s find out with the results\n\nimage_label_bool = (image_label==255)\n\n\ncluster_one.dtype\n\ndtype('bool')\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_one.ravel())\n\nConfusion matrix:\n [[1078  499]\n [2255 3668]]\nOverall accuracy: 0.633 \nPrecision: 0.323 \nRecall 0.684 \nDice Coefficient 0.439\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_two.ravel())\n\nConfusion matrix:\n [[ 109 1468]\n [2356 3567]]\nOverall accuracy: 0.49 \nPrecision: 0.044 \nRecall 0.069 \nDice Coefficient 0.054\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_three.ravel())\n\nConfusion matrix:\n [[ 390 1187]\n [1312 4611]]\nOverall accuracy: 0.667 \nPrecision: 0.229 \nRecall 0.247 \nDice Coefficient 0.238\n\n\n\ndef identify_road_cluster(clustered_image, image_label):\n\n    cluster_labels = np.unique(clustered_image)\n\n    best_precision = 0\n    best_cluster = -1\n\n    for i in cluster_labels:\n        cluster = (sk_segmented_image_pca==i)\n        C = confusion_matrix(image_label.ravel(), cluster.ravel(), labels=(True, False))\n\n        # # Overall accuracy rate\n        # acc = (C[0,0] + C[1,1])/C.sum()\n        # # Recall\n        # recall = (C[0,0])/(C[0,0] + C[1,0])\n        # Precision\n        prec = (C[0,0])/(C[0,0] + C[0,1])\n\n        if prec &gt; best_precision:\n            best_precision = prec\n            best_cluster = i\n\n    return best_cluster\n\n\ncluster = identify_road_cluster(sk_segmented_image_pca, image_label_bool)\n\n\nimage_label\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)\n\n\n\nfig = plt.figure(1, figsize=(15, 15))\nplt.clf()\n\nax = fig.add_subplot(111, projection=\"3d\", elev=45, azim=90)\nax.set_position([0, 0, 0.95, 1])\n\nX = new_features\ny = image_label.ravel()\n\nfor name, label in [(\"Background\", 0), (\"Road\", 255)]:\n    ax.text3D(\n        X[y == label, 0].mean(),\n        X[y == label, 1].mean() + 1.5,\n        X[y == label, 2].mean(),\n        name,\n        horizontalalignment=\"center\",\n        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n    )\n# Reorder the labels to have colors matching the cluster results\n# y = np.choose(y, [0, 255]).astype(float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='Accent', edgecolor=\"k\")\n\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndef create_features(image):\n\n    # Calculation Canny gradient\n    image_gray = skol.rgb2gray(image)\n    canny_edges = feature.canny(image_gray, sigma=3)\n\n    # Create disk\n    disk = skimor.disk(1)\n\n    # Area closing for hough lines\n    closed_edges = skimor.dilation(canny_edges, footprint = disk)\n    closed_edges = closed_edges * 255\n\n    lines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n    hough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n\n    # Draw the detected lines on the canvas\n    for line in lines:\n        p0, p1 = line\n        # Draw line segment\n        rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n        hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n\n    #create gray mask\n    gray_mask = classify_gray(image)\n    gray_mask = gray_mask.reshape((image.shape[0], image.shape[1]))\n\n    # Create additional features\n\n    # Range of values (gray pixels will have low range)\n    r = image.max(axis = 2) - image.min(axis = 2)\n\n    # Canny edge detection\n    canny_edges_r = feature.canny(image[:,:,0], sigma=4);\n    canny_edges_g = feature.canny(image[:,:,1], sigma=4);\n    canny_edges_b = feature.canny(image[:,:,2], sigma=4);\n\n    # Gaussian blur sigma = 1\n    gaus_r_1 = gaussian_filter(image[:,:,0], sigma = 1)\n    gaus_g_1 = gaussian_filter(image[:,:,1], sigma = 1)\n    gaus_b_1 = gaussian_filter(image[:,:,2], sigma = 1)\n\n    # Gaussian blur sigma = 3\n    gaus_r_3 = gaussian_filter(image[:,:,0], sigma = 3)\n    gaus_g_3 = gaussian_filter(image[:,:,1], sigma = 3)\n    gaus_b_3 = gaussian_filter(image[:,:,2], sigma = 3)\n\n    # Gaussian blur sigma = 5\n    gaus_r_5 = gaussian_filter(image[:,:,0], sigma = 5)\n    gaus_g_5 = gaussian_filter(image[:,:,1], sigma = 5)\n    gaus_b_5 = gaussian_filter(image[:,:,2], sigma = 5)\n\n    # LoG blur sigma = .5\n    log_r_5 = gaussian_laplace(image[:,:,0], sigma = .5)\n    log_g_5 = gaussian_laplace(image[:,:,1], sigma = .5)\n    log_b_5 = gaussian_laplace(image[:,:,2], sigma = .5)\n\n    # LoG blur sigma = .6\n    log_r_6 = gaussian_laplace(image[:,:,0], sigma = .6)\n    log_g_6 = gaussian_laplace(image[:,:,1], sigma = .6)\n    log_b_6 = gaussian_laplace(image[:,:,2], sigma = .6)\n\n    # LoG blur sigma = .8\n    log_r_8 = gaussian_laplace(image[:,:,0], sigma = .8)\n    log_g_8 = gaussian_laplace(image[:,:,1], sigma = .8)\n    log_b_8 = gaussian_laplace(image[:,:,2], sigma = .8)\n\n    # Add layers to model\n    image_layers = np.dstack([image, r, canny_edges_r, canny_edges_g, canny_edges_b,\n                                gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                                gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                                log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8, hough_lines, gray_mask])\n    \n    image_layers = image_layers.reshape(image_layers.shape[0] * image_layers.shape[1], image_layers.shape[2])\n    \n    scaler = StandardScaler()\n    image_layers_scaled = scaler.fit_transform(image_layers)\n\n    print(image_layers.shape)\n    \n    return image_layers\n\n\ndef get_pca(image_layers, n_components):\n\n    # Initialize PCA and fit the scaled data\n    pca = PCA(n_components=n_components)\n    layers_pca = pca.fit_transform(image_layers)\n\n    # Explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    new_features = pca.transform(image_layers)\n\n    return new_features\n\n\ndef run_k_means(image_layers, n_clusters):\n\n    sk_kmeans = KMeans(n_clusters=n_clusters, verbose=1).fit(image_layers)\n\n    sk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n    best_cluster = identify_road_cluster(sk_segmented_image_pca, image_label)\n\n    segmented_roads = (sk_segmented_image_pca==best_cluster)\n\n    return segmented_roads\n\n\ndef full_pipeline(image, image_label, n_clusters=3, with_pca=False, pca_n_components=5):\n    \n    print(\"Creating features...\")\n    image_layers = create_features(image)\n    if(with_pca):\n        print(\"Reducing dimensions...\")\n        image_layers = get_pca(image_layers=image_layers, n_components=pca_n_components)\n    \n    print(\"Segmenting Roads...\")\n    segmented_roads = run_k_means(image_layers=image_layers, n_clusters=n_clusters)\n    print(\"Segmentation Complete\")\n    \n    plt.imshow(segmented_roads, cmap='gray')\n    plt.show()\n\n    print(\"Analyzing metrics...\")\n    accuracy_metrics(segmented_roads, image_label)\n\n\nimage.shape\n\n(75, 100, 3)\n\n\n\n# full_pipeline(image, image_label_bool)"
  },
  {
    "objectID": "code/k_means.html#lets-use-canny-edge-detection-instead",
    "href": "code/k_means.html#lets-use-canny-edge-detection-instead",
    "title": "Testing K-means Clustering",
    "section": "",
    "text": "# Calculation Canny gradient\ncanny_edges = feature.canny(image_gray, sigma=3)\n\n\n\n\n# Create disk\ndisk = skimor.disk(1)\n\n# Area closing\nclosed_edges = skimor.dilation(canny_edges, footprint = disk)\nclosed_edges = closed_edges * 255\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\nax = axes.ravel()\n\nax[0].imshow(canny_edges, cmap='gray')\nax[0].set_title('Canny Edges')\n\nax[1].imshow(closed_edges, cmap='gray')\nax[1].set_title('Closed Canny edges')\n\nax[2].imshow(image)\nax[2].set_title('Original Image')\n\nfor a in ax:\n    a.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNow lets add this new channel to the image and see what happens\n\nimage_with_canny = np.dstack((image, closed_edges.astype(np.uint8) * 255))\n\n\nfeature_vals = image_with_canny.reshape((-1,4))\nfeature_vals = np.float32(feature_vals)\n\n\nsk_kmeans = KMeans(n_clusters=3, verbose=1).fit(feature_vals)\n\nInitialization complete\nIteration 0, inertia 7957098.0.\nIteration 1, inertia 6929594.5.\nIteration 2, inertia 6871233.0.\nIteration 3, inertia 6866522.5.\nIteration 4, inertia 6865844.0.\nConverged at iteration 4: center shift 0.07451267540454865 within tolerance 0.13574781494140625.\n\n\n\nsk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n\nskio.imshow(sk_segmented_image_pca, cmap='gray')\n\n/Users/aidenpape/miniconda3/lib/python3.11/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Low image data range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)\n\n\n\n\n\n\n\n\n\n\nnp.unique(sk_segmented_image_pca)\n\narray([0, 1, 2], dtype=int32)\n\n\n\n# Generating figure 2\nfig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n\ncluster_one = (sk_segmented_image_pca==0)\naxes[0][0].imshow(cluster_one, cmap='gray')\naxes[0][0].set_title('Cluster One')\n\ncluster_two = (sk_segmented_image_pca==1)\naxes[0][1].imshow(cluster_two, cmap='gray')\naxes[0][1].set_title('Cluster Two')\n\ncluster_three = (sk_segmented_image_pca==2)\naxes[1][0].imshow(cluster_three, cmap='gray')\naxes[1][0].set_title('Cluster Three')\n\naxes[1][1].imshow(image)\naxes[1][1].set_title('Original Image')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nlines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\nax = axes.ravel()\n\nax[0].imshow(image_gray, cmap='gray')\nax[0].set_title('Input image')\n\nax[1].imshow(canny_edges, cmap='gray')\nax[1].set_title('Canny edges')\n\nax[2].imshow(canny_edges * 0)\nfor line in lines:\n    p0, p1 = line\n    ax[2].plot((p0[0], p1[0]), (p0[1], p1[1]))\nax[2].set_xlim((0, image_gray.shape[1]))\nax[2].set_ylim((image_gray.shape[0], 0))\nax[2].set_title('Probabilistic Hough')\n\nfor a in ax:\n    a.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create a blank canvas to draw lines on\nhough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n\n# Draw the detected lines on the canvas\nfor line in lines:\n    p0, p1 = line\n    # Draw line segment\n    rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n    hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n\n# # Display the canvas with the detected lines\n# plt.imshow(hough_lines, cmap='gray')\n# plt.title('Image with Detected Lines')\n# plt.axis('off')\n# plt.show()"
  },
  {
    "objectID": "code/k_means.html#lets-try-to-classify-gray-pixels-these-may-help-identify-the-roads",
    "href": "code/k_means.html#lets-try-to-classify-gray-pixels-these-may-help-identify-the-roads",
    "title": "Testing K-means Clustering",
    "section": "",
    "text": "def classify_gray(image):\n\n    # Convert the image to Lab color space\n\n    # Compute the standard deviation of the r, g, and b channels\n    # std_dev = np.std(image[:,:,0], image[:,:,1], image[:,:,2])\n    std_dev = np.std(image, axis = 2)\n\n    # Define a threshold for classifying gray pixels\n    diff_threshold = 6 # Adjust as needed\n\n    # Classify pixels as gray or not gray based on the standard deviation\n    gray_mask = std_dev &lt; diff_threshold\n        \n    return gray_mask\n\n\ngray_mask = classify_gray(image)\ngray_mask = gray_mask.reshape((image.shape[0], image.shape[1]))\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 2, figsize=(15, 10), sharex=True, sharey=True)\n\naxes[0].imshow(gray_mask, cmap='gray')\naxes[0].set_title('Gray Mask')\n\naxes[1].imshow(image)\naxes[1].set_title('Original Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# Create additional features\n\n# Range of values (gray pixels will have low range)\nr = image.max(axis = 2) - image.min(axis = 2)\n\n# Canny edge detection\ncanny_edges_r = feature.canny(image[:,:,0], sigma=4);\ncanny_edges_g = feature.canny(image[:,:,1], sigma=4);\ncanny_edges_b = feature.canny(image[:,:,2], sigma=4);\n\n# Gaussian blur sigma = 1\ngaus_r_1 = gaussian_filter(image[:,:,0], sigma = 1)\ngaus_g_1 = gaussian_filter(image[:,:,1], sigma = 1)\ngaus_b_1 = gaussian_filter(image[:,:,2], sigma = 1)\n\n# Gaussian blur sigma = 3\ngaus_r_3 = gaussian_filter(image[:,:,0], sigma = 3)\ngaus_g_3 = gaussian_filter(image[:,:,1], sigma = 3)\ngaus_b_3 = gaussian_filter(image[:,:,2], sigma = 3)\n\n# Gaussian blur sigma = 5\ngaus_r_5 = gaussian_filter(image[:,:,0], sigma = 5)\ngaus_g_5 = gaussian_filter(image[:,:,1], sigma = 5)\ngaus_b_5 = gaussian_filter(image[:,:,2], sigma = 5)\n\n# LoG blur sigma = .5\nlog_r_5 = gaussian_laplace(image[:,:,0], sigma = .5)\nlog_g_5 = gaussian_laplace(image[:,:,1], sigma = .5)\nlog_b_5 = gaussian_laplace(image[:,:,2], sigma = .5)\n\n# LoG blur sigma = .6\nlog_r_6 = gaussian_laplace(image[:,:,0], sigma = .6)\nlog_g_6 = gaussian_laplace(image[:,:,1], sigma = .6)\nlog_b_6 = gaussian_laplace(image[:,:,2], sigma = .6)\n\n# LoG blur sigma = .8\nlog_r_8 = gaussian_laplace(image[:,:,0], sigma = .8)\nlog_g_8 = gaussian_laplace(image[:,:,1], sigma = .8)\nlog_b_8 = gaussian_laplace(image[:,:,2], sigma = .8)\n\n# Add layers to model\nimage_layers = np.dstack([image, r, canny_edges_r, canny_edges_g, canny_edges_b,\n                             gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                             gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                             log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8, hough_lines, gray_mask])"
  },
  {
    "objectID": "code/k_means.html#pca-to-select-important-features",
    "href": "code/k_means.html#pca-to-select-important-features",
    "title": "Testing K-means Clustering",
    "section": "",
    "text": "image_layers.shape\n\n(75, 100, 27)\n\n\n\nimage_layers = image_layers.reshape(image_layers.shape[0] * image_layers.shape[1], image_layers.shape[2])\nimage_layers.shape\n\n(7500, 27)\n\n\n\n# Standardize the features\nscaler = StandardScaler()\nimage_layers_scaled = scaler.fit_transform(image_layers)\n\n\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=5)\nlayers_pca = pca.fit_transform(image_layers_scaled)\n\n# Explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Plotting the explained variance ratio\nplt.figure(figsize=(8, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\n\nlayers_pca.shape\n\n(7500, 5)\n\n\nTransform to create new features in less dimensions\n\nnew_features = pca.transform(image_layers_scaled)\n\n\nnew_features.shape\n\n(7500, 5)\n\n\n\n\n\nsk_kmeans_pca = KMeans(n_clusters=3, verbose=1).fit(new_features)\n\nInitialization complete\nIteration 0, inertia 123716.19268453149.\nIteration 1, inertia 81206.56131639544.\nIteration 2, inertia 80379.63761496074.\nIteration 3, inertia 80248.56084677183.\nIteration 4, inertia 80221.05356165496.\nIteration 5, inertia 80216.4490686662.\nConverged at iteration 5: center shift 0.00034726557264135494 within tolerance 0.00039411965957254487.\n\n\n\nsk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n\n# Generating figure 2\nfig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n\ncluster_one = (sk_segmented_image_pca==0)\naxes[0][0].imshow(cluster_one, cmap='gray')\naxes[0][0].set_title('Cluster One')\n\ncluster_two = (sk_segmented_image_pca==1)\naxes[0][1].imshow(cluster_two, cmap='gray')\naxes[0][1].set_title('Cluster Two')\n\ncluster_three = (sk_segmented_image_pca==2)\naxes[1][0].imshow(cluster_three, cmap='gray')\naxes[1][0].set_title('Cluster Three')\n\naxes[1][1].imshow(image)\naxes[1][1].set_title('Original Image')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhich cluster best captures the roads? Let’s find out with the results\n\nimage_label_bool = (image_label==255)\n\n\ncluster_one.dtype\n\ndtype('bool')\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_one.ravel())\n\nConfusion matrix:\n [[1078  499]\n [2255 3668]]\nOverall accuracy: 0.633 \nPrecision: 0.323 \nRecall 0.684 \nDice Coefficient 0.439\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_two.ravel())\n\nConfusion matrix:\n [[ 109 1468]\n [2356 3567]]\nOverall accuracy: 0.49 \nPrecision: 0.044 \nRecall 0.069 \nDice Coefficient 0.054\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_three.ravel())\n\nConfusion matrix:\n [[ 390 1187]\n [1312 4611]]\nOverall accuracy: 0.667 \nPrecision: 0.229 \nRecall 0.247 \nDice Coefficient 0.238\n\n\n\ndef identify_road_cluster(clustered_image, image_label):\n\n    cluster_labels = np.unique(clustered_image)\n\n    best_precision = 0\n    best_cluster = -1\n\n    for i in cluster_labels:\n        cluster = (sk_segmented_image_pca==i)\n        C = confusion_matrix(image_label.ravel(), cluster.ravel(), labels=(True, False))\n\n        # # Overall accuracy rate\n        # acc = (C[0,0] + C[1,1])/C.sum()\n        # # Recall\n        # recall = (C[0,0])/(C[0,0] + C[1,0])\n        # Precision\n        prec = (C[0,0])/(C[0,0] + C[0,1])\n\n        if prec &gt; best_precision:\n            best_precision = prec\n            best_cluster = i\n\n    return best_cluster\n\n\ncluster = identify_road_cluster(sk_segmented_image_pca, image_label_bool)\n\n\nimage_label\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)\n\n\n\nfig = plt.figure(1, figsize=(15, 15))\nplt.clf()\n\nax = fig.add_subplot(111, projection=\"3d\", elev=45, azim=90)\nax.set_position([0, 0, 0.95, 1])\n\nX = new_features\ny = image_label.ravel()\n\nfor name, label in [(\"Background\", 0), (\"Road\", 255)]:\n    ax.text3D(\n        X[y == label, 0].mean(),\n        X[y == label, 1].mean() + 1.5,\n        X[y == label, 2].mean(),\n        name,\n        horizontalalignment=\"center\",\n        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n    )\n# Reorder the labels to have colors matching the cluster results\n# y = np.choose(y, [0, 255]).astype(float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='Accent', edgecolor=\"k\")\n\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndef create_features(image):\n\n    # Calculation Canny gradient\n    image_gray = skol.rgb2gray(image)\n    canny_edges = feature.canny(image_gray, sigma=3)\n\n    # Create disk\n    disk = skimor.disk(1)\n\n    # Area closing for hough lines\n    closed_edges = skimor.dilation(canny_edges, footprint = disk)\n    closed_edges = closed_edges * 255\n\n    lines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n    hough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n\n    # Draw the detected lines on the canvas\n    for line in lines:\n        p0, p1 = line\n        # Draw line segment\n        rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n        hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n\n    #create gray mask\n    gray_mask = classify_gray(image)\n    gray_mask = gray_mask.reshape((image.shape[0], image.shape[1]))\n\n    # Create additional features\n\n    # Range of values (gray pixels will have low range)\n    r = image.max(axis = 2) - image.min(axis = 2)\n\n    # Canny edge detection\n    canny_edges_r = feature.canny(image[:,:,0], sigma=4);\n    canny_edges_g = feature.canny(image[:,:,1], sigma=4);\n    canny_edges_b = feature.canny(image[:,:,2], sigma=4);\n\n    # Gaussian blur sigma = 1\n    gaus_r_1 = gaussian_filter(image[:,:,0], sigma = 1)\n    gaus_g_1 = gaussian_filter(image[:,:,1], sigma = 1)\n    gaus_b_1 = gaussian_filter(image[:,:,2], sigma = 1)\n\n    # Gaussian blur sigma = 3\n    gaus_r_3 = gaussian_filter(image[:,:,0], sigma = 3)\n    gaus_g_3 = gaussian_filter(image[:,:,1], sigma = 3)\n    gaus_b_3 = gaussian_filter(image[:,:,2], sigma = 3)\n\n    # Gaussian blur sigma = 5\n    gaus_r_5 = gaussian_filter(image[:,:,0], sigma = 5)\n    gaus_g_5 = gaussian_filter(image[:,:,1], sigma = 5)\n    gaus_b_5 = gaussian_filter(image[:,:,2], sigma = 5)\n\n    # LoG blur sigma = .5\n    log_r_5 = gaussian_laplace(image[:,:,0], sigma = .5)\n    log_g_5 = gaussian_laplace(image[:,:,1], sigma = .5)\n    log_b_5 = gaussian_laplace(image[:,:,2], sigma = .5)\n\n    # LoG blur sigma = .6\n    log_r_6 = gaussian_laplace(image[:,:,0], sigma = .6)\n    log_g_6 = gaussian_laplace(image[:,:,1], sigma = .6)\n    log_b_6 = gaussian_laplace(image[:,:,2], sigma = .6)\n\n    # LoG blur sigma = .8\n    log_r_8 = gaussian_laplace(image[:,:,0], sigma = .8)\n    log_g_8 = gaussian_laplace(image[:,:,1], sigma = .8)\n    log_b_8 = gaussian_laplace(image[:,:,2], sigma = .8)\n\n    # Add layers to model\n    image_layers = np.dstack([image, r, canny_edges_r, canny_edges_g, canny_edges_b,\n                                gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                                gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                                log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8, hough_lines, gray_mask])\n    \n    image_layers = image_layers.reshape(image_layers.shape[0] * image_layers.shape[1], image_layers.shape[2])\n    \n    scaler = StandardScaler()\n    image_layers_scaled = scaler.fit_transform(image_layers)\n\n    print(image_layers.shape)\n    \n    return image_layers\n\n\ndef get_pca(image_layers, n_components):\n\n    # Initialize PCA and fit the scaled data\n    pca = PCA(n_components=n_components)\n    layers_pca = pca.fit_transform(image_layers)\n\n    # Explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    new_features = pca.transform(image_layers)\n\n    return new_features\n\n\ndef run_k_means(image_layers, n_clusters):\n\n    sk_kmeans = KMeans(n_clusters=n_clusters, verbose=1).fit(image_layers)\n\n    sk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n    best_cluster = identify_road_cluster(sk_segmented_image_pca, image_label)\n\n    segmented_roads = (sk_segmented_image_pca==best_cluster)\n\n    return segmented_roads\n\n\ndef full_pipeline(image, image_label, n_clusters=3, with_pca=False, pca_n_components=5):\n    \n    print(\"Creating features...\")\n    image_layers = create_features(image)\n    if(with_pca):\n        print(\"Reducing dimensions...\")\n        image_layers = get_pca(image_layers=image_layers, n_components=pca_n_components)\n    \n    print(\"Segmenting Roads...\")\n    segmented_roads = run_k_means(image_layers=image_layers, n_clusters=n_clusters)\n    print(\"Segmentation Complete\")\n    \n    plt.imshow(segmented_roads, cmap='gray')\n    plt.show()\n\n    print(\"Analyzing metrics...\")\n    accuracy_metrics(segmented_roads, image_label)\n\n\nimage.shape\n\n(75, 100, 3)\n\n\n\n# full_pipeline(image, image_label_bool)"
  },
  {
    "objectID": "Segmentation_UNET.html",
    "href": "Segmentation_UNET.html",
    "title": "",
    "section": "",
    "text": "# Import packages\nimport numpy as np\nimport skimage.io as skio\nimport skimage.morphology as skm\nfrom skimage import feature\nfrom skimage.util import view_as_blocks\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n# !pip install opendatasets\n# import opendatasets as od\nimport os\nimport torch\nimport cv2\nimport math\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import concatenate, Concatenate, Conv2D,Conv3D, MaxPooling2D, Conv2DTranspose\nfrom keras.layers import Input, UpSampling3D, BatchNormalization, UpSampling2D\nfrom tensorflow.keras.optimizers.legacy import Adam\n\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as keras\n\n# data = od.download(\"https://www.kaggle.com/datasets/balraj98/massachusetts-roads-dataset\")\n\n2024-05-14 10:23:00.840810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n# Function to print several accuracy metrics\ndef accuracy_metrics(y_true, y_pred):\n    # Create confusion matrix\n\n    if(np.max(y_true)&gt;1):\n        y_true = y_true / 255\n\n    if(np.max(y_pred)&gt;1):\n        y_pred = y_pred / 255\n\n    y_true = y_true.ravel()\n    y_pred = y_pred.ravel()\n\n    C = confusion_matrix(y_true, y_pred, labels=(True, False))\n\n    # Overall accuracy rate\n    acc = (C[0,0] + C[1,1])/C.sum()\n\n    # Recall\n    recall = (C[0,0])/(C[0,0] + C[1,0])\n    \n    # Precision\n    prec = (C[0,0])/(C[0,0] + C[0,1])\n\n    smooth = 1\n    dice = (2. * (np.sum(y_true * y_pred)) + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)\n\n    # Print results\n    print(\"Confusion matrix:\\n\", C)\n    print(\"Overall accuracy:\", np.round(acc, 3), \"\\nPrecision:\", np.round(recall, 3), \"\\nRecall\", np.round(prec, 3), \"\\nDice Coefficient\", np.round(dice, 3))\n# Read the data\nrgb = skio.imread(\"./data/massRoads/tiff/train/10828735_15.tiff\")\nans = skio.imread(\"./data/massRoads/tiff/train_labels/10828735_15.tif\")\n\n# Display training data and correct output\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(rgb, ax = ax[0])\nax[0].set_title(\"Data\")\nskio.imshow(ans, ax = ax[1])\nax[1].set_title(\"Label\")\n\nText(0.5, 1.0, 'Label')\nDATA_DIR = './data/massRoads/tiff/'\n\nx_train_dir = os.path.join(DATA_DIR, 'train')\ny_train_dir = os.path.join(DATA_DIR, 'train_labels')\n\nx_valid_dir = os.path.join(DATA_DIR, 'val')\ny_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n\nx_test_dir = os.path.join(DATA_DIR, 'test')\ny_test_dir = os.path.join(DATA_DIR, 'test_labels')\nclass_dict = pd.read_csv(\"./data/massRoads/label_class_dict.csv\")\n# Get class names\nclass_names = class_dict['name'].tolist()\n# Get class RGB values\nclass_rgb_values = class_dict[['r','g','b']].values.tolist()\n\nprint('All dataset classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)\n\nAll dataset classes and their corresponding RGB values in labels:\nClass Names:  ['background', 'road']\nClass RGB values:  [[0, 0, 0], [255, 255, 255]]\n# image paths\nimage_paths = [os.path.join(x_train_dir, image_id) for image_id in sorted(os.listdir(x_train_dir))]\nmask_paths = [os.path.join(y_train_dir, image_id) for image_id in sorted(os.listdir(y_train_dir))]\n\nval_img_paths = [os.path.join(x_valid_dir, image_id) for image_id in sorted(os.listdir(x_valid_dir))]\nval_mask_paths = [os.path.join(y_valid_dir, image_id) for image_id in sorted(os.listdir(y_valid_dir))]\n\ntest_img_paths = [os.path.join(x_test_dir, image_id) for image_id in sorted(os.listdir(x_test_dir))]\ntest_mask_paths = [os.path.join(y_test_dir, image_id) for image_id in sorted(os.listdir(y_test_dir))]\n# prep image training\ntrain_len = 50\npatch_size = 128\nnum_patches = 1500 // patch_size  # Number of patches per dimension\n\nsmall_train_images = []\nsmall_train_labels = []\n\nfor i in range(train_len):\n    # read images for each path\n    training_image = cv2.imread(image_paths[i])\n    training_mask = cv2.imread(mask_paths[i])\n\n    for i in range(num_patches):\n        for j in range(num_patches):\n            # Extract patch from the image\n            image_patch = training_image[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size, :]\n            mask_patch = training_mask[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size, :]\n            \n            if(len(np.unique(mask_patch))&gt;1):\n\n                #normalize\n                image_patch = image_patch / 255\n                mask_patch = mask_patch / 255\n\n                small_train_images.append(image_patch)\n                small_train_labels.append(mask_patch)\n\nsmall_train_images = np.array(small_train_images)\nsmall_train_labels = np.array(small_train_labels)[:,:,:,0]\nsmall_train_images.shape\n\n(3448, 128, 128, 3)\ncount = 0\n\nfor label in small_train_labels[:2000]:\n    if(len(np.unique(label))==1):\n       count += 1\n\n\nprint(\"Number of blank images: \" + str(count))\n\nNumber of blank images: 0\nval_len = len(val_img_paths)\n\nval_img = [0] * val_len\nval_masks = [0] * val_len\n\nfor i in range(val_len):\n    val_img[i] = cv2.imread(val_img_paths[i])\n    val_img[i] = val_img[i] / 255\n    val_masks[i] = cv2.imread(val_mask_paths[i])\n    val_masks[i] = val_masks[i] / 255\n\nval_img = np.array(val_img[:])\nval_img = val_img[:, :patch_size, :patch_size]\nval_masks = np.array(val_masks)[:,:,:,0]\nval_masks = val_masks[:, :patch_size, :patch_size]\ncount = 0\n\nfor label in val_masks:\n    if(len(np.unique(label))==1):\n       count += 1\n\n\nprint(\"Number of blank images: \" + str(count))\n\nNumber of blank images: 1\ntest_len = len(test_img_paths)\n\ntest_img = [0] * test_len\ntest_masks = [0] * test_len\n\nfor i in range(test_len):\n    test_img[i] = cv2.imread(test_img_paths[i])\n    test_img[i] = test_img[i] / 255\n    test_masks[i] = cv2.imread(test_mask_paths[i])\n    test_masks[i] = test_masks[i] / 255\n\ntest_img = np.array(test_img[:])\ntest_img = test_img[:, :patch_size, :patch_size]\ntest_masks = np.array(test_masks)[:,:,:,0]\ntest_masks = test_masks[:, :patch_size, :patch_size]\ncount = 0\n\nfor label in test_masks:\n    if(len(np.unique(label))==1):\n       count += 1\n\n\nprint(\"Number of blank images: \" + str(count))\n\nNumber of blank images: 19\nprint(small_train_images.shape)\nprint(small_train_labels.shape)\nprint(val_img.shape)\nprint(val_masks.shape)\nprint(test_img.shape)\nprint(test_masks.shape)\n\n(3448, 128, 128, 3)\n(3448, 128, 128)\n(14, 128, 128, 3)\n(14, 128, 128)\n(49, 128, 128, 3)\n(49, 128, 128)\n# Display training data and correct output\ni = 500\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nax[0].imshow(small_train_images[i], cmap='gray')\nax[0].set_title(\"Image\")\nax[1].imshow(small_train_labels[i], cmap='gray')\nax[1].set_title(\"Label\")\n\nText(0.5, 1.0, 'Label')\nsmooth=1\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n# I found this architecture with the shape (128, 128, 1).\n\ndef unet(input_size = (128,128,3)):\n    inputs = Input(input_size)\n    conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n    conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    up6 = Conv2DTranspose(128,2,strides=(2,2),padding='same')(drop5)\n    merge6 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n    conv6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n    up7 = Conv2DTranspose(64,2,strides=(2,2),padding='same')(conv6)\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n    conv7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n    up8 = Conv2DTranspose(32,2,strides=(2,2),padding='same')(conv7)\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n    conv8 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n    up9 = Conv2DTranspose(16,2,strides=(2,2),padding='same')(conv8)\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n    conv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n\n    model = Model(inputs, conv10)\n\n    model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=[dice_coef])\n\n    model.summary()\n    \n    return model\nmodel = unet()\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (None, 128, 128,  │          0 │ -                 │\n│ (InputLayer)        │ 3)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d (Conv2D)     │ (None, 128, 128,  │        448 │ input_layer[0][0] │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_1 (Conv2D)   │ (None, 128, 128,  │      2,320 │ conv2d[0][0]      │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d       │ (None, 64, 64,    │          0 │ conv2d_1[0][0]    │\n│ (MaxPooling2D)      │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_2 (Conv2D)   │ (None, 64, 64,    │      4,640 │ max_pooling2d[0]… │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_3 (Conv2D)   │ (None, 64, 64,    │      9,248 │ conv2d_2[0][0]    │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_1     │ (None, 32, 32,    │          0 │ conv2d_3[0][0]    │\n│ (MaxPooling2D)      │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_4 (Conv2D)   │ (None, 32, 32,    │     18,496 │ max_pooling2d_1[… │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_5 (Conv2D)   │ (None, 32, 32,    │     36,928 │ conv2d_4[0][0]    │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_2     │ (None, 16, 16,    │          0 │ conv2d_5[0][0]    │\n│ (MaxPooling2D)      │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_6 (Conv2D)   │ (None, 16, 16,    │     73,856 │ max_pooling2d_2[… │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_7 (Conv2D)   │ (None, 16, 16,    │    147,584 │ conv2d_6[0][0]    │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (Dropout)   │ (None, 16, 16,    │          0 │ conv2d_7[0][0]    │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_3     │ (None, 8, 8, 128) │          0 │ dropout[0][0]     │\n│ (MaxPooling2D)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_8 (Conv2D)   │ (None, 8, 8, 256) │    295,168 │ max_pooling2d_3[… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_9 (Conv2D)   │ (None, 8, 8, 256) │    590,080 │ conv2d_8[0][0]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (Dropout) │ (None, 8, 8, 256) │          0 │ conv2d_9[0][0]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose    │ (None, 16, 16,    │    131,200 │ dropout_1[0][0]   │\n│ (Conv2DTranspose)   │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (None, 16, 16,    │          0 │ dropout[0][0],    │\n│ (Concatenate)       │ 256)              │            │ conv2d_transpose… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_10 (Conv2D)  │ (None, 16, 16,    │    295,040 │ concatenate[0][0] │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_11 (Conv2D)  │ (None, 16, 16,    │    147,584 │ conv2d_10[0][0]   │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_1  │ (None, 32, 32,    │     32,832 │ conv2d_11[0][0]   │\n│ (Conv2DTranspose)   │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (None, 32, 32,    │          0 │ conv2d_5[0][0],   │\n│ (Concatenate)       │ 128)              │            │ conv2d_transpose… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_12 (Conv2D)  │ (None, 32, 32,    │     73,792 │ concatenate_1[0]… │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_13 (Conv2D)  │ (None, 32, 32,    │     36,928 │ conv2d_12[0][0]   │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_2  │ (None, 64, 64,    │      8,224 │ conv2d_13[0][0]   │\n│ (Conv2DTranspose)   │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_2       │ (None, 64, 64,    │          0 │ conv2d_3[0][0],   │\n│ (Concatenate)       │ 64)               │            │ conv2d_transpose… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_14 (Conv2D)  │ (None, 64, 64,    │     18,464 │ concatenate_2[0]… │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_15 (Conv2D)  │ (None, 64, 64,    │      9,248 │ conv2d_14[0][0]   │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_3  │ (None, 128, 128,  │      2,064 │ conv2d_15[0][0]   │\n│ (Conv2DTranspose)   │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_3       │ (None, 128, 128,  │          0 │ conv2d_1[0][0],   │\n│ (Concatenate)       │ 32)               │            │ conv2d_transpose… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_16 (Conv2D)  │ (None, 128, 128,  │      4,624 │ concatenate_3[0]… │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_17 (Conv2D)  │ (None, 128, 128,  │      2,320 │ conv2d_16[0][0]   │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_18 (Conv2D)  │ (None, 128, 128,  │        290 │ conv2d_17[0][0]   │\n│                     │ 2)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_19 (Conv2D)  │ (None, 128, 128,  │          3 │ conv2d_18[0][0]   │\n│                     │ 1)                │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 1,941,381 (7.41 MB)\n\n\n\n Trainable params: 1,941,381 (7.41 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\nTrain with first 750 images\ncallbacks = [\n#     tf.keras.callbacks.ModelCheckpoint('unet_model.h5', save_best_only=True, verbose=2),\n    tf.keras.callbacks.EarlyStopping(patience=5, monitor= 'val_dice_coef', mode='max', restore_best_weights=True)\n]\n\n# Train the model\nhistory = model.fit(x=small_train_images[:750], y=small_train_labels[:750], \n                    validation_data = (val_img, val_masks), epochs = 20, callbacks=[callbacks], \n                    verbose = 1)\n\nEpoch 1/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.4494 - loss: 0.1150 - val_dice_coef: 0.4529 - val_loss: 0.1392\nEpoch 2/20\n 3/24 ━━━━━━━━━━━━━━━━━━━━ 37s 2s/step - dice_coef: 0.5123 - loss: 0.0988\n\n\nKeyboardInterrupt:\nprediction = model.predict(test_img)\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 471ms/step\n# Display training data and correct output\ni = 20\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\nax[0].imshow(prediction[i], cmap='gray')\nax[0].set_title(\"Prediction\")\nax[1].imshow(test_masks[i], cmap='gray')\nax[1].set_title(\"Label\")\nax[2].imshow(test_img[i], cmap='gray')\nax[2].set_title(\"Image\")\n\nText(0.5, 1.0, 'Image')\nTrain the model again wiht images 750-1500\n# Train the model\nhistory = model.fit(x=small_train_images[750:1500], y=small_train_labels[750:1500], \n                    validation_data = (val_img, val_masks), epochs = 20, callbacks=[callbacks], \n                    verbose = 1)\n\nEpoch 1/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 38s 2s/step - dice_coef: 0.2316 - loss: 0.1549 - val_dice_coef: 0.2238 - val_loss: 0.2140\nEpoch 2/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 38s 2s/step - dice_coef: 0.2553 - loss: 0.1472 - val_dice_coef: 0.2162 - val_loss: 0.2248\nEpoch 3/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 38s 2s/step - dice_coef: 0.3001 - loss: 0.1399 - val_dice_coef: 0.2092 - val_loss: 0.2282\nEpoch 4/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - dice_coef: 0.3319 - loss: 0.1331 - val_dice_coef: 0.2704 - val_loss: 0.2174\nEpoch 5/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.3906 - loss: 0.1293 - val_dice_coef: 0.3306 - val_loss: 0.2072\nEpoch 6/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.4741 - loss: 0.1135 - val_dice_coef: 0.3308 - val_loss: 0.2239\nEpoch 7/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.4950 - loss: 0.1127 - val_dice_coef: 0.3040 - val_loss: 0.1911\nEpoch 8/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 39s 2s/step - dice_coef: 0.5130 - loss: 0.1014 - val_dice_coef: 0.3675 - val_loss: 0.2199\nEpoch 9/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - dice_coef: 0.5580 - loss: 0.0961 - val_dice_coef: 0.3538 - val_loss: 0.1848\nEpoch 10/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 43s 2s/step - dice_coef: 0.5581 - loss: 0.0943 - val_dice_coef: 0.3786 - val_loss: 0.1927\nEpoch 11/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - dice_coef: 0.5946 - loss: 0.0858 - val_dice_coef: 0.3710 - val_loss: 0.1812\nEpoch 12/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - dice_coef: 0.5862 - loss: 0.0878 - val_dice_coef: 0.3663 - val_loss: 0.1856\nEpoch 13/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.6282 - loss: 0.0802 - val_dice_coef: 0.3766 - val_loss: 0.2192\nEpoch 14/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6466 - loss: 0.0761 - val_dice_coef: 0.4121 - val_loss: 0.1739\nEpoch 15/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6484 - loss: 0.0730 - val_dice_coef: 0.4263 - val_loss: 0.1737\nEpoch 16/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6541 - loss: 0.0736 - val_dice_coef: 0.4321 - val_loss: 0.1925\nEpoch 17/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6694 - loss: 0.0705 - val_dice_coef: 0.4549 - val_loss: 0.1918\nEpoch 18/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6854 - loss: 0.0647 - val_dice_coef: 0.4776 - val_loss: 0.1604\nEpoch 19/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6670 - loss: 0.0694 - val_dice_coef: 0.4870 - val_loss: 0.1762\nEpoch 20/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6790 - loss: 0.0691 - val_dice_coef: 0.4810 - val_loss: 0.1806\nprediction_2 = model.predict(test_img)\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 216ms/step\n# Display training data and correct output\ni = 20\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\nax[0].imshow(prediction_2[i], cmap='gray')\nax[0].set_title(\"Prediction\")\nax[1].imshow(test_masks[i], cmap='gray')\nax[1].set_title(\"Label\")\nax[2].imshow(test_img[i], cmap='gray')\nax[2].set_title(\"Image\")\n\nText(0.5, 1.0, 'Image')\nTrain the model on training images 1500-2250\n# Train the model\nhistory = model.fit(x=small_train_images[1500:2250], y=small_train_labels[1500:2250], \n                    validation_data = (val_img, val_masks), epochs = 20, callbacks=[callbacks], \n                    verbose = 1)\n\nEpoch 1/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - dice_coef: 0.5745 - loss: 0.1068 - val_dice_coef: 0.4897 - val_loss: 0.1779\nEpoch 2/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.6172 - loss: 0.0948 - val_dice_coef: 0.5241 - val_loss: 0.1367\nEpoch 3/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6208 - loss: 0.0882 - val_dice_coef: 0.5413 - val_loss: 0.1394\nEpoch 4/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6338 - loss: 0.0875 - val_dice_coef: 0.5516 - val_loss: 0.1368\nEpoch 5/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6411 - loss: 0.0853 - val_dice_coef: 0.5266 - val_loss: 0.1661\nEpoch 6/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6694 - loss: 0.0782 - val_dice_coef: 0.5727 - val_loss: 0.1696\nEpoch 7/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6900 - loss: 0.0746 - val_dice_coef: 0.5259 - val_loss: 0.1468\nprediction_3 = model.predict(test_img)\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 238ms/step\n# Display training data and correct output\ni = 40\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\nax[0].imshow(prediction_3[i], cmap='gray')\nax[0].set_title(\"Prediction\")\nax[1].imshow(test_masks[i], cmap='gray')\nax[1].set_title(\"Label\")\nax[2].imshow(test_img[i], cmap='gray')\nax[2].set_title(\"Image\")\n\nText(0.5, 1.0, 'Image')"
  },
  {
    "objectID": "Segmentation_UNET.html#predicting-our-test-image",
    "href": "Segmentation_UNET.html#predicting-our-test-image",
    "title": "",
    "section": "Predicting our test image",
    "text": "Predicting our test image\n\nimage_filepath = \"./data/massRoads/tiff\"\n# /train/... for images and /train_labels/... for labels\ntesting_image = skio.imread(image_filepath + \"/train/10828735_15.tiff\")\ntesting_image_label = skio.imread(image_filepath + \"/train_labels/10828735_15.tif\")\n\n\nsmall_test_images = []\n\nfor i in range(num_patches):\n    for j in range(num_patches):\n        # Extract patch from the image\n        patch = testing_image[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size, :]\n        small_test_images.append(patch)\n\nsmall_test_images = np.array(small_test_images)\n\n\ntest_prediction = model.predict(small_test_images)\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 2s 472ms/step\n\n\n\nReconstruct the image\n\ndef reconstruct_image(image_batches, num_patches, patch_size):\n    #  Calculate the dimensions of the original image\n    \n    image_dim = num_patches * patch_size\n\n    # Reshape the array\n    reshaped_patches = np.reshape(image_batches, (num_patches, num_patches, patch_size, patch_size))\n\n    # Initialize an empty array to store the reconstructed image\n    reconstructed_image = np.zeros((image_dim, image_dim), dtype=np.uint8)\n\n    # Reconstruct the image\n    for i in range(num_patches):\n        for j in range(num_patches):\n            # Get the current patch\n            patch = reshaped_patches[i][j]\n            patch = patch &gt; 0.5\n            \n            # Calculate the starting and ending indices for placing the patch\n            start_i, start_j = i * patch_size, j * patch_size\n            end_i, end_j = start_i + patch_size, start_j + patch_size\n            \n            # Place the patch in the reconstructed image\n            reconstructed_image[start_i:end_i, start_j:end_j] = patch\n    \n    return reconstructed_image\n\n\nreconstructed_image = reconstruct_image(test_prediction, num_patches=num_patches, patch_size=patch_size)\n\n\ntesting_image_label_shrunk = testing_image_label[:reconstructed_image.shape[0], :reconstructed_image.shape[1]]\ntesting_image_shrunk = testing_image[:reconstructed_image.shape[0], :reconstructed_image.shape[1]]\n\n\n# Display training data and correct output\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\nax[0].imshow(reconstructed_image, cmap='gray')\nax[0].set_title(\"Prediction\")\nax[1].imshow(testing_image_label_shrunk, cmap='gray')\nax[1].set_title(\"Label\")\nax[2].imshow(testing_image_shrunk, cmap='gray')\nax[2].set_title(\"Image\")\n\nplt.show()\n\nText(0.5, 1.0, 'Image')\n\n\n\n\n\n\n\n\n\n\naccuracy_metrics(reconstructed_image, testing_image_label_shrunk)\n\nConfusion matrix:\n [[  10955   44651]\n [ 171527 1755331]]\nOverall accuracy: 0.891 \nPrecision: 0.06 \nRecall 0.197 \nDice Coefficient 0.092"
  },
  {
    "objectID": "code/rf.html",
    "href": "code/rf.html",
    "title": "Packages, Data, Functions",
    "section": "",
    "text": "# Import packages\nimport numpy as np\nimport skimage.io as skio\nimport skimage.morphology as skm\nfrom skimage import feature\nfrom skimage.color import rgb2gray\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.cluster import KMeans\nfrom sklearn.utils.random import sample_without_replacement\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom scipy.ndimage import gaussian_filter\nfrom scipy.ndimage import gaussian_laplace\nfrom scipy.ndimage import maximum_filter\nfrom scipy.ndimage import minimum_filter\nfrom scipy.ndimage import median_filter\n\n\n# Read the data\nrgb = skio.imread(\"../../data/MA_roads/tiff/train/10828735_15.tiff\")\nans = skio.imread(\"../../data/MA_roads/tiff/train_labels/10828735_15.tif\") &gt; 0\n\nrgb_test = skio.imread(\"../../data/MA_roads/tiff/train/21929005_15.tiff\")\nans_test = skio.imread(\"../../data/MA_roads/tiff/train_labels/21929005_15.tif\") &gt; 0\n\n# Display training data and correct output\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(rgb, ax = ax[0])\nax[0].set_title(\"Data\")\nskio.imshow(ans, ax = ax[1])\nax[1].set_title(\"Solution\");\n\n\n\n\n\n\n\n\n\n# Function to compute DICE\nsmooth=1\ndef dice_coef(y_true, y_pred):\n    intersection = np.sum(y_true * y_pred)\n    return (2. * intersection + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)\n        \n# Function to print several accuracy metrics\ndef accuracy_metrics(y_true, y_pred):\n    # Create confusion matrix\n    C = confusion_matrix(y_true, y_pred, labels=(True, False))\n\n    # Overall accuracy rate\n    acc = (C[0,0] + C[1,1])/C.sum()\n\n    # Recall\n    recall = (C[0,0])/(C[0,0] + C[1,0])\n    \n    # Precision\n    prec = (C[0,0])/(C[0,0] + C[0,1])\n\n    # DICE\n    dice = dice_coef(y_true, y_pred)\n\n\n    # Print results\n    print(\"Confusion matrix:\\n\", C)\n    print(\"Overall accuracy:\", np.round(acc, 3), \"\\nPrecision:\", np.round(recall, 3),\n            \"\\nRecall\", np.round(prec, 3), \"\\nDICE:\", np.round(dice, 3))\n\n# Function to compute layers for additional model features\ndef compute_features(img, include_categorical = True):    \n    # Range of values (gray pixels will have low range)\n    r = img.max(axis = 2) - img.min(axis = 2)\n\n    if include_categorical:\n        # Canny edge detection\n        canny_edges_r = feature.canny(img[:,:,0], sigma=4)\n        canny_edges_g = feature.canny(img[:,:,1], sigma=4)\n        canny_edges_b = feature.canny(img[:,:,2], sigma=4)\n        img = np.dstack([img, canny_edges_r, canny_edges_g, canny_edges_b])\n    \n    # Gaussian blur sigma = 1\n    gaus_r_1 = gaussian_filter(img[:,:,0], sigma = 1)\n    gaus_g_1 = gaussian_filter(img[:,:,1], sigma = 1)\n    gaus_b_1 = gaussian_filter(img[:,:,2], sigma = 1)\n    \n    # Gaussian blur sigma = 3\n    gaus_r_3 = gaussian_filter(img[:,:,0], sigma = 3)\n    gaus_g_3 = gaussian_filter(img[:,:,1], sigma = 3)\n    gaus_b_3 = gaussian_filter(img[:,:,2], sigma = 3)\n\n    # Gaussian blur sigma = 5\n    gaus_r_5 = gaussian_filter(img[:,:,0], sigma = 5)\n    gaus_g_5 = gaussian_filter(img[:,:,1], sigma = 5)\n    gaus_b_5 = gaussian_filter(img[:,:,2], sigma = 5)\n    \n    # LoG blur sigma = .5\n    log_r_5 = gaussian_laplace(img[:,:,0], sigma = .5)\n    log_g_5 = gaussian_laplace(img[:,:,1], sigma = .5)\n    log_b_5 = gaussian_laplace(img[:,:,2], sigma = .5)\n    \n    # LoG blur sigma = .6\n    log_r_6 = gaussian_laplace(img[:,:,0], sigma = .6)\n    log_g_6 = gaussian_laplace(img[:,:,1], sigma = .6)\n    log_b_6 = gaussian_laplace(img[:,:,2], sigma = .6)\n    \n    # LoG blur sigma = .8\n    log_r_8 = gaussian_laplace(img[:,:,0], sigma = .8)\n    log_g_8 = gaussian_laplace(img[:,:,1], sigma = .8)\n    log_b_8 = gaussian_laplace(img[:,:,2], sigma = .8)\n    \n    # Add layers to model\n    return np.dstack([img, r,\n                     gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                     gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                     log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8])"
  },
  {
    "objectID": "code/rf.html#train-model",
    "href": "code/rf.html#train-model",
    "title": "Packages, Data, Functions",
    "section": "Train model",
    "text": "Train model\n\n# Flatten images\ntrain_small_rgb = small_rgb.reshape(small_rgb.shape[0]*small_rgb.shape[1], 3)\ny_train = small_ans.reshape(small_ans.shape[0]*small_ans.shape[1])\n\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel1 = RF.fit(train_small_rgb, y_train)\n\n\n# Predictions on training data\nmodel1_pred = model1.predict(train_small_rgb)\n\n# Confusion matrix\naccuracy_metrics(y_train, model1_pred)\n\nConfusion matrix:\n [[  8989   4034]\n [  1545 105432]]\nOverall accuracy: 0.954 \nPrecision: 0.853 \nRecall 0.69 \nDICE: 0.763\n\n\nWhile we have a really good overall accuracy rate, we are correctly predicting only 68.7% of the actual road pixels. With a precision of 0.854, about 85.4% of our road predictions are actually roads.\n\n# Convert predictions to image\ntrain_preds = model1_pred.reshape(small_ans.shape[0], small_ans.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\n\nskio.imshow(train_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans, ax = ax[1])\nax[1].set_title(\"Actual Solution\");\n\n\n\n\n\n\n\n\nVisually, our solution looks alright, but it obviously has room for improvement. Let’s see what our results look like on the testing data."
  },
  {
    "objectID": "code/rf.html#test-model",
    "href": "code/rf.html#test-model",
    "title": "Packages, Data, Functions",
    "section": "Test model",
    "text": "Test model\n\n# Flatten images\ntest_small_rgb = small_rgb_test.reshape(small_rgb_test.shape[0]*small_rgb_test.shape[1], 3)\ny_test = small_ans_test.reshape(small_ans_test.shape[0]*small_ans_test.shape[1])\n\n\n# Predictions on testing data\nmodel1_test_pred = model1.predict(test_small_rgb)\n\n# Confusion matrix\naccuracy_metrics(y_test, model1_test_pred)\n\nConfusion matrix:\n [[   598   6505]\n [  4840 108057]]\nOverall accuracy: 0.905 \nPrecision: 0.11 \nRecall 0.084 \nDICE: 0.095\n\n\nWhile we still have a good overall accuracy rate, our predictions of roads is substantially worse. We have only classified 8.3% of the road pixels correctly, and only 11.3% of our road predictions were actually roads.\n\n# Convert predictions to image\ntest_preds = model1_test_pred.reshape(small_ans_test.shape[0], small_ans_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\n\nskio.imshow(test_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans_test, ax = ax[1])\nax[1].set_title(\"Actual Solution\");\n\n\n\n\n\n\n\n\nOur model did NOT generalize well! This looks terrible!"
  },
  {
    "objectID": "code/rf.html#create-features",
    "href": "code/rf.html#create-features",
    "title": "Packages, Data, Functions",
    "section": "Create Features",
    "text": "Create Features\nNear the beginning of this notebook, we wrote a function to calculate a number of features that may help with road identification. Here, we call our function and inspect some of those layers.\n\n# Create features\nsmall_rgb_layers = compute_features(small_rgb)\n\n\n# Inspect features\nfig, ax = plt.subplots(2, 2, figsize = (8, 8))\nskio.imshow(small_rgb_layers[:,:,6], ax = ax[0,0])\nax[0,0].set_title(\"Range of RGB\")\nskio.imshow(small_rgb_layers[:,:,3], cmap = \"gray\", ax = ax[0,1])\nax[0,1].set_title(\"Canny Edges Red\")\nskio.imshow(small_rgb_layers[:,:,10:13], ax = ax[1,0])\nax[1,0].set_title(\"Gaussian Blur RGB\")\nskio.imshow(small_rgb_layers[:,:,16:19], ax = ax[1,1])\nax[1,1].set_title(\"Log of Gaussian RGB\");\n\n/Users/liamsmith/opt/anaconda3/envs/csci0452/lib/python3.12/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Low image data range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)\n\n\n\n\n\n\n\n\n\nOn the top left is the range of red, green and blue for each pixel. We chose this feature because roads are gray, and in the RGB color space, gray pixels have similar values of red, green and blue.\nOn the top right is canny edges for the red channel. We created this feature hoping to detect the edges of roads, but as you can see, it detects edges in many objects other than roads. We include canny edges in the red, green and blue channels.\nOn the bottom left is the original image after gaussian blurring with \\(\\sigma = 3\\). Our thought process here was that there might be some noise in the image leading random pixels to have the same R, G, and B values as roads. By blurring the image, we hoped to account for this by giving some weight to the values of nearby pixels. We include gaussian blur with \\(\\sigma = 1\\), \\(\\sigma = 3\\), and \\(\\sigma = 5\\) for red, green and blue, hoping that our model might learn from multiple blurring radii.\nOn the bottom right is our image after the log of gaussian filter has been applied to the red, green and blue channels. We hopes to pick up on the width/frequency of roads with this filter, so we included this filter for \\(\\sigma = 0.5\\), \\(\\sigma = 0.6\\), and \\(\\sigma = 0.8\\)."
  },
  {
    "objectID": "code/rf.html#train-model-1",
    "href": "code/rf.html#train-model-1",
    "title": "Packages, Data, Functions",
    "section": "Train Model",
    "text": "Train Model\n\n# Train model\n\n# Flatten image\ntrain_small_rgb_layers = small_rgb_layers.reshape(small_rgb_layers.shape[0]*small_rgb_layers.shape[1], 25)\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel2 = RF.fit(train_small_rgb_layers, y_train)\n\n# Predictions on training data\nmodel2_pred = model2.predict(train_small_rgb_layers)\n\n# Confusion matrix\naccuracy_metrics(y_train, model2_pred)\n\nConfusion matrix:\n [[ 13021      2]\n [     0 106977]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nBefore adding the additional layers to our training data, our overall accuracy was 0.954, precision was 0.854, and recall was 0.69 on our training data. Now we have virtually perfect results! Let’s look at an image of the output.\n\n# Convert predictions to image\ntrain_preds = model2_pred.reshape(small_ans.shape[0], small_ans.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\n\nskio.imshow(train_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans, ax = ax[1])\nax[1].set_title(\"Actual Solution\")\n\nskio.imshow(train_preds==small_ans, ax = ax[2])\nax[2].set_title(\"Errors (look closely)\");\n\n\n\n\n\n\n\n\nYep, can’t even find the errors without looking closely at the difference between the two images. Let’s evaluate our results on the testing data!"
  },
  {
    "objectID": "code/rf.html#test-model-1",
    "href": "code/rf.html#test-model-1",
    "title": "Packages, Data, Functions",
    "section": "Test Model",
    "text": "Test Model\n\n# Create additional features\nsmall_rgb_test_layers = compute_features(small_rgb_test)\n\n\n# Flatten image\ntest_small_rgb_layers = small_rgb_test_layers.reshape(small_rgb_test_layers.shape[0]*small_rgb_test_layers.shape[1], 25)\n\n# Predictions on testing data\nmodel2_test_pred = model2.predict(test_small_rgb_layers)\n\n# Confusion matrix\naccuracy_metrics(y_test, model2_test_pred)\n\nConfusion matrix:\n [[   345   6758]\n [  1914 110983]]\nOverall accuracy: 0.928 \nPrecision: 0.153 \nRecall 0.049 \nDICE: 0.074\n\n\n\n# Convert predictions to image\ntest_preds = model2_test_pred.reshape(small_ans_test.shape[0], small_ans_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\n\nskio.imshow(test_preds, ax = ax[0], cmap = \"gray\")\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans_test, ax = ax[1])\nax[1].set_title(\"Actual Solution\")\n\nskio.imshow(test_preds==small_ans_test, ax = ax[2])\nax[2].set_title(\"Errors\");\n\n\n\n\n\n\n\n\nAdding these filters to our model had negligible impact on our results. It improved the accuracy from 0.906 to 0.927 and the precision from 0.113 to 0.147, but the recall dropped from 0.086 to 0.047. This means that of the pixels that actually represent roads, we are only correctly classifying 4.7% of them. With perfect results on our training data and pitiful results on our testing data, it appears that incorporating these features in our training data led to severe overfitting!"
  },
  {
    "objectID": "code/rf.html#train-rgb-model",
    "href": "code/rf.html#train-rgb-model",
    "title": "Packages, Data, Functions",
    "section": "Train RGB Model",
    "text": "Train RGB Model\nFirst, let’s use this method on a model with just RGB layers.\n\n# Flatten training images\ntrain_rgb = rgb.reshape(rgb.shape[0]*rgb.shape[1], 3)\ny_train = ans.reshape(ans.shape[0]*ans.shape[1])\n\n# Subset training data by label\ny_train_true = y_train[y_train]\ny_train_false = y_train[~y_train]\ntrain_rgb_true = train_rgb[y_train]\ntrain_rgb_false = train_rgb[~y_train]\n\n# Sample indices of each label\ntrue_indices = sample_without_replacement(y_train_true.shape[0], 10000)\nfalse_indices = sample_without_replacement(y_train_false.shape[0], 10000)\n\n# Create modified training data\ny_train_mod = np.concatenate([y_train_true[true_indices[:5000]], y_train_false[false_indices[:5000]]])\ntrain_rgb_mod = np.concatenate([train_rgb_true[true_indices[:5000]], train_rgb_false[false_indices[:5000]]])\n\n# Create modified testing data\ny_test_mod = np.concatenate([y_train_true[true_indices[5000:]], y_train_false[false_indices[5000:]]])\ntest_rgb_mod = np.concatenate([train_rgb_true[true_indices[5000:]], train_rgb_false[false_indices[5000:]]])\n\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel3 = RF.fit(train_rgb_mod, y_train_mod)\n\n# Predictions on training data\nmodel3_pred = model3.predict(train_rgb_mod)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model3_pred)\n\nConfusion matrix:\n [[4969   31]\n [  81 4919]]\nOverall accuracy: 0.989 \nPrecision: 0.984 \nRecall 0.994 \nDICE: 0.989\n\n\nWhile this model does not have 100% training accuracy like the additional layers model, it has improved significantly over the original RGB model. Most notably, the training recall has improved from less than 70% to roughly 99%. Let’s see if we maintain this performance when we make predictions on our testing data."
  },
  {
    "objectID": "code/rf.html#test-rgb-model",
    "href": "code/rf.html#test-rgb-model",
    "title": "Packages, Data, Functions",
    "section": "Test RGB Model",
    "text": "Test RGB Model\n\n# Predictions on testing data\nmodel3_test_pred = model3.predict(test_rgb_mod)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model3_test_pred)\n\nConfusion matrix:\n [[3724 1276]\n [1234 3766]]\nOverall accuracy: 0.749 \nPrecision: 0.751 \nRecall 0.745 \nDICE: 0.748\n\n\nOur results are encouraging! Our overall accuracy, precision, and recall are all approximatly 0.75. In the original RGB model, the overall accuracy was over 90%, while the precision and recall were roughly 10%. By balancing the amount of training data in each class, we were able to balance the different accuracy metrics, improving our predictions of roads at the expense of our predictions of non-roads. Perhaps if we incorporate our additional layers into the model, these balance improvements will translate to balanced and higher accuracy metrics."
  },
  {
    "objectID": "code/rf.html#test-rgb-model-on-new-image",
    "href": "code/rf.html#test-rgb-model-on-new-image",
    "title": "Packages, Data, Functions",
    "section": "Test RGB Model on New Image",
    "text": "Test RGB Model on New Image\nWhile our results above are encouraging, our training and testing data were both drawn from the same image, so our model may have overtrained to this image. Let’s form predictions and compute accuracy metrics on a different image.\n\n# Flatten testing images\nflat_rgb_test = rgb_test.reshape(rgb_test.shape[0]*rgb_test.shape[1], 3)\ny_test = ans_test.reshape(ans_test.shape[0]*ans_test.shape[1])\n\n# Predictions on testing data\nmodel3_test_pred_2 = model3.predict(flat_rgb_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model3_test_pred_2)\n\nConfusion matrix:\n [[  99858   40407]\n [ 329894 1779841]]\nOverall accuracy: 0.835 \nPrecision: 0.232 \nRecall 0.712 \nDICE: 0.35\n\n\nSurprisingly, the overall accuracy is higher in the testing image than in the training image! The recall is still over 70%, indicating that we are capturing most pixels representing roads correctly. With a much lower precision, we must be predicting road pixels frequently where there are not actually roads.\nSince we are working with an entire image, we can inspect our results!\n\n# Convert predictions to image\ntest_preds = model3_test_pred_2.reshape(rgb_test.shape[0], rgb_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\nIt looks like we tend to label pixels as roads when they in reality represent other human features like buildings. We also exagerate the width of some roads"
  },
  {
    "objectID": "code/rf.html#train-additional-layers-model",
    "href": "code/rf.html#train-additional-layers-model",
    "title": "Packages, Data, Functions",
    "section": "Train Additional Layers Model",
    "text": "Train Additional Layers Model\n\n# Create additional features\ntrain_rgb_mod_layers = compute_features(rgb)\n\n# Flatten training image with extra layers\ntrain_rgb_2 = train_rgb_mod_layers.reshape(train_rgb_mod_layers.shape[0]*train_rgb_mod_layers.shape[1], 25)\n\n# Subset training data by label\ntrain_rgb_true_2 = train_rgb_2[y_train]\ntrain_rgb_false_2 = train_rgb_2[~y_train]\n\n# Create modified training data\ntrain_rgb_mod_2 = np.concatenate([train_rgb_true_2[true_indices[:5000]], train_rgb_false_2[false_indices[:5000]]])\n\n# Create modified testing data\ntest_rgb_mod_2 = np.concatenate([train_rgb_true_2[true_indices[5000:]], train_rgb_false_2[false_indices[5000:]]])\n\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel4 = RF.fit(train_rgb_mod_2, y_train_mod)\n\n# Predictions on training data\nmodel4_pred = model4.predict(train_rgb_mod_2)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model4_pred)\n\nConfusion matrix:\n [[5000    0]\n [   0 5000]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nOur training results are literally perfect. Does this translate to our testing data?"
  },
  {
    "objectID": "code/rf.html#test-additional-layers-model",
    "href": "code/rf.html#test-additional-layers-model",
    "title": "Packages, Data, Functions",
    "section": "Test Additional Layers Model",
    "text": "Test Additional Layers Model\n\n# Predictions on testing data\nmodel4_test_pred = model4.predict(test_rgb_mod_2)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model4_test_pred)\n\nConfusion matrix:\n [[4067  933]\n [1040 3960]]\nOverall accuracy: 0.803 \nPrecision: 0.796 \nRecall 0.813 \nDICE: 0.805\n\n\nIt appears that there were some errors on our testing data. Going from the RGB model to the additional layers model, our overall accuracy improved from 0.755 to 0.812, the precision improved from 0.749 to 0.796, and the recall improved from 0.767 to 0.84. These are the most accurate road predictions yet!\nWhile our training and testing data contained none of the same pixels, they were both drawn from the same image, so it is possible that they were overtrained to our particular image of choice. Perhaps a more valid testing metric would involve testing our model on pixels from a different image. Let’s form predictions and compute accuracy metrics on the entirety of another image."
  },
  {
    "objectID": "code/rf.html#test-additional-layers-model-on-new-image",
    "href": "code/rf.html#test-additional-layers-model-on-new-image",
    "title": "Packages, Data, Functions",
    "section": "Test Additional Layers Model on New Image",
    "text": "Test Additional Layers Model on New Image\n\n# Create additional features\ntest_rgb_layers_3 = compute_features(rgb_test)\n\n# Flatten testing images\ntest_rgb_3 = test_rgb_layers_3.reshape(test_rgb_layers_3.shape[0]*test_rgb_layers_3.shape[1], 25)\n\n\n# Predictions on testing data\nmodel4_test_pred_2 = model4.predict(test_rgb_3)\n\n# Confusion matrix\naccuracy_metrics(y_test, model4_test_pred_2)\n\nConfusion matrix:\n [[  86978   53287]\n [ 234460 1875275]]\nOverall accuracy: 0.872 \nPrecision: 0.271 \nRecall 0.62 \nDICE: 0.377\n\n\nSimilar to the RGB model, the results on the testing image were largely similar to the results on the previous image, except for the precision dropping by over 50% The overall accuracy is over 85%, but the recall is now 65.9% and the precision is now 26.6%. While this is certainly not perfect, the precision and recall are still a substantial improvement over the models without sampling. However, the recall was actually slightly higher in the sampled RGB model, indicating that the RGB model generalized better in terms of predicting road pixels. Perhaps there are tactics we can use to combat overfitting.\nAlso, since we are now working with a complete image, we can once again inspect a full image illustrating our predictions versus the truth.\n\n# Convert predictions to image\ntest_preds = model4_test_pred_2.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\nOnce again, our model tends to incorrectly predict roads where there are other human features like buildings, and it exaggerates the width of some roads."
  },
  {
    "objectID": "code/rf.html#train-model-2",
    "href": "code/rf.html#train-model-2",
    "title": "Packages, Data, Functions",
    "section": "Train Model",
    "text": "Train Model\nFirst, we apply Principal Component Analysis to our training data and plot the percent variance explained by each component. Note that PCA is only applicable for continuous features, so we cannot include binary features such as canny edges in this model.\n\n# Add layers to model\ntrain_pca_layers = compute_features(rgb, include_categorical = False)   \n\n# Flatten training image with extra layers\ntrain_pca_layers_flat = train_pca_layers.reshape(train_pca_layers.shape[0]*train_pca_layers.shape[1], 22)\n\n# Standardize the features\nscaler = StandardScaler()\ntrain_pca_layers_scaled = scaler.fit_transform(train_pca_layers_flat)\n\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=22)\nlayers_pca = pca.fit_transform(train_pca_layers_scaled)\n\n# Explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Plotting the explained variance ratio\nplt.figure(figsize=(8, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\nApparently, over 50% of the variance in our data can be explained by the first component! The second component only accounts for about 11.5% of the variance in the data, and the numbers continue to drop after that.\n\nexplained_variance_ratio[0:5].sum()\n\n0.843420909342875\n\n\nApparently, the first 5 components account for over 84% of the variation in our data. Let’s try only retaining the first 5 components for our model and seeing whether our performance improves.\n\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=5)\nlayers_pca = pca.fit_transform(train_pca_layers_scaled)\n\n# Subset training data by label\nlayers_pca_true = layers_pca[y_train]\nlayers_pca_false = layers_pca[~y_train]\n\n# Create modified training data\nlayers_pca_mod_train = np.concatenate([layers_pca_true[true_indices[:5000]], layers_pca_false[false_indices[:5000]]])\n\n# Create modified testing data\nlayers_pca_mod_test = np.concatenate([layers_pca_true[true_indices[5000:]], layers_pca_false[false_indices[5000:]]])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel5 = RF.fit(layers_pca_mod_train, y_train_mod)\n\n# Predictions on training data\nmodel5_pred = model5.predict(layers_pca_mod_train)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model5_pred)\n\nConfusion matrix:\n [[5000    0]\n [   0 5000]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nPer usual, our model’s performance is perfect on the training data."
  },
  {
    "objectID": "code/rf.html#test-model-2",
    "href": "code/rf.html#test-model-2",
    "title": "Packages, Data, Functions",
    "section": "Test Model",
    "text": "Test Model\n\n# Predictions on testing data\nmodel5_test_pred = model5.predict(layers_pca_mod_test)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model5_test_pred)\n\nConfusion matrix:\n [[3837 1163]\n [1436 3564]]\nOverall accuracy: 0.74 \nPrecision: 0.728 \nRecall 0.767 \nDICE: 0.747\n\n\nOn the testing data, all of our model’s performance metrics are lower than its non-PCA counterpart, although not by that much."
  },
  {
    "objectID": "code/rf.html#test-model-on-new-image",
    "href": "code/rf.html#test-model-on-new-image",
    "title": "Packages, Data, Functions",
    "section": "Test Model on New Image",
    "text": "Test Model on New Image\n\n# Add layers to model\ntest_pca_layers = compute_features(rgb_test, include_categorical = False)\n\n# Flatten testing images\ntest_pca_layers_flat = test_pca_layers.reshape(test_pca_layers.shape[0]*test_pca_layers.shape[1], 22)\n\n# Standardize the features\ntest_pca_layers_scaled = scaler.fit_transform(test_pca_layers_flat)\n\n# Project onto principal components\nlayers_pca_test = pca.transform(test_pca_layers_scaled)\n\n\n# Predictions on testing data\nmodel5_test_pred_2 = model5.predict(layers_pca_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model5_test_pred_2)\n\nConfusion matrix:\n [[  79489   60776]\n [ 535222 1574513]]\nOverall accuracy: 0.735 \nPrecision: 0.129 \nRecall 0.567 \nDICE: 0.211\n\n\nAgain, on the testing image, all of our model’s performance metrics are lower than its non-PCA counterpart. In this scenario, it appears that the components explaining very little variation in the data were actually somewhat useful for predictions. Note that we tried this with a variety of number of retained components, and we found that the model’s performance improved as we increased the number of components.\nBelow, we inspect the image of our predictions.\n\n# Convert predictions to image\ntest_preds = model5_test_pred_2.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\nVisually, our predictions appear somewhat worse that those from our non-PCA additional layers model. There is overall a lot more noise in our predictions, and interestingly, we are not predicting a road in much of the massive highway."
  },
  {
    "objectID": "code/rf.html#training-sample-rgb",
    "href": "code/rf.html#training-sample-rgb",
    "title": "Packages, Data, Functions",
    "section": "Training Sample RGB",
    "text": "Training Sample RGB\n\n# Store id's of images\nimgs = [\"10828735_15\", \"10228675_15\", \"10228705_15\", \"10228720_15\", \"10228735_15\", \"10528675_15\", \"10528750_15\", \"10978720_15\", \"11128825_15\", \"12028750_15\"]\n\n# Initialize arrays to store training data\ny_train = np.array([])\nrgb_train = np.zeros((0,3))\n\n# Sample from each image\nfor img in imgs:\n    rgb = skio.imread(\"../../data/MA_roads/tiff/train/\" + img + \".tiff\")\n    ans = skio.imread(\"../../data/MA_roads/tiff/train_labels/\" + img + \".tif\") &gt; 0\n    \n    # Flatten training images\n    rgb_flat = rgb.reshape(rgb.shape[0]*rgb.shape[1], 3)\n    ans_flat = ans.reshape(ans.shape[0]*ans.shape[1])\n    \n    # Subset training data by label\n    ans_true = ans_flat[ans_flat]\n    ans_false = ans_flat[~ans_flat]\n    rgb_true = rgb_flat[ans_flat]\n    rgb_false = rgb_flat[~ans_flat]\n    \n    # Sample indices of each label\n    true_indices = sample_without_replacement(ans_true.shape[0], 5000)\n    false_indices = sample_without_replacement(ans_false.shape[0], 5000)\n    \n    # Create modified training data\n    y_train = np.concatenate([y_train, ans_true[true_indices], ans_false[false_indices]])\n    rgb_train = np.concatenate([rgb_train, rgb_true[true_indices], rgb_false[false_indices]])"
  },
  {
    "objectID": "code/rf.html#train-rgb-model-1",
    "href": "code/rf.html#train-rgb-model-1",
    "title": "Packages, Data, Functions",
    "section": "Train RGB Model",
    "text": "Train RGB Model\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel6 = RF.fit(rgb_train, y_train)\n\n# Predictions on training data\nmodel6_pred = model6.predict(rgb_train)\n\n# Confusion matrix\naccuracy_metrics(y_train, model6_pred)\n\nConfusion matrix:\n [[48279  1721]\n [ 2575 47425]]\nOverall accuracy: 0.957 \nPrecision: 0.949 \nRecall 0.966 \nDICE: 0.957"
  },
  {
    "objectID": "code/rf.html#test-rgb-model-on-new-image-1",
    "href": "code/rf.html#test-rgb-model-on-new-image-1",
    "title": "Packages, Data, Functions",
    "section": "Test RGB Model on New Image",
    "text": "Test RGB Model on New Image\n\n# Predictions on testing data\nmodel6_test_pred = model6.predict(flat_rgb_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model6_test_pred)\n\nConfusion matrix:\n [[ 109928   30337]\n [ 385742 1723993]]\nOverall accuracy: 0.815 \nPrecision: 0.222 \nRecall 0.784 \nDICE: 0.346\n\n\n\n# Convert predictions to image\ntest_preds = model6_test_pred.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");"
  },
  {
    "objectID": "code/rf.html#training-sample-additional-layers",
    "href": "code/rf.html#training-sample-additional-layers",
    "title": "Packages, Data, Functions",
    "section": "Training Sample Additional Layers",
    "text": "Training Sample Additional Layers\n\n# Store id's of images\nimgs = [\"10828735_15\", \"10228675_15\", \"10228705_15\", \"10228720_15\", \"10228735_15\", \"10528675_15\", \"10528750_15\", \"10978720_15\", \"11128825_15\", \"12028750_15\"]\n\n# Initialize arrays to store training data\ny_train = np.array([])\nrgb_train = np.zeros((0,25))\n\n# Sample from each image\nfor img in imgs:\n    rgb = skio.imread(\"../../data/MA_roads/tiff/train/\" + img + \".tiff\")\n    ans = skio.imread(\"../../data/MA_roads/tiff/train_labels/\" + img + \".tif\") &gt; 0\n\n    # Create additional layers\n    rgb_layers = compute_features(rgb)\n    \n    # Flatten training images\n    rgb_flat = rgb_layers.reshape(rgb_layers.shape[0]*rgb_layers.shape[1], rgb_layers.shape[2])\n    ans_flat = ans.reshape(ans.shape[0]*ans.shape[1])\n    \n    # Subset training data by label\n    ans_true = ans_flat[ans_flat]\n    ans_false = ans_flat[~ans_flat]\n    rgb_true = rgb_flat[ans_flat]\n    rgb_false = rgb_flat[~ans_flat]\n    \n    # Sample indices of each label\n    true_indices = sample_without_replacement(ans_true.shape[0], 5000)\n    false_indices = sample_without_replacement(ans_false.shape[0], 5000)\n    \n    # Create modified training data\n    y_train = np.concatenate([y_train, ans_true[true_indices], ans_false[false_indices]])\n    rgb_train = np.concatenate([rgb_train, rgb_true[true_indices], rgb_false[false_indices]])"
  },
  {
    "objectID": "code/rf.html#train-additional-layers-model-1",
    "href": "code/rf.html#train-additional-layers-model-1",
    "title": "Packages, Data, Functions",
    "section": "Train Additional Layers Model",
    "text": "Train Additional Layers Model\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel7 = RF.fit(rgb_train, y_train)\n\n# Predictions on training data\nmodel7_pred = model7.predict(rgb_train)\n\n# Confusion matrix\naccuracy_metrics(y_train, model7_pred)\n\nConfusion matrix:\n [[49998     2]\n [    0 50000]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0"
  },
  {
    "objectID": "code/rf.html#test-additional-layers-model-on-new-image-1",
    "href": "code/rf.html#test-additional-layers-model-on-new-image-1",
    "title": "Packages, Data, Functions",
    "section": "Test Additional Layers Model on New Image",
    "text": "Test Additional Layers Model on New Image\n\n# Predictions on testing data\nmodel7_test_pred = model7.predict(test_rgb_3)\n\n# Confusion matrix\naccuracy_metrics(y_test, model7_test_pred)\n\nConfusion matrix:\n [[ 116138   24127]\n [ 324661 1785074]]\nOverall accuracy: 0.845 \nPrecision: 0.263 \nRecall 0.828 \nDICE: 0.4\n\n\n\n# Convert predictions to image\ntest_preds = model7_test_pred.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");"
  },
  {
    "objectID": "pdal_tutorial/pdal_tutorial.html",
    "href": "pdal_tutorial/pdal_tutorial.html",
    "title": "",
    "section": "",
    "text": "From https://pdal.io/en/2.6.0/tutorial/iowa-entwine.html#install-pdal\nFirst, follow their instructions for installing pdal.\nThen run the code below (I copied their iowa.json and dem-colors.txt files)\n\n!pdal pipeline iowa.json --debug\n\n\n!gdaldem color-relief iowa.tif dem-colors.txt iowa-color.png\n\nPDAL is a C++ library for handling point cloud data. PDAL provides some Python support and understanding that more completely would require reading their documentation: https://pdal.io/en/2.7-maintenance/python.html#python"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Road Segmentation",
    "section": "",
    "text": "Title\nThis is a test\n\n1 + 1\n\n2\n\n\n\nfor i in range(10):\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9"
  }
]