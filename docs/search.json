[
  {
    "objectID": "SAT Segmentation_Oh.html",
    "href": "SAT Segmentation_Oh.html",
    "title": "",
    "section": "",
    "text": "import os\nimport  cv2\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom patchify import patchify\nfrom PIL import Image\nimport segmentation_models as sm\nfrom tensorflow.keras.metrics import MeanIoU\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nscaler = MinMaxScaler()\n\nroot_directory = 'Semantic segmentation dataset/'\n\npatch_size = 256\n\nModuleNotFoundError: No module named 'cv2'\n\n\n\nimport opendatasets as od\nimport pandas\n\n\nod.download(\"https://www.kaggle.com/datasets/balraj98/massachusetts-roads-dataset\")\n\nModuleNotFoundError: No module named 'opendatasets'\n\n\n\ndef patches\n    image_dataset = []  \n    for path, subdirs, files in os.walk(root_directory):\n        #print(path)  \n        dirname = path.split(os.path.sep)[-1]\n        if dirname == 'images':   #Find all 'images' directories\n            images = os.listdir(path)  #List of all image names in this subdirectory\n            for i, image_name in enumerate(images):  \n                if image_name.endswith(\".jpg\"):   #Only read jpg images...\n\n                    image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n                    SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n                    SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n                    image = Image.fromarray(image)\n                    image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n                    #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n                    image = np.array(image)             \n\n                    #Extract patches from each image\n                    print(\"Now patchifying image:\", path+\"/\"+image_name)\n                    patches_img = patchify(image, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n\n                    for i in range(patches_img.shape[0]):\n                        for j in range(patches_img.shape[1]):\n\n                            single_patch_img = patches_img[i,j,:,:]\n\n                            #Use minmaxscaler instead of just dividing by 255. \n                            single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape)\n\n                            #single_patch_img = (single_patch_img.astype('float32')) / 255. \n                            single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.                               \n                            image_dataset.append(single_patch_img)"
  },
  {
    "objectID": "segmentation_tutorial/segmentation.html",
    "href": "segmentation_tutorial/segmentation.html",
    "title": "",
    "section": "",
    "text": "From https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/?utm_source=blog&utm_medium=computer-vision-implementing-mask-r-cnn-image-segmentation#2"
  },
  {
    "objectID": "segmentation_tutorial/segmentation.html#threshold-segmentation",
    "href": "segmentation_tutorial/segmentation.html#threshold-segmentation",
    "title": "",
    "section": "Threshold Segmentation",
    "text": "Threshold Segmentation\nMost basic form of segmentation: threshold segmentation\nIf we have two classes, we define a single global threshold. If we have multiple classes, we define several local thresholds.\n\n# Import packages\nfrom skimage.color import rgb2gray\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy import ndimage\n\n\n# Read and display image\nimage = plt.imread('1.jpeg')\nplt.imshow(image);\nprint(image.shape)\n\n(192, 263, 3)\n\n\n\n\n\n\n\n\n\n\n# Convert to grayscale to illustrate thresholding\ngray = rgb2gray(image)\n# cv2.imshow('img',gray)\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()\nplt.imshow(gray, cmap = \"gray\");\n\n\n\n\n\n\n\n\n\n# Show shape\ngray.shape\n\n(192, 263)\n\n\n\n# Threshold using mean value\n    # no need for a loop...\ngray_r = gray.reshape(gray.shape[0]*gray.shape[1])\nfor i in range(gray_r.shape[0]):\n    if gray_r[i] &gt; gray_r.mean():\n        gray_r[i] = 1\n    else:\n        gray_r[i] = 0\ngray = gray_r.reshape(gray.shape[0],gray.shape[1])\nplt.imshow(gray, cmap='gray')\n\n\n\n\n\n\n\n\n\n# Threshold with several values\n    # no need for a loop...\ngray = rgb2gray(image)\ngray_r = gray.reshape(gray.shape[0]*gray.shape[1])\nfor i in range(gray_r.shape[0]):\n    if gray_r[i] &gt; gray_r.mean():\n        gray_r[i] = 3\n    elif gray_r[i] &gt; 0.5:\n        gray_r[i] = 2\n    elif gray_r[i] &gt; 0.25:\n        gray_r[i] = 1\n    else:\n        gray_r[i] = 0\ngray = gray_r.reshape(gray.shape[0],gray.shape[1])\nplt.imshow(gray, cmap='gray')\n\n\n\n\n\n\n\n\nAdvantages: - Easy to implement - Runs fast - Works well if contrast is high\nLimitations: - Only uses grayscale - Performs bad if low grayscale contrast or grayscale value overlap"
  },
  {
    "objectID": "segmentation_tutorial/segmentation.html#edge-detection-segmentation",
    "href": "segmentation_tutorial/segmentation.html#edge-detection-segmentation",
    "title": "",
    "section": "Edge detection segmentation",
    "text": "Edge detection segmentation\nFirst convolve the image with a kernel that detects edges. Then threshold (presumably).\n\n# Read and display image\nimage = plt.imread('index.png')\nplt.imshow(image);\n\n\n\n\n\n\n\n\n\n# Convert to grayscale\ngray = rgb2gray(image[:,:,:3])\n\n# Defining the sobel filters (specific kernels for extracting edges)\nsobel_horizontal = np.array([[1, 2, 1],[0, 0, 0],[-1, -2, -1]])\nprint(sobel_horizontal, 'is a kernel for detecting horizontal edges')\n \nsobel_vertical = np.array([[-1, 0, 1],[-2, 0, 2],[-1, 0, 1]])\nprint(sobel_vertical, 'is a kernel for detecting vertical edges')\n\n[[ 1  2  1]\n [ 0  0  0]\n [-1 -2 -1]] is a kernel for detecting horizontal edges\n[[-1  0  1]\n [-2  0  2]\n [-1  0  1]] is a kernel for detecting vertical edges\n\n\n\n# Convolve\nout_h = ndimage.convolve(gray, sobel_horizontal, mode='reflect')\nout_v = ndimage.convolve(gray, sobel_vertical, mode='reflect')\n# here mode determines how the input array is extended when the filter overlaps a border.\n\n# Show results\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].imshow(out_h, cmap='gray')\nax[1].imshow(out_v, cmap='gray');\n\n\n\n\n\n\n\n\n\n# The laplacer operator/kernel can detect both horizontal and vertical edges\nkernel_laplace = np.array([np.array([1, 1, 1]), np.array([1, -8, 1]), np.array([1, 1, 1])])\nprint(kernel_laplace, 'is a laplacian kernel')\n\n[[ 1  1  1]\n [ 1 -8  1]\n [ 1  1  1]] is a laplacian kernel\n\n\n\n# Convolve, show results\nout_l = ndimage.convolve(gray, kernel_laplace, mode='reflect')\nplt.imshow(out_l, cmap='gray');"
  },
  {
    "objectID": "segmentation_tutorial/segmentation.html#clustering-segmentation",
    "href": "segmentation_tutorial/segmentation.html#clustering-segmentation",
    "title": "",
    "section": "Clustering segmentation",
    "text": "Clustering segmentation\nLet’s try segmentation with k-means clustering\n\n# Read and display\npic = plt.imread('1.jpeg')/255  # dividing by 255 to bring the pixel values between 0 and 1\nprint(pic.shape)\nplt.imshow(pic);\n\n(192, 263, 3)\n\n\n\n\n\n\n\n\n\n\n# Reshape to have row for each pixel, column for each channel (we will not be doing anything spatial)\npic_n = pic.reshape(pic.shape[0]*pic.shape[1], pic.shape[2])\npic_n.shape\n\n(50496, 3)\n\n\n\n# Apply k-means clustering\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=5, random_state=0).fit(pic_n) \npic2show = kmeans.cluster_centers_[kmeans.labels_] # returns clusters centers\n\n\n# Display results\ncluster_pic = pic2show.reshape(pic.shape[0], pic.shape[1], pic.shape[2])\nplt.imshow(cluster_pic);\n\n\n\n\n\n\n\n\nK-means often performs well on small datasets!\nBut not very well on large numbers of images."
  },
  {
    "objectID": "segmentation_tutorial/segmentation.html#mask-r-cnn-segmentation",
    "href": "segmentation_tutorial/segmentation.html#mask-r-cnn-segmentation",
    "title": "",
    "section": "Mask R-CNN segmentation",
    "text": "Mask R-CNN segmentation\nThis is an extension of the Faster R-CNN method, which identifies the class and a bounding box for each object. The benefit of Mask R-CNN is that it adds a pixel-wise mask as well.\nThe first tutorial briefly discusses how this works, but does not include Python code for doing it ourselves.\nI think the implementation is included in the next tutorial! https://www.analyticsvidhya.com/blog/2019/07/computer-vision-implementing-mask-r-cnn-image-segmentation/\nLet’s work through the second tutorial now.\nIncludes a helpful graphic for distinguishing between Faster R-CNN and Mask R-CNN.\nBriefly explains how Faster R-CNN works (I didn’t understand this).\nExplains more in-depth how Mask R-CNN works (I also didn’t understand this).\nTraining time for CNNs is quite high. It took the author of the tutorial 1-2 days to train their model. Hence we use a pre-trained model below.\n\n# Clone repository with the architecture for RCNN\n!git clone https://github.com/matterport/Mask_RCNN.git\n\nCloning into 'Mask_RCNN'...\nremote: Enumerating objects: 956, done.\nremote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\nReceiving objects: 100% (956/956), 137.67 MiB | 16.08 MiB/s, done.\nResolving deltas: 100% (558/558), done.\n\n\n\n# Install required packages\n!pip install cython tensorflow keras opencv-python h5py imgaug\n\nCollecting cython\n  Downloading Cython-3.0.10-cp312-cp312-macosx_10_9_x86_64.whl.metadata (3.2 kB)\nDownloading Cython-3.0.10-cp312-cp312-macosx_10_9_x86_64.whl (3.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 12.9 MB/s eta 0:00:0000:0100:01\nInstalling collected packages: cython\nSuccessfully installed cython-3.0.10\n\n\nDownload pre-trained weights: https://github.com/matterport/Mask_RCNN/releases\nI went with the 2.0 version, downloading just mask_rcnn_coco.h5.\nThen place the download in the samples folder of the cloned GitHub repo.\nNow look in the samples folder for part 2.\nLook at level sets and active contours?"
  },
  {
    "objectID": "code/k_means.html",
    "href": "code/k_means.html",
    "title": "Testing K-means Clustering",
    "section": "",
    "text": "Imports\n\nimport skimage.io as skio\nimport skimage.util as sku\nimport skimage.color as skol\nfrom skimage import filters, feature, transform\nimport skimage.morphology as skimor\nimport skimage.draw as draw\nfrom scipy.signal import convolve2d\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d\nimport numpy as np\nimport cv2\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.ndimage import gaussian_filter, gaussian_laplace\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import datasets\nimport tensorflow.keras.backend as K\nimport random\n\n\nrandom.seed(10)\n\n\n\n\n\nimage_filepath = \"../../data/massRoads/tiff\"\n# /train/... for images and /train_labels/... for labels\n\n\nfull_image = skio.imread(image_filepath + \"/train/21929005_15.tiff\")\nfull_image_label = skio.imread(image_filepath + \"/train_labels/21929005_15.tif\")\n\n\nfig, axes = plt.subplots(1,2, figsize = (10,15))\n\naxes[0].imshow(full_image)\naxes[1].imshow(full_image_label, cmap='gray')\n\nplt.show()\n\n\n\n\n\n\n\n\nCropping Image for testing\n\nx1, x2 = 0, 1500\ny1, y2 = 0, 1500\n# x1, x2 = 1500, 1500\n# y1, y2 = 1500, 1500\nimage = full_image[y1:y2, x1:x2, :]\nimage_label = full_image_label[y1:y2, x1:x2]\n\n\nfig, axes = plt.subplots(1,2, figsize = (10,15))\n\naxes[0].imshow(image)\naxes[1].imshow(image_label, cmap='gray')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = np.sum(y_true * y_pred)\n    return (2. * intersection + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)\n\n\n# Function to print several accuracy metrics\ndef accuracy_metrics(y_true, y_pred):\n    # Create confusion matrix\n    C = confusion_matrix(y_true, y_pred, labels=(True, False))\n\n    # Overall accuracy rate\n    acc = (C[0,0] + C[1,1])/C.sum()\n\n    # Recall\n    recall = (C[0,0])/(C[0,0] + C[1,0])\n    \n    # Precision\n    prec = (C[0,0])/(C[0,0] + C[0,1])\n\n    dice = dice_coef(y_true=y_true, y_pred=y_pred)\n\n    # Print results\n    print(\"Confusion matrix:\\n\", C)\n    print(\"Overall accuracy:\", np.round(acc, 3), \"\\nPrecision:\", np.round(recall, 3), \"\\nRecall\", np.round(prec, 3), \"\\nDice Coefficient\", np.round(dice, 3)) \n\n\n#what values are used in label image\nnp.unique(image_label)\n\narray([  0, 255], dtype=uint8)\n\n\n\n\n\n\npixel_vals = image.reshape((-1,3))\npixel_vals = np.float32(pixel_vals)\n\n\n\n\n\nsk_kmeans = KMeans(n_clusters=3, verbose=1).fit(pixel_vals)\n\nInitialization complete\nIteration 0, inertia 4702888960.0.\nIteration 1, inertia 3000623104.0.\nIteration 2, inertia 2794777600.0.\nIteration 3, inertia 2693770496.0.\nIteration 4, inertia 2641820160.0.\nIteration 5, inertia 2614184448.0.\nIteration 6, inertia 2598637824.0.\nIteration 7, inertia 2589610752.0.\nIteration 8, inertia 2584902144.0.\nIteration 9, inertia 2582124544.0.\nIteration 10, inertia 2580577792.0.\nIteration 11, inertia 2579322624.0.\nIteration 12, inertia 2578926336.0.\nConverged at iteration 12: center shift 0.21074505150318146 within tolerance 0.24255793457031252.\n\n\n\nsk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n\nskio.imshow(sk_segmented_image_pca, cmap='gray')\n\n/Users/aidenpape/miniconda3/lib/python3.11/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Low image data range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s try to put a horizontal and vertically run edge detection using Sobel kernels\n\nimage_gray = skol.rgb2gray(image)\n\n# Define Sobel kernels\nsobel_horizontal = np.array([[-1, -2, -1],\n                             [ 0,  0,  0],\n                             [ 1,  2,  1]])\n\nsobel_vertical = np.array([[-1, 0, 1],\n                           [-2, 0, 2],\n                           [-1, 0, 1]])\n\n# Perform convolutions\nedges_horizontal = convolve2d(image_gray, sobel_horizontal, mode='same', boundary='symm')\nedges_vertical = convolve2d(image_gray, sobel_vertical, mode='same', boundary='symm')\n\n# Calculate the magnitude of the gradient\nedges = np.sqrt(np.square(edges_horizontal) + np.square(edges_vertical))\n\n\nskio.imshow(edges, cmap='gray')\n\n/Users/aidenpape/miniconda3/lib/python3.11/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Float image out of standard range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Calculation Canny gradient\ncanny_edges = feature.canny(image_gray, sigma=3)\n\n\n\n\n# Create disk\ndisk = skimor.disk(1)\n\n# Area closing\nclosed_edges = skimor.dilation(canny_edges, footprint = disk)\nclosed_edges = closed_edges * 255\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\nax = axes.ravel()\n\nax[0].imshow(canny_edges, cmap='gray')\nax[0].set_title('Canny Edges')\n\nax[1].imshow(closed_edges, cmap='gray')\nax[1].set_title('Closed Canny edges')\n\nax[2].imshow(image)\nax[2].set_title('Original Image')\n\nfor a in ax:\n    a.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNow lets add this new channel to the image and see what happens\n\nimage_with_canny = np.dstack((image, closed_edges.astype(np.uint8) * 255))\n\n\nfeature_vals = image_with_canny.reshape((-1,4))\nfeature_vals = np.float32(feature_vals)\n\n\nsk_kmeans = KMeans(n_clusters=3, verbose=1).fit(feature_vals)\n\nInitialization complete\nIteration 0, inertia 3043438080.0.\nIteration 1, inertia 2653675520.0.\nIteration 2, inertia 2619363840.0.\nIteration 3, inertia 2601338880.0.\nIteration 4, inertia 2591492864.0.\nIteration 5, inertia 2585991424.0.\nIteration 6, inertia 2582735616.0.\nIteration 7, inertia 2581043200.0.\nIteration 8, inertia 2579904768.0.\nIteration 9, inertia 2579204608.0.\nIteration 10, inertia 2578784256.0.\nConverged at iteration 10: center shift 0.1291225403547287 within tolerance 0.18192149658203127.\n\n\n\nsk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n\nskio.imshow(sk_segmented_image_pca, cmap='gray')\n\n/Users/aidenpape/miniconda3/lib/python3.11/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Low image data range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)\n\n\n\n\n\n\n\n\n\n\nnp.unique(sk_segmented_image_pca)\n\narray([0, 1, 2], dtype=int32)\n\n\n\n# Generating figure 2\nfig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n\ncluster_one = (sk_segmented_image_pca==0)\naxes[0][0].imshow(cluster_one, cmap='gray')\naxes[0][0].set_title('Cluster One')\n\ncluster_two = (sk_segmented_image_pca==1)\naxes[0][1].imshow(cluster_two, cmap='gray')\naxes[0][1].set_title('Cluster Two')\n\ncluster_three = (sk_segmented_image_pca==2)\naxes[1][0].imshow(cluster_three, cmap='gray')\naxes[1][0].set_title('Cluster Three')\n\naxes[1][1].imshow(image)\naxes[1][1].set_title('Original Image')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nlines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\nax = axes.ravel()\n\nax[0].imshow(image_gray, cmap='gray')\nax[0].set_title('Input image')\n\nax[1].imshow(canny_edges, cmap='gray')\nax[1].set_title('Canny edges')\n\nax[2].imshow(canny_edges * 0)\nfor line in lines:\n    p0, p1 = line\n    ax[2].plot((p0[0], p1[0]), (p0[1], p1[1]))\nax[2].set_xlim((0, image_gray.shape[1]))\nax[2].set_ylim((image_gray.shape[0], 0))\nax[2].set_title('Probabilistic Hough')\n\nfor a in ax:\n    a.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create a blank canvas to draw lines on\nhough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n\n# Draw the detected lines on the canvas\nfor line in lines:\n    p0, p1 = line\n    # Draw line segment\n    rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n    hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n\n# # Display the canvas with the detected lines\n# plt.imshow(hough_lines, cmap='gray')\n# plt.title('Image with Detected Lines')\n# plt.axis('off')\n# plt.show()\n\n\n\n\n\n\ndef classify_gray(image):\n\n    # Convert the image to Lab color space\n\n    # Compute the standard deviation of the r, g, and b channels\n    # std_dev = np.std(image[:,:,0], image[:,:,1], image[:,:,2])\n    std_dev = np.std(image, axis = 2)\n\n    # Define a threshold for classifying gray pixels\n    diff_threshold = 6 # Adjust as needed\n\n    # Classify pixels as gray or not gray based on the standard deviation\n    gray_mask = std_dev &lt; diff_threshold\n        \n    return gray_mask\n\n\ngray_mask = classify_gray(image)\ngray_mask = gray_mask.reshape((image.shape[0], image.shape[1]))\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 2, figsize=(15, 10), sharex=True, sharey=True)\n\naxes[0].imshow(gray_mask, cmap='gray')\naxes[0].set_title('Gray Mask')\n\naxes[1].imshow(image)\naxes[1].set_title('Original Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# Create additional features\n\n# Range of values (gray pixels will have low range)\nr = image.max(axis = 2) - image.min(axis = 2)\n\n# Canny edge detection\ncanny_edges_r = feature.canny(image[:,:,0], sigma=4)\ncanny_edges_g = feature.canny(image[:,:,1], sigma=4)\ncanny_edges_b = feature.canny(image[:,:,2], sigma=4)\n\n# Gaussian blur sigma = 1\ngaus_r_1 = gaussian_filter(image[:,:,0], sigma = 1)\ngaus_g_1 = gaussian_filter(image[:,:,1], sigma = 1)\ngaus_b_1 = gaussian_filter(image[:,:,2], sigma = 1)\n\n# Gaussian blur sigma = 3\ngaus_r_3 = gaussian_filter(image[:,:,0], sigma = 3)\ngaus_g_3 = gaussian_filter(image[:,:,1], sigma = 3)\ngaus_b_3 = gaussian_filter(image[:,:,2], sigma = 3)\n\n# Gaussian blur sigma = 5\ngaus_r_5 = gaussian_filter(image[:,:,0], sigma = 5)\ngaus_g_5 = gaussian_filter(image[:,:,1], sigma = 5)\ngaus_b_5 = gaussian_filter(image[:,:,2], sigma = 5)\n\n# LoG blur sigma = .5\nlog_r_5 = gaussian_laplace(image[:,:,0], sigma = .5)\nlog_g_5 = gaussian_laplace(image[:,:,1], sigma = .5)\nlog_b_5 = gaussian_laplace(image[:,:,2], sigma = .5)\n\n# LoG blur sigma = .6\nlog_r_6 = gaussian_laplace(image[:,:,0], sigma = .6)\nlog_g_6 = gaussian_laplace(image[:,:,1], sigma = .6)\nlog_b_6 = gaussian_laplace(image[:,:,2], sigma = .6)\n\n# LoG blur sigma = .8\nlog_r_8 = gaussian_laplace(image[:,:,0], sigma = .8)\nlog_g_8 = gaussian_laplace(image[:,:,1], sigma = .8)\nlog_b_8 = gaussian_laplace(image[:,:,2], sigma = .8)\n\n# Add layers to model\nimage_layers = np.dstack([image, r, canny_edges_r, canny_edges_g, canny_edges_b,\n                             gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                             gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                             log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8, hough_lines, gray_mask])\n\n\n\n\n\n\nimage_layers.shape\n\n(1500, 1500, 27)\n\n\n\nimage_layers = image_layers.reshape(image_layers.shape[0] * image_layers.shape[1], image_layers.shape[2])\nimage_layers.shape\n\n(2250000, 27)\n\n\n\n# Standardize the features\nscaler = StandardScaler()\nimage_layers_scaled = scaler.fit_transform(image_layers)\n\n\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=5)\nlayers_pca = pca.fit_transform(image_layers_scaled)\n\n# Explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Plotting the explained variance ratio\nplt.figure(figsize=(8, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\n\nlayers_pca.shape\n\n(2250000, 5)\n\n\nTransform to create new features in less dimensions\n\nnew_features = pca.transform(image_layers_scaled)\n\n\nnew_features.shape\n\n(2250000, 5)\n\n\n\n\n\n# run with pca reduce features\nsk_kmeans_pca = KMeans(n_clusters=3, verbose=1).fit(new_features)\n# run with all features\n# sk_kmeans_pca = KMeans(n_clusters=3, verbose=1).fit(image_layers)\n\nInitialization complete\nIteration 0, inertia 42054121.57276795.\nIteration 1, inertia 26981420.397716623.\nIteration 2, inertia 24075770.985596742.\nIteration 3, inertia 23204850.034456.\nIteration 4, inertia 22773059.031056065.\nIteration 5, inertia 22546215.95384517.\nIteration 6, inertia 22423341.63887473.\nIteration 7, inertia 22355134.86347936.\nIteration 8, inertia 22316315.6773539.\nIteration 9, inertia 22293824.586302444.\nIteration 10, inertia 22280357.514531154.\nIteration 11, inertia 22272380.15946948.\nIteration 12, inertia 22267615.2482199.\nIteration 13, inertia 22264705.49134808.\nIteration 14, inertia 22262877.13615176.\nIteration 15, inertia 22261773.899383508.\nIteration 16, inertia 22261102.7837751.\nConverged at iteration 16: center shift 0.0003135446683950635 within tolerance 0.0004137793031581444.\n\n\n\nsk_segmented_image_pca = sk_kmeans_pca.labels_.reshape((image.shape[0], image.shape[1]))\n\n\n# Generating figure 2\nfig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n\ncluster_one = (sk_segmented_image_pca==0)\naxes[0][0].imshow(cluster_one, cmap='gray')\naxes[0][0].set_title('Cluster One')\n\ncluster_two = (sk_segmented_image_pca==1)\naxes[0][1].imshow(cluster_two, cmap='gray')\naxes[0][1].set_title('Cluster Two')\n\ncluster_three = (sk_segmented_image_pca==2)\naxes[1][0].imshow(cluster_three, cmap='gray')\naxes[1][0].set_title('Cluster Three')\n\naxes[1][1].imshow(image)\naxes[1][1].set_title('Original Image')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhich cluster best captures the roads? Let’s find out with the results\n\nimage_label_bool = (image_label==255)\n\n\ncluster_one.dtype\n\ndtype('bool')\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_one.ravel())\n\nConfusion matrix:\n [[  92737   47528]\n [ 403434 1706301]]\nOverall accuracy: 0.8 \nPrecision: 0.187 \nRecall 0.661 \nDice Coefficient 0.291\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_two.ravel())\n\nConfusion matrix:\n [[  42148   98117]\n [1097545 1012190]]\nOverall accuracy: 0.469 \nPrecision: 0.037 \nRecall 0.3 \nDice Coefficient 0.066\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_three.ravel())\n\nConfusion matrix:\n [[   5380  134885]\n [ 608756 1500979]]\nOverall accuracy: 0.669 \nPrecision: 0.009 \nRecall 0.038 \nDice Coefficient 0.014\n\n\n\ndef identify_road_cluster(clustered_image, image_label):\n\n    cluster_labels = np.unique(clustered_image)\n\n    best_precision = 0\n    best_cluster = -1\n\n    for i in cluster_labels:\n        cluster = (sk_segmented_image_pca==i)\n        C = confusion_matrix(image_label.ravel(), cluster.ravel(), labels=(True, False))\n\n        # # Overall accuracy rate\n        # acc = (C[0,0] + C[1,1])/C.sum()\n        # # Recall\n        # recall = (C[0,0])/(C[0,0] + C[1,0])\n        # Precision\n        prec = (C[0,0])/(C[0,0] + C[0,1])\n\n        if prec &gt; best_precision:\n            best_precision = prec\n            best_cluster = i\n\n    return best_cluster\n\n\ncluster = identify_road_cluster(sk_segmented_image_pca, image_label_bool)\n\n\nimage_label\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)\n\n\n\nfig = plt.figure(1, figsize=(15, 15))\nplt.clf()\n\nax = fig.add_subplot(111, projection=\"3d\", elev=45, azim=90)\nax.set_position([0, 0, 0.95, 1])\n\nX = new_features\ny = image_label.ravel()\n\nfor name, label in [(\"Background\", 0), (\"Road\", 255)]:\n    ax.text3D(\n        X[y == label, 0].mean(),\n        X[y == label, 1].mean() + 1.5,\n        X[y == label, 2].mean(),\n        name,\n        horizontalalignment=\"center\",\n        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n    )\n# Reorder the labels to have colors matching the cluster results\n# y = np.choose(y, [0, 255]).astype(float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='Accent', edgecolor=\"k\")\n\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndef create_features(image):\n\n    # Calculation Canny gradient\n    image_gray = skol.rgb2gray(image)\n    canny_edges = feature.canny(image_gray, sigma=3)\n\n    # Create disk\n    disk = skimor.disk(1)\n\n    # Area closing for hough lines\n    closed_edges = skimor.dilation(canny_edges, footprint = disk)\n    closed_edges = closed_edges * 255\n\n    lines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n    hough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n\n    # Draw the detected lines on the canvas\n    for line in lines:\n        p0, p1 = line\n        # Draw line segment\n        rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n        hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n\n    #create gray mask\n    gray_mask = classify_gray(image)\n    gray_mask = gray_mask.reshape((image.shape[0], image.shape[1]))\n\n    # Create additional features\n\n    # Range of values (gray pixels will have low range)\n    r = image.max(axis = 2) - image.min(axis = 2)\n\n    # Canny edge detection\n    canny_edges_r = feature.canny(image[:,:,0], sigma=4);\n    canny_edges_g = feature.canny(image[:,:,1], sigma=4);\n    canny_edges_b = feature.canny(image[:,:,2], sigma=4);\n\n    # Gaussian blur sigma = 1\n    gaus_r_1 = gaussian_filter(image[:,:,0], sigma = 1)\n    gaus_g_1 = gaussian_filter(image[:,:,1], sigma = 1)\n    gaus_b_1 = gaussian_filter(image[:,:,2], sigma = 1)\n\n    # Gaussian blur sigma = 3\n    gaus_r_3 = gaussian_filter(image[:,:,0], sigma = 3)\n    gaus_g_3 = gaussian_filter(image[:,:,1], sigma = 3)\n    gaus_b_3 = gaussian_filter(image[:,:,2], sigma = 3)\n\n    # Gaussian blur sigma = 5\n    gaus_r_5 = gaussian_filter(image[:,:,0], sigma = 5)\n    gaus_g_5 = gaussian_filter(image[:,:,1], sigma = 5)\n    gaus_b_5 = gaussian_filter(image[:,:,2], sigma = 5)\n\n    # LoG blur sigma = .5\n    log_r_5 = gaussian_laplace(image[:,:,0], sigma = .5)\n    log_g_5 = gaussian_laplace(image[:,:,1], sigma = .5)\n    log_b_5 = gaussian_laplace(image[:,:,2], sigma = .5)\n\n    # LoG blur sigma = .6\n    log_r_6 = gaussian_laplace(image[:,:,0], sigma = .6)\n    log_g_6 = gaussian_laplace(image[:,:,1], sigma = .6)\n    log_b_6 = gaussian_laplace(image[:,:,2], sigma = .6)\n\n    # LoG blur sigma = .8\n    log_r_8 = gaussian_laplace(image[:,:,0], sigma = .8)\n    log_g_8 = gaussian_laplace(image[:,:,1], sigma = .8)\n    log_b_8 = gaussian_laplace(image[:,:,2], sigma = .8)\n\n    # Add layers to model\n    image_layers = np.dstack([image, r, canny_edges_r, canny_edges_g, canny_edges_b,\n                                gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                                gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                                log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8, hough_lines, gray_mask])\n    \n    image_layers = image_layers.reshape(image_layers.shape[0] * image_layers.shape[1], image_layers.shape[2])\n    \n    scaler = StandardScaler()\n    image_layers_scaled = scaler.fit_transform(image_layers)\n\n    print(image_layers.shape)\n    \n    return image_layers\n\n\ndef get_pca(image_layers, n_components):\n\n    # Initialize PCA and fit the scaled data\n    pca = PCA(n_components=n_components)\n    layers_pca = pca.fit_transform(image_layers)\n\n    # Explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    new_features = pca.transform(image_layers)\n\n    return new_features\n\n\ndef run_k_means(image_layers, n_clusters):\n\n    sk_kmeans = KMeans(n_clusters=n_clusters, verbose=1).fit(image_layers)\n\n    sk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n    best_cluster = identify_road_cluster(sk_segmented_image_pca, image_label)\n\n    segmented_roads = (sk_segmented_image_pca==best_cluster)\n\n    return segmented_roads\n\n\ndef full_pipeline(image, image_label, n_clusters=3, with_pca=False, pca_n_components=5):\n    \n    print(\"Creating features...\")\n    image_layers = create_features(image)\n    if(with_pca):\n        print(\"Reducing dimensions...\")\n        image_layers = get_pca(image_layers=image_layers, n_components=pca_n_components)\n    \n    print(\"Segmenting Roads...\")\n    segmented_roads = run_k_means(image_layers=image_layers, n_clusters=n_clusters)\n    print(\"Segmentation Complete\")\n    \n    plt.imshow(segmented_roads, cmap='gray')\n    plt.show()\n\n    print(\"Analyzing metrics...\")\n    accuracy_metrics(segmented_roads, image_label)\n\n\nimage.shape\n\n(1500, 1500, 3)\n\n\n\n# full_pipeline(image, image_label_bool)"
  },
  {
    "objectID": "code/k_means.html#lets-use-canny-edge-detection-instead",
    "href": "code/k_means.html#lets-use-canny-edge-detection-instead",
    "title": "Testing K-means Clustering",
    "section": "",
    "text": "# Calculation Canny gradient\ncanny_edges = feature.canny(image_gray, sigma=3)\n\n\n\n\n# Create disk\ndisk = skimor.disk(1)\n\n# Area closing\nclosed_edges = skimor.dilation(canny_edges, footprint = disk)\nclosed_edges = closed_edges * 255\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\nax = axes.ravel()\n\nax[0].imshow(canny_edges, cmap='gray')\nax[0].set_title('Canny Edges')\n\nax[1].imshow(closed_edges, cmap='gray')\nax[1].set_title('Closed Canny edges')\n\nax[2].imshow(image)\nax[2].set_title('Original Image')\n\nfor a in ax:\n    a.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNow lets add this new channel to the image and see what happens\n\nimage_with_canny = np.dstack((image, closed_edges.astype(np.uint8) * 255))\n\n\nfeature_vals = image_with_canny.reshape((-1,4))\nfeature_vals = np.float32(feature_vals)\n\n\nsk_kmeans = KMeans(n_clusters=3, verbose=1).fit(feature_vals)\n\nInitialization complete\nIteration 0, inertia 3043438080.0.\nIteration 1, inertia 2653675520.0.\nIteration 2, inertia 2619363840.0.\nIteration 3, inertia 2601338880.0.\nIteration 4, inertia 2591492864.0.\nIteration 5, inertia 2585991424.0.\nIteration 6, inertia 2582735616.0.\nIteration 7, inertia 2581043200.0.\nIteration 8, inertia 2579904768.0.\nIteration 9, inertia 2579204608.0.\nIteration 10, inertia 2578784256.0.\nConverged at iteration 10: center shift 0.1291225403547287 within tolerance 0.18192149658203127.\n\n\n\nsk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n\nskio.imshow(sk_segmented_image_pca, cmap='gray')\n\n/Users/aidenpape/miniconda3/lib/python3.11/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Low image data range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)\n\n\n\n\n\n\n\n\n\n\nnp.unique(sk_segmented_image_pca)\n\narray([0, 1, 2], dtype=int32)\n\n\n\n# Generating figure 2\nfig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n\ncluster_one = (sk_segmented_image_pca==0)\naxes[0][0].imshow(cluster_one, cmap='gray')\naxes[0][0].set_title('Cluster One')\n\ncluster_two = (sk_segmented_image_pca==1)\naxes[0][1].imshow(cluster_two, cmap='gray')\naxes[0][1].set_title('Cluster Two')\n\ncluster_three = (sk_segmented_image_pca==2)\naxes[1][0].imshow(cluster_three, cmap='gray')\naxes[1][0].set_title('Cluster Three')\n\naxes[1][1].imshow(image)\naxes[1][1].set_title('Original Image')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nlines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\nax = axes.ravel()\n\nax[0].imshow(image_gray, cmap='gray')\nax[0].set_title('Input image')\n\nax[1].imshow(canny_edges, cmap='gray')\nax[1].set_title('Canny edges')\n\nax[2].imshow(canny_edges * 0)\nfor line in lines:\n    p0, p1 = line\n    ax[2].plot((p0[0], p1[0]), (p0[1], p1[1]))\nax[2].set_xlim((0, image_gray.shape[1]))\nax[2].set_ylim((image_gray.shape[0], 0))\nax[2].set_title('Probabilistic Hough')\n\nfor a in ax:\n    a.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create a blank canvas to draw lines on\nhough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n\n# Draw the detected lines on the canvas\nfor line in lines:\n    p0, p1 = line\n    # Draw line segment\n    rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n    hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n\n# # Display the canvas with the detected lines\n# plt.imshow(hough_lines, cmap='gray')\n# plt.title('Image with Detected Lines')\n# plt.axis('off')\n# plt.show()"
  },
  {
    "objectID": "code/k_means.html#lets-try-to-classify-gray-pixels-these-may-help-identify-the-roads",
    "href": "code/k_means.html#lets-try-to-classify-gray-pixels-these-may-help-identify-the-roads",
    "title": "Testing K-means Clustering",
    "section": "",
    "text": "def classify_gray(image):\n\n    # Convert the image to Lab color space\n\n    # Compute the standard deviation of the r, g, and b channels\n    # std_dev = np.std(image[:,:,0], image[:,:,1], image[:,:,2])\n    std_dev = np.std(image, axis = 2)\n\n    # Define a threshold for classifying gray pixels\n    diff_threshold = 6 # Adjust as needed\n\n    # Classify pixels as gray or not gray based on the standard deviation\n    gray_mask = std_dev &lt; diff_threshold\n        \n    return gray_mask\n\n\ngray_mask = classify_gray(image)\ngray_mask = gray_mask.reshape((image.shape[0], image.shape[1]))\n\n\n# Generating figure 2\nfig, axes = plt.subplots(1, 2, figsize=(15, 10), sharex=True, sharey=True)\n\naxes[0].imshow(gray_mask, cmap='gray')\naxes[0].set_title('Gray Mask')\n\naxes[1].imshow(image)\naxes[1].set_title('Original Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# Create additional features\n\n# Range of values (gray pixels will have low range)\nr = image.max(axis = 2) - image.min(axis = 2)\n\n# Canny edge detection\ncanny_edges_r = feature.canny(image[:,:,0], sigma=4)\ncanny_edges_g = feature.canny(image[:,:,1], sigma=4)\ncanny_edges_b = feature.canny(image[:,:,2], sigma=4)\n\n# Gaussian blur sigma = 1\ngaus_r_1 = gaussian_filter(image[:,:,0], sigma = 1)\ngaus_g_1 = gaussian_filter(image[:,:,1], sigma = 1)\ngaus_b_1 = gaussian_filter(image[:,:,2], sigma = 1)\n\n# Gaussian blur sigma = 3\ngaus_r_3 = gaussian_filter(image[:,:,0], sigma = 3)\ngaus_g_3 = gaussian_filter(image[:,:,1], sigma = 3)\ngaus_b_3 = gaussian_filter(image[:,:,2], sigma = 3)\n\n# Gaussian blur sigma = 5\ngaus_r_5 = gaussian_filter(image[:,:,0], sigma = 5)\ngaus_g_5 = gaussian_filter(image[:,:,1], sigma = 5)\ngaus_b_5 = gaussian_filter(image[:,:,2], sigma = 5)\n\n# LoG blur sigma = .5\nlog_r_5 = gaussian_laplace(image[:,:,0], sigma = .5)\nlog_g_5 = gaussian_laplace(image[:,:,1], sigma = .5)\nlog_b_5 = gaussian_laplace(image[:,:,2], sigma = .5)\n\n# LoG blur sigma = .6\nlog_r_6 = gaussian_laplace(image[:,:,0], sigma = .6)\nlog_g_6 = gaussian_laplace(image[:,:,1], sigma = .6)\nlog_b_6 = gaussian_laplace(image[:,:,2], sigma = .6)\n\n# LoG blur sigma = .8\nlog_r_8 = gaussian_laplace(image[:,:,0], sigma = .8)\nlog_g_8 = gaussian_laplace(image[:,:,1], sigma = .8)\nlog_b_8 = gaussian_laplace(image[:,:,2], sigma = .8)\n\n# Add layers to model\nimage_layers = np.dstack([image, r, canny_edges_r, canny_edges_g, canny_edges_b,\n                             gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                             gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                             log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8, hough_lines, gray_mask])"
  },
  {
    "objectID": "code/k_means.html#pca-to-select-important-features",
    "href": "code/k_means.html#pca-to-select-important-features",
    "title": "Testing K-means Clustering",
    "section": "",
    "text": "image_layers.shape\n\n(1500, 1500, 27)\n\n\n\nimage_layers = image_layers.reshape(image_layers.shape[0] * image_layers.shape[1], image_layers.shape[2])\nimage_layers.shape\n\n(2250000, 27)\n\n\n\n# Standardize the features\nscaler = StandardScaler()\nimage_layers_scaled = scaler.fit_transform(image_layers)\n\n\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=5)\nlayers_pca = pca.fit_transform(image_layers_scaled)\n\n# Explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Plotting the explained variance ratio\nplt.figure(figsize=(8, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\n\nlayers_pca.shape\n\n(2250000, 5)\n\n\nTransform to create new features in less dimensions\n\nnew_features = pca.transform(image_layers_scaled)\n\n\nnew_features.shape\n\n(2250000, 5)\n\n\n\n\n\n# run with pca reduce features\nsk_kmeans_pca = KMeans(n_clusters=3, verbose=1).fit(new_features)\n# run with all features\n# sk_kmeans_pca = KMeans(n_clusters=3, verbose=1).fit(image_layers)\n\nInitialization complete\nIteration 0, inertia 42054121.57276795.\nIteration 1, inertia 26981420.397716623.\nIteration 2, inertia 24075770.985596742.\nIteration 3, inertia 23204850.034456.\nIteration 4, inertia 22773059.031056065.\nIteration 5, inertia 22546215.95384517.\nIteration 6, inertia 22423341.63887473.\nIteration 7, inertia 22355134.86347936.\nIteration 8, inertia 22316315.6773539.\nIteration 9, inertia 22293824.586302444.\nIteration 10, inertia 22280357.514531154.\nIteration 11, inertia 22272380.15946948.\nIteration 12, inertia 22267615.2482199.\nIteration 13, inertia 22264705.49134808.\nIteration 14, inertia 22262877.13615176.\nIteration 15, inertia 22261773.899383508.\nIteration 16, inertia 22261102.7837751.\nConverged at iteration 16: center shift 0.0003135446683950635 within tolerance 0.0004137793031581444.\n\n\n\nsk_segmented_image_pca = sk_kmeans_pca.labels_.reshape((image.shape[0], image.shape[1]))\n\n\n# Generating figure 2\nfig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n\ncluster_one = (sk_segmented_image_pca==0)\naxes[0][0].imshow(cluster_one, cmap='gray')\naxes[0][0].set_title('Cluster One')\n\ncluster_two = (sk_segmented_image_pca==1)\naxes[0][1].imshow(cluster_two, cmap='gray')\naxes[0][1].set_title('Cluster Two')\n\ncluster_three = (sk_segmented_image_pca==2)\naxes[1][0].imshow(cluster_three, cmap='gray')\naxes[1][0].set_title('Cluster Three')\n\naxes[1][1].imshow(image)\naxes[1][1].set_title('Original Image')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhich cluster best captures the roads? Let’s find out with the results\n\nimage_label_bool = (image_label==255)\n\n\ncluster_one.dtype\n\ndtype('bool')\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_one.ravel())\n\nConfusion matrix:\n [[  92737   47528]\n [ 403434 1706301]]\nOverall accuracy: 0.8 \nPrecision: 0.187 \nRecall 0.661 \nDice Coefficient 0.291\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_two.ravel())\n\nConfusion matrix:\n [[  42148   98117]\n [1097545 1012190]]\nOverall accuracy: 0.469 \nPrecision: 0.037 \nRecall 0.3 \nDice Coefficient 0.066\n\n\n\naccuracy_metrics(image_label_bool.ravel(), cluster_three.ravel())\n\nConfusion matrix:\n [[   5380  134885]\n [ 608756 1500979]]\nOverall accuracy: 0.669 \nPrecision: 0.009 \nRecall 0.038 \nDice Coefficient 0.014\n\n\n\ndef identify_road_cluster(clustered_image, image_label):\n\n    cluster_labels = np.unique(clustered_image)\n\n    best_precision = 0\n    best_cluster = -1\n\n    for i in cluster_labels:\n        cluster = (sk_segmented_image_pca==i)\n        C = confusion_matrix(image_label.ravel(), cluster.ravel(), labels=(True, False))\n\n        # # Overall accuracy rate\n        # acc = (C[0,0] + C[1,1])/C.sum()\n        # # Recall\n        # recall = (C[0,0])/(C[0,0] + C[1,0])\n        # Precision\n        prec = (C[0,0])/(C[0,0] + C[0,1])\n\n        if prec &gt; best_precision:\n            best_precision = prec\n            best_cluster = i\n\n    return best_cluster\n\n\ncluster = identify_road_cluster(sk_segmented_image_pca, image_label_bool)\n\n\nimage_label\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)\n\n\n\nfig = plt.figure(1, figsize=(15, 15))\nplt.clf()\n\nax = fig.add_subplot(111, projection=\"3d\", elev=45, azim=90)\nax.set_position([0, 0, 0.95, 1])\n\nX = new_features\ny = image_label.ravel()\n\nfor name, label in [(\"Background\", 0), (\"Road\", 255)]:\n    ax.text3D(\n        X[y == label, 0].mean(),\n        X[y == label, 1].mean() + 1.5,\n        X[y == label, 2].mean(),\n        name,\n        horizontalalignment=\"center\",\n        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n    )\n# Reorder the labels to have colors matching the cluster results\n# y = np.choose(y, [0, 255]).astype(float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='Accent', edgecolor=\"k\")\n\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndef create_features(image):\n\n    # Calculation Canny gradient\n    image_gray = skol.rgb2gray(image)\n    canny_edges = feature.canny(image_gray, sigma=3)\n\n    # Create disk\n    disk = skimor.disk(1)\n\n    # Area closing for hough lines\n    closed_edges = skimor.dilation(canny_edges, footprint = disk)\n    closed_edges = closed_edges * 255\n\n    lines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n    hough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n\n    # Draw the detected lines on the canvas\n    for line in lines:\n        p0, p1 = line\n        # Draw line segment\n        rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n        hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n\n    #create gray mask\n    gray_mask = classify_gray(image)\n    gray_mask = gray_mask.reshape((image.shape[0], image.shape[1]))\n\n    # Create additional features\n\n    # Range of values (gray pixels will have low range)\n    r = image.max(axis = 2) - image.min(axis = 2)\n\n    # Canny edge detection\n    canny_edges_r = feature.canny(image[:,:,0], sigma=4);\n    canny_edges_g = feature.canny(image[:,:,1], sigma=4);\n    canny_edges_b = feature.canny(image[:,:,2], sigma=4);\n\n    # Gaussian blur sigma = 1\n    gaus_r_1 = gaussian_filter(image[:,:,0], sigma = 1)\n    gaus_g_1 = gaussian_filter(image[:,:,1], sigma = 1)\n    gaus_b_1 = gaussian_filter(image[:,:,2], sigma = 1)\n\n    # Gaussian blur sigma = 3\n    gaus_r_3 = gaussian_filter(image[:,:,0], sigma = 3)\n    gaus_g_3 = gaussian_filter(image[:,:,1], sigma = 3)\n    gaus_b_3 = gaussian_filter(image[:,:,2], sigma = 3)\n\n    # Gaussian blur sigma = 5\n    gaus_r_5 = gaussian_filter(image[:,:,0], sigma = 5)\n    gaus_g_5 = gaussian_filter(image[:,:,1], sigma = 5)\n    gaus_b_5 = gaussian_filter(image[:,:,2], sigma = 5)\n\n    # LoG blur sigma = .5\n    log_r_5 = gaussian_laplace(image[:,:,0], sigma = .5)\n    log_g_5 = gaussian_laplace(image[:,:,1], sigma = .5)\n    log_b_5 = gaussian_laplace(image[:,:,2], sigma = .5)\n\n    # LoG blur sigma = .6\n    log_r_6 = gaussian_laplace(image[:,:,0], sigma = .6)\n    log_g_6 = gaussian_laplace(image[:,:,1], sigma = .6)\n    log_b_6 = gaussian_laplace(image[:,:,2], sigma = .6)\n\n    # LoG blur sigma = .8\n    log_r_8 = gaussian_laplace(image[:,:,0], sigma = .8)\n    log_g_8 = gaussian_laplace(image[:,:,1], sigma = .8)\n    log_b_8 = gaussian_laplace(image[:,:,2], sigma = .8)\n\n    # Add layers to model\n    image_layers = np.dstack([image, r, canny_edges_r, canny_edges_g, canny_edges_b,\n                                gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                                gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                                log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8, hough_lines, gray_mask])\n    \n    image_layers = image_layers.reshape(image_layers.shape[0] * image_layers.shape[1], image_layers.shape[2])\n    \n    scaler = StandardScaler()\n    image_layers_scaled = scaler.fit_transform(image_layers)\n\n    print(image_layers.shape)\n    \n    return image_layers\n\n\ndef get_pca(image_layers, n_components):\n\n    # Initialize PCA and fit the scaled data\n    pca = PCA(n_components=n_components)\n    layers_pca = pca.fit_transform(image_layers)\n\n    # Explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    new_features = pca.transform(image_layers)\n\n    return new_features\n\n\ndef run_k_means(image_layers, n_clusters):\n\n    sk_kmeans = KMeans(n_clusters=n_clusters, verbose=1).fit(image_layers)\n\n    sk_segmented_image_pca = sk_kmeans.labels_.reshape((image.shape[0], image.shape[1]))\n\n    best_cluster = identify_road_cluster(sk_segmented_image_pca, image_label)\n\n    segmented_roads = (sk_segmented_image_pca==best_cluster)\n\n    return segmented_roads\n\n\ndef full_pipeline(image, image_label, n_clusters=3, with_pca=False, pca_n_components=5):\n    \n    print(\"Creating features...\")\n    image_layers = create_features(image)\n    if(with_pca):\n        print(\"Reducing dimensions...\")\n        image_layers = get_pca(image_layers=image_layers, n_components=pca_n_components)\n    \n    print(\"Segmenting Roads...\")\n    segmented_roads = run_k_means(image_layers=image_layers, n_clusters=n_clusters)\n    print(\"Segmentation Complete\")\n    \n    plt.imshow(segmented_roads, cmap='gray')\n    plt.show()\n\n    print(\"Analyzing metrics...\")\n    accuracy_metrics(segmented_roads, image_label)\n\n\nimage.shape\n\n(1500, 1500, 3)\n\n\n\n# full_pipeline(image, image_label_bool)"
  },
  {
    "objectID": "Segmentation_UNET.html",
    "href": "Segmentation_UNET.html",
    "title": "",
    "section": "",
    "text": "# Import packages\nimport numpy as np\nimport skimage.io as skio\nimport skimage.morphology as skm\nfrom skimage import feature\nfrom skimage.util import view_as_blocks\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n# !pip install opendatasets\n# import opendatasets as od\nimport os\nimport torch\nimport cv2\nimport math\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import concatenate, Concatenate, Conv2D,Conv3D, MaxPooling2D, Conv2DTranspose\nfrom keras.layers import Input, UpSampling3D, BatchNormalization, UpSampling2D\nfrom tensorflow.keras.optimizers.legacy import Adam\n\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as keras\n\n# data = od.download(\"https://www.kaggle.com/datasets/balraj98/massachusetts-roads-dataset\")\n\n2024-05-14 10:23:00.840810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n# Function to print several accuracy metrics\ndef accuracy_metrics(y_true, y_pred):\n    # Create confusion matrix\n\n    if(np.max(y_true)&gt;1):\n        y_true = y_true / 255\n\n    if(np.max(y_pred)&gt;1):\n        y_pred = y_pred / 255\n\n    y_true = y_true.ravel()\n    y_pred = y_pred.ravel()\n\n    C = confusion_matrix(y_true, y_pred, labels=(True, False))\n\n    # Overall accuracy rate\n    acc = (C[0,0] + C[1,1])/C.sum()\n\n    # Recall\n    recall = (C[0,0])/(C[0,0] + C[1,0])\n    \n    # Precision\n    prec = (C[0,0])/(C[0,0] + C[0,1])\n\n    smooth = 1\n    dice = (2. * (np.sum(y_true * y_pred)) + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)\n\n    # Print results\n    print(\"Confusion matrix:\\n\", C)\n    print(\"Overall accuracy:\", np.round(acc, 3), \"\\nPrecision:\", np.round(recall, 3), \"\\nRecall\", np.round(prec, 3), \"\\nDice Coefficient\", np.round(dice, 3))\n# Read the data\nrgb = skio.imread(\"./data/massRoads/tiff/train/10828735_15.tiff\")\nans = skio.imread(\"./data/massRoads/tiff/train_labels/10828735_15.tif\")\n\n# Display training data and correct output\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(rgb, ax = ax[0])\nax[0].set_title(\"Data\")\nskio.imshow(ans, ax = ax[1])\nax[1].set_title(\"Label\")\n\nText(0.5, 1.0, 'Label')\nDATA_DIR = './data/massRoads/tiff/'\n\nx_train_dir = os.path.join(DATA_DIR, 'train')\ny_train_dir = os.path.join(DATA_DIR, 'train_labels')\n\nx_valid_dir = os.path.join(DATA_DIR, 'val')\ny_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n\nx_test_dir = os.path.join(DATA_DIR, 'test')\ny_test_dir = os.path.join(DATA_DIR, 'test_labels')\nclass_dict = pd.read_csv(\"./data/massRoads/label_class_dict.csv\")\n# Get class names\nclass_names = class_dict['name'].tolist()\n# Get class RGB values\nclass_rgb_values = class_dict[['r','g','b']].values.tolist()\n\nprint('All dataset classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)\n\nAll dataset classes and their corresponding RGB values in labels:\nClass Names:  ['background', 'road']\nClass RGB values:  [[0, 0, 0], [255, 255, 255]]\n# image paths\nimage_paths = [os.path.join(x_train_dir, image_id) for image_id in sorted(os.listdir(x_train_dir))]\nmask_paths = [os.path.join(y_train_dir, image_id) for image_id in sorted(os.listdir(y_train_dir))]\n\nval_img_paths = [os.path.join(x_valid_dir, image_id) for image_id in sorted(os.listdir(x_valid_dir))]\nval_mask_paths = [os.path.join(y_valid_dir, image_id) for image_id in sorted(os.listdir(y_valid_dir))]\n\ntest_img_paths = [os.path.join(x_test_dir, image_id) for image_id in sorted(os.listdir(x_test_dir))]\ntest_mask_paths = [os.path.join(y_test_dir, image_id) for image_id in sorted(os.listdir(y_test_dir))]\n# prep image training\ntrain_len = 50\npatch_size = 128\nnum_patches = 1500 // patch_size  # Number of patches per dimension\n\nsmall_train_images = []\nsmall_train_labels = []\n\nfor i in range(train_len):\n    # read images for each path\n    training_image = cv2.imread(image_paths[i])\n    training_mask = cv2.imread(mask_paths[i])\n\n    for i in range(num_patches):\n        for j in range(num_patches):\n            # Extract patch from the image\n            image_patch = training_image[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size, :]\n            mask_patch = training_mask[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size, :]\n            \n            if(len(np.unique(mask_patch))&gt;1):\n\n                #normalize\n                image_patch = image_patch / 255\n                mask_patch = mask_patch / 255\n\n                small_train_images.append(image_patch)\n                small_train_labels.append(mask_patch)\n\nsmall_train_images = np.array(small_train_images)\nsmall_train_labels = np.array(small_train_labels)[:,:,:,0]\nsmall_train_images.shape\n\n(3448, 128, 128, 3)\ncount = 0\n\nfor label in small_train_labels[:2000]:\n    if(len(np.unique(label))==1):\n       count += 1\n\n\nprint(\"Number of blank images: \" + str(count))\n\nNumber of blank images: 0\nval_len = len(val_img_paths)\n\nval_img = [0] * val_len\nval_masks = [0] * val_len\n\nfor i in range(val_len):\n    val_img[i] = cv2.imread(val_img_paths[i])\n    val_img[i] = val_img[i] / 255\n    val_masks[i] = cv2.imread(val_mask_paths[i])\n    val_masks[i] = val_masks[i] / 255\n\nval_img = np.array(val_img[:])\nval_img = val_img[:, :patch_size, :patch_size]\nval_masks = np.array(val_masks)[:,:,:,0]\nval_masks = val_masks[:, :patch_size, :patch_size]\ncount = 0\n\nfor label in val_masks:\n    if(len(np.unique(label))==1):\n       count += 1\n\n\nprint(\"Number of blank images: \" + str(count))\n\nNumber of blank images: 1\ntest_len = len(test_img_paths)\n\ntest_img = [0] * test_len\ntest_masks = [0] * test_len\n\nfor i in range(test_len):\n    test_img[i] = cv2.imread(test_img_paths[i])\n    test_img[i] = test_img[i] / 255\n    test_masks[i] = cv2.imread(test_mask_paths[i])\n    test_masks[i] = test_masks[i] / 255\n\ntest_img = np.array(test_img[:])\ntest_img = test_img[:, :patch_size, :patch_size]\ntest_masks = np.array(test_masks)[:,:,:,0]\ntest_masks = test_masks[:, :patch_size, :patch_size]\ncount = 0\n\nfor label in test_masks:\n    if(len(np.unique(label))==1):\n       count += 1\n\n\nprint(\"Number of blank images: \" + str(count))\n\nNumber of blank images: 19\nprint(small_train_images.shape)\nprint(small_train_labels.shape)\nprint(val_img.shape)\nprint(val_masks.shape)\nprint(test_img.shape)\nprint(test_masks.shape)\n\n(3448, 128, 128, 3)\n(3448, 128, 128)\n(14, 128, 128, 3)\n(14, 128, 128)\n(49, 128, 128, 3)\n(49, 128, 128)\n# Display training data and correct output\ni = 500\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nax[0].imshow(small_train_images[i], cmap='gray')\nax[0].set_title(\"Image\")\nax[1].imshow(small_train_labels[i], cmap='gray')\nax[1].set_title(\"Label\")\n\nText(0.5, 1.0, 'Label')\nsmooth=1\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n# I found this architecture with the shape (128, 128, 1).\n\ndef unet(input_size = (128,128,3)):\n    inputs = Input(input_size)\n    conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n    conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    up6 = Conv2DTranspose(128,2,strides=(2,2),padding='same')(drop5)\n    merge6 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n    conv6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n    up7 = Conv2DTranspose(64,2,strides=(2,2),padding='same')(conv6)\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n    conv7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n    up8 = Conv2DTranspose(32,2,strides=(2,2),padding='same')(conv7)\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n    conv8 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n    up9 = Conv2DTranspose(16,2,strides=(2,2),padding='same')(conv8)\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n    conv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n\n    model = Model(inputs, conv10)\n\n    model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=[dice_coef])\n\n    model.summary()\n    \n    return model\nmodel = unet()\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (None, 128, 128,  │          0 │ -                 │\n│ (InputLayer)        │ 3)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d (Conv2D)     │ (None, 128, 128,  │        448 │ input_layer[0][0] │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_1 (Conv2D)   │ (None, 128, 128,  │      2,320 │ conv2d[0][0]      │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d       │ (None, 64, 64,    │          0 │ conv2d_1[0][0]    │\n│ (MaxPooling2D)      │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_2 (Conv2D)   │ (None, 64, 64,    │      4,640 │ max_pooling2d[0]… │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_3 (Conv2D)   │ (None, 64, 64,    │      9,248 │ conv2d_2[0][0]    │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_1     │ (None, 32, 32,    │          0 │ conv2d_3[0][0]    │\n│ (MaxPooling2D)      │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_4 (Conv2D)   │ (None, 32, 32,    │     18,496 │ max_pooling2d_1[… │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_5 (Conv2D)   │ (None, 32, 32,    │     36,928 │ conv2d_4[0][0]    │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_2     │ (None, 16, 16,    │          0 │ conv2d_5[0][0]    │\n│ (MaxPooling2D)      │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_6 (Conv2D)   │ (None, 16, 16,    │     73,856 │ max_pooling2d_2[… │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_7 (Conv2D)   │ (None, 16, 16,    │    147,584 │ conv2d_6[0][0]    │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (Dropout)   │ (None, 16, 16,    │          0 │ conv2d_7[0][0]    │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_3     │ (None, 8, 8, 128) │          0 │ dropout[0][0]     │\n│ (MaxPooling2D)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_8 (Conv2D)   │ (None, 8, 8, 256) │    295,168 │ max_pooling2d_3[… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_9 (Conv2D)   │ (None, 8, 8, 256) │    590,080 │ conv2d_8[0][0]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (Dropout) │ (None, 8, 8, 256) │          0 │ conv2d_9[0][0]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose    │ (None, 16, 16,    │    131,200 │ dropout_1[0][0]   │\n│ (Conv2DTranspose)   │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (None, 16, 16,    │          0 │ dropout[0][0],    │\n│ (Concatenate)       │ 256)              │            │ conv2d_transpose… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_10 (Conv2D)  │ (None, 16, 16,    │    295,040 │ concatenate[0][0] │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_11 (Conv2D)  │ (None, 16, 16,    │    147,584 │ conv2d_10[0][0]   │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_1  │ (None, 32, 32,    │     32,832 │ conv2d_11[0][0]   │\n│ (Conv2DTranspose)   │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (None, 32, 32,    │          0 │ conv2d_5[0][0],   │\n│ (Concatenate)       │ 128)              │            │ conv2d_transpose… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_12 (Conv2D)  │ (None, 32, 32,    │     73,792 │ concatenate_1[0]… │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_13 (Conv2D)  │ (None, 32, 32,    │     36,928 │ conv2d_12[0][0]   │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_2  │ (None, 64, 64,    │      8,224 │ conv2d_13[0][0]   │\n│ (Conv2DTranspose)   │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_2       │ (None, 64, 64,    │          0 │ conv2d_3[0][0],   │\n│ (Concatenate)       │ 64)               │            │ conv2d_transpose… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_14 (Conv2D)  │ (None, 64, 64,    │     18,464 │ concatenate_2[0]… │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_15 (Conv2D)  │ (None, 64, 64,    │      9,248 │ conv2d_14[0][0]   │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_3  │ (None, 128, 128,  │      2,064 │ conv2d_15[0][0]   │\n│ (Conv2DTranspose)   │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_3       │ (None, 128, 128,  │          0 │ conv2d_1[0][0],   │\n│ (Concatenate)       │ 32)               │            │ conv2d_transpose… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_16 (Conv2D)  │ (None, 128, 128,  │      4,624 │ concatenate_3[0]… │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_17 (Conv2D)  │ (None, 128, 128,  │      2,320 │ conv2d_16[0][0]   │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_18 (Conv2D)  │ (None, 128, 128,  │        290 │ conv2d_17[0][0]   │\n│                     │ 2)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_19 (Conv2D)  │ (None, 128, 128,  │          3 │ conv2d_18[0][0]   │\n│                     │ 1)                │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 1,941,381 (7.41 MB)\n\n\n\n Trainable params: 1,941,381 (7.41 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\nTrain with first 750 images\ncallbacks = [\n#     tf.keras.callbacks.ModelCheckpoint('unet_model.h5', save_best_only=True, verbose=2),\n    tf.keras.callbacks.EarlyStopping(patience=5, monitor= 'val_dice_coef', mode='max', restore_best_weights=True)\n]\n\n# Train the model\nhistory = model.fit(x=small_train_images[:750], y=small_train_labels[:750], \n                    validation_data = (val_img, val_masks), epochs = 20, callbacks=[callbacks], \n                    verbose = 1)\n\nEpoch 1/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.4494 - loss: 0.1150 - val_dice_coef: 0.4529 - val_loss: 0.1392\nEpoch 2/20\n 3/24 ━━━━━━━━━━━━━━━━━━━━ 37s 2s/step - dice_coef: 0.5123 - loss: 0.0988\n\n\nKeyboardInterrupt:\nprediction = model.predict(test_img)\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 471ms/step\n# Display training data and correct output\ni = 20\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\nax[0].imshow(prediction[i], cmap='gray')\nax[0].set_title(\"Prediction\")\nax[1].imshow(test_masks[i], cmap='gray')\nax[1].set_title(\"Label\")\nax[2].imshow(test_img[i], cmap='gray')\nax[2].set_title(\"Image\")\n\nText(0.5, 1.0, 'Image')\nTrain the model again wiht images 750-1500\n# Train the model\nhistory = model.fit(x=small_train_images[750:1500], y=small_train_labels[750:1500], \n                    validation_data = (val_img, val_masks), epochs = 20, callbacks=[callbacks], \n                    verbose = 1)\n\nEpoch 1/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 38s 2s/step - dice_coef: 0.2316 - loss: 0.1549 - val_dice_coef: 0.2238 - val_loss: 0.2140\nEpoch 2/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 38s 2s/step - dice_coef: 0.2553 - loss: 0.1472 - val_dice_coef: 0.2162 - val_loss: 0.2248\nEpoch 3/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 38s 2s/step - dice_coef: 0.3001 - loss: 0.1399 - val_dice_coef: 0.2092 - val_loss: 0.2282\nEpoch 4/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - dice_coef: 0.3319 - loss: 0.1331 - val_dice_coef: 0.2704 - val_loss: 0.2174\nEpoch 5/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.3906 - loss: 0.1293 - val_dice_coef: 0.3306 - val_loss: 0.2072\nEpoch 6/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.4741 - loss: 0.1135 - val_dice_coef: 0.3308 - val_loss: 0.2239\nEpoch 7/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.4950 - loss: 0.1127 - val_dice_coef: 0.3040 - val_loss: 0.1911\nEpoch 8/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 39s 2s/step - dice_coef: 0.5130 - loss: 0.1014 - val_dice_coef: 0.3675 - val_loss: 0.2199\nEpoch 9/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - dice_coef: 0.5580 - loss: 0.0961 - val_dice_coef: 0.3538 - val_loss: 0.1848\nEpoch 10/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 43s 2s/step - dice_coef: 0.5581 - loss: 0.0943 - val_dice_coef: 0.3786 - val_loss: 0.1927\nEpoch 11/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - dice_coef: 0.5946 - loss: 0.0858 - val_dice_coef: 0.3710 - val_loss: 0.1812\nEpoch 12/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - dice_coef: 0.5862 - loss: 0.0878 - val_dice_coef: 0.3663 - val_loss: 0.1856\nEpoch 13/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.6282 - loss: 0.0802 - val_dice_coef: 0.3766 - val_loss: 0.2192\nEpoch 14/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6466 - loss: 0.0761 - val_dice_coef: 0.4121 - val_loss: 0.1739\nEpoch 15/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6484 - loss: 0.0730 - val_dice_coef: 0.4263 - val_loss: 0.1737\nEpoch 16/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6541 - loss: 0.0736 - val_dice_coef: 0.4321 - val_loss: 0.1925\nEpoch 17/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6694 - loss: 0.0705 - val_dice_coef: 0.4549 - val_loss: 0.1918\nEpoch 18/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6854 - loss: 0.0647 - val_dice_coef: 0.4776 - val_loss: 0.1604\nEpoch 19/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6670 - loss: 0.0694 - val_dice_coef: 0.4870 - val_loss: 0.1762\nEpoch 20/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6790 - loss: 0.0691 - val_dice_coef: 0.4810 - val_loss: 0.1806\nprediction_2 = model.predict(test_img)\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 216ms/step\n# Display training data and correct output\ni = 20\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\nax[0].imshow(prediction_2[i], cmap='gray')\nax[0].set_title(\"Prediction\")\nax[1].imshow(test_masks[i], cmap='gray')\nax[1].set_title(\"Label\")\nax[2].imshow(test_img[i], cmap='gray')\nax[2].set_title(\"Image\")\n\nText(0.5, 1.0, 'Image')\nTrain the model on training images 1500-2250\n# Train the model\nhistory = model.fit(x=small_train_images[1500:2250], y=small_train_labels[1500:2250], \n                    validation_data = (val_img, val_masks), epochs = 20, callbacks=[callbacks], \n                    verbose = 1)\n\nEpoch 1/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - dice_coef: 0.5745 - loss: 0.1068 - val_dice_coef: 0.4897 - val_loss: 0.1779\nEpoch 2/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - dice_coef: 0.6172 - loss: 0.0948 - val_dice_coef: 0.5241 - val_loss: 0.1367\nEpoch 3/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6208 - loss: 0.0882 - val_dice_coef: 0.5413 - val_loss: 0.1394\nEpoch 4/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6338 - loss: 0.0875 - val_dice_coef: 0.5516 - val_loss: 0.1368\nEpoch 5/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6411 - loss: 0.0853 - val_dice_coef: 0.5266 - val_loss: 0.1661\nEpoch 6/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6694 - loss: 0.0782 - val_dice_coef: 0.5727 - val_loss: 0.1696\nEpoch 7/20\n24/24 ━━━━━━━━━━━━━━━━━━━━ 40s 2s/step - dice_coef: 0.6900 - loss: 0.0746 - val_dice_coef: 0.5259 - val_loss: 0.1468\nprediction_3 = model.predict(test_img)\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 238ms/step\n# Display training data and correct output\ni = 40\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\nax[0].imshow(prediction_3[i], cmap='gray')\nax[0].set_title(\"Prediction\")\nax[1].imshow(test_masks[i], cmap='gray')\nax[1].set_title(\"Label\")\nax[2].imshow(test_img[i], cmap='gray')\nax[2].set_title(\"Image\")\n\nText(0.5, 1.0, 'Image')"
  },
  {
    "objectID": "Segmentation_UNET.html#predicting-our-test-image",
    "href": "Segmentation_UNET.html#predicting-our-test-image",
    "title": "",
    "section": "Predicting our test image",
    "text": "Predicting our test image\n\nimage_filepath = \"./data/massRoads/tiff\"\n# /train/... for images and /train_labels/... for labels\ntesting_image = skio.imread(image_filepath + \"/train/10828735_15.tiff\")\ntesting_image_label = skio.imread(image_filepath + \"/train_labels/10828735_15.tif\")\n\n\nsmall_test_images = []\n\nfor i in range(num_patches):\n    for j in range(num_patches):\n        # Extract patch from the image\n        patch = testing_image[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size, :]\n        small_test_images.append(patch)\n\nsmall_test_images = np.array(small_test_images)\n\n\ntest_prediction = model.predict(small_test_images)\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 2s 472ms/step\n\n\n\nReconstruct the image\n\ndef reconstruct_image(image_batches, num_patches, patch_size):\n    #  Calculate the dimensions of the original image\n    \n    image_dim = num_patches * patch_size\n\n    # Reshape the array\n    reshaped_patches = np.reshape(image_batches, (num_patches, num_patches, patch_size, patch_size))\n\n    # Initialize an empty array to store the reconstructed image\n    reconstructed_image = np.zeros((image_dim, image_dim), dtype=np.uint8)\n\n    # Reconstruct the image\n    for i in range(num_patches):\n        for j in range(num_patches):\n            # Get the current patch\n            patch = reshaped_patches[i][j]\n            patch = patch &gt; 0.5\n            \n            # Calculate the starting and ending indices for placing the patch\n            start_i, start_j = i * patch_size, j * patch_size\n            end_i, end_j = start_i + patch_size, start_j + patch_size\n            \n            # Place the patch in the reconstructed image\n            reconstructed_image[start_i:end_i, start_j:end_j] = patch\n    \n    return reconstructed_image\n\n\nreconstructed_image = reconstruct_image(test_prediction, num_patches=num_patches, patch_size=patch_size)\n\n\ntesting_image_label_shrunk = testing_image_label[:reconstructed_image.shape[0], :reconstructed_image.shape[1]]\ntesting_image_shrunk = testing_image[:reconstructed_image.shape[0], :reconstructed_image.shape[1]]\n\n\n# Display training data and correct output\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\nax[0].imshow(reconstructed_image, cmap='gray')\nax[0].set_title(\"Prediction\")\nax[1].imshow(testing_image_label_shrunk, cmap='gray')\nax[1].set_title(\"Label\")\nax[2].imshow(testing_image_shrunk, cmap='gray')\nax[2].set_title(\"Image\")\n\nplt.show()\n\nText(0.5, 1.0, 'Image')\n\n\n\n\n\n\n\n\n\n\naccuracy_metrics(reconstructed_image, testing_image_label_shrunk)\n\nConfusion matrix:\n [[  10955   44651]\n [ 171527 1755331]]\nOverall accuracy: 0.891 \nPrecision: 0.06 \nRecall 0.197 \nDice Coefficient 0.092"
  },
  {
    "objectID": "code/rf.html",
    "href": "code/rf.html",
    "title": "Random Forest for Road Segmentation",
    "section": "",
    "text": "# Import packages\nimport numpy as np\nimport skimage.io as skio\nimport skimage.morphology as skm\nfrom skimage import filters, feature, transform\nfrom skimage.color import rgb2gray\nimport skimage.draw as draw\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.cluster import KMeans\nfrom sklearn.utils.random import sample_without_replacement\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom scipy.ndimage import gaussian_filter\nfrom scipy.ndimage import gaussian_laplace\nfrom scipy.ndimage import maximum_filter\nfrom scipy.ndimage import minimum_filter\nfrom scipy.ndimage import median_filter\n\n\n# Read the data\nrgb = skio.imread(\"../../data/MA_roads/tiff/train/10828735_15.tiff\")\nans = skio.imread(\"../../data/MA_roads/tiff/train_labels/10828735_15.tif\") &gt; 0\n\nrgb_test = skio.imread(\"../../data/MA_roads/tiff/train/21929005_15.tiff\")\nans_test = skio.imread(\"../../data/MA_roads/tiff/train_labels/21929005_15.tif\") &gt; 0\n\n# Display training data and correct output\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(rgb, ax = ax[0])\nax[0].set_title(\"Data\")\nskio.imshow(ans, ax = ax[1])\nax[1].set_title(\"Solution\");\n\n\n\n\n\n\n\n\n\n# Function to compute DICE\nsmooth=1\ndef dice_coef(y_true, y_pred):\n    intersection = np.sum(y_true * y_pred)\n    return (2. * intersection + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)\n        \n# Function to print several accuracy metrics\ndef accuracy_metrics(y_true, y_pred):\n    # Create confusion matrix\n    C = confusion_matrix(y_true, y_pred, labels=(True, False))\n\n    # Overall accuracy rate\n    acc = (C[0,0] + C[1,1])/C.sum()\n\n    # Recall\n    recall = (C[0,0])/(C[0,0] + C[1,0])\n    \n    # Precision\n    prec = (C[0,0])/(C[0,0] + C[0,1])\n\n    # DICE\n    dice = dice_coef(y_true, y_pred)\n\n\n    # Print results\n    print(\"Confusion matrix:\\n\", C)\n    print(\"Overall accuracy:\", np.round(acc, 3), \"\\nPrecision:\", np.round(recall, 3),\n            \"\\nRecall\", np.round(prec, 3), \"\\nDICE:\", np.round(dice, 3))\n\n# Function to create input layer\ndef classify_gray(image):\n\n    # Compute the standard deviation of the r, g, and b channels\n    std_dev = np.std(image, axis = 2)\n\n    # Define a threshold for classifying gray pixels\n    diff_threshold = 6 # Adjust as needed\n\n    # Classify pixels as gray or not gray based on the standard deviation\n    gray_mask = std_dev &lt; diff_threshold\n        \n    return gray_mask\n    \n# Function to compute layers for additional model features\ndef compute_features(img, include_categorical = True):    \n    # Range of values (gray pixels will have low range)\n    r = img.max(axis = 2) - img.min(axis = 2)\n\n    if include_categorical:\n        \n        # Canny edge detection\n        canny_edges_r = feature.canny(img[:,:,0], sigma=4)\n        canny_edges_g = feature.canny(img[:,:,1], sigma=4)\n        canny_edges_b = feature.canny(img[:,:,2], sigma=4)\n\n         # Calculation Canny gradient\n        image_gray = rgb2gray(img)\n        canny_edges = feature.canny(image_gray, sigma=3)\n    \n        # Create disk\n        disk = skm.disk(1)\n    \n        # Area closing for hough lines\n        closed_edges = skm.dilation(canny_edges, footprint = disk)\n        closed_edges = closed_edges * 255\n    \n        lines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n        hough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n    \n        # Draw the detected lines on the canvas\n        for line in lines:\n            p0, p1 = line\n            # Draw line segment\n            rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n            hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n    \n        #create gray mask\n        gray_mask = classify_gray(img)\n        gray_mask = gray_mask.reshape((img.shape[0], img.shape[1]))\n    \n\n        img = np.dstack([img, canny_edges_r, canny_edges_g, canny_edges_b, gray_mask, hough_lines])\n        \n    \n    # Gaussian blur sigma = 1\n    gaus_r_1 = gaussian_filter(img[:,:,0], sigma = 1)\n    gaus_g_1 = gaussian_filter(img[:,:,1], sigma = 1)\n    gaus_b_1 = gaussian_filter(img[:,:,2], sigma = 1)\n    \n    # Gaussian blur sigma = 3\n    gaus_r_3 = gaussian_filter(img[:,:,0], sigma = 3)\n    gaus_g_3 = gaussian_filter(img[:,:,1], sigma = 3)\n    gaus_b_3 = gaussian_filter(img[:,:,2], sigma = 3)\n\n    # Gaussian blur sigma = 5\n    gaus_r_5 = gaussian_filter(img[:,:,0], sigma = 5)\n    gaus_g_5 = gaussian_filter(img[:,:,1], sigma = 5)\n    gaus_b_5 = gaussian_filter(img[:,:,2], sigma = 5)\n    \n    # LoG blur sigma = .5\n    log_r_5 = gaussian_laplace(img[:,:,0], sigma = .5)\n    log_g_5 = gaussian_laplace(img[:,:,1], sigma = .5)\n    log_b_5 = gaussian_laplace(img[:,:,2], sigma = .5)\n    \n    # LoG blur sigma = .6\n    log_r_6 = gaussian_laplace(img[:,:,0], sigma = .6)\n    log_g_6 = gaussian_laplace(img[:,:,1], sigma = .6)\n    log_b_6 = gaussian_laplace(img[:,:,2], sigma = .6)\n    \n    # LoG blur sigma = .8\n    log_r_8 = gaussian_laplace(img[:,:,0], sigma = .8)\n    log_g_8 = gaussian_laplace(img[:,:,1], sigma = .8)\n    log_b_8 = gaussian_laplace(img[:,:,2], sigma = .8)\n    \n    # Add layers to model\n    return np.dstack([img, r,\n                     gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                     gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                     log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8])"
  },
  {
    "objectID": "code/rf.html#train-model",
    "href": "code/rf.html#train-model",
    "title": "Random Forest for Road Segmentation",
    "section": "Train model",
    "text": "Train model\n\n# Flatten images\ntrain_small_rgb = small_rgb.reshape(small_rgb.shape[0]*small_rgb.shape[1], 3)\ny_train = small_ans.reshape(small_ans.shape[0]*small_ans.shape[1])\n\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel1 = RF.fit(train_small_rgb, y_train)\n\n\n# Predictions on training data\nmodel1_pred = model1.predict(train_small_rgb)\n\n# Confusion matrix\naccuracy_metrics(y_train, model1_pred)\n\nConfusion matrix:\n [[  9005   4018]\n [  1558 105419]]\nOverall accuracy: 0.954 \nPrecision: 0.853 \nRecall 0.691 \nDICE: 0.764\n\n\nWhile we have a really good overall accuracy rate, we are correctly predicting only 68.7% of the actual road pixels. With a precision of 0.854, about 85.4% of our road predictions are actually roads.\n\n# Convert predictions to image\ntrain_preds = model1_pred.reshape(small_ans.shape[0], small_ans.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\n\nskio.imshow(train_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans, ax = ax[1])\nax[1].set_title(\"Actual Solution\");\n\n\n\n\n\n\n\n\nVisually, our solution looks alright, but it obviously has room for improvement. Let’s see what our results look like on the testing data."
  },
  {
    "objectID": "code/rf.html#test-model",
    "href": "code/rf.html#test-model",
    "title": "Random Forest for Road Segmentation",
    "section": "Test model",
    "text": "Test model\n\n# Flatten images\ntest_small_rgb = small_rgb_test.reshape(small_rgb_test.shape[0]*small_rgb_test.shape[1], 3)\ny_test = small_ans_test.reshape(small_ans_test.shape[0]*small_ans_test.shape[1])\n\n\n# Predictions on testing data\nmodel1_test_pred = model1.predict(test_small_rgb)\n\n# Confusion matrix\naccuracy_metrics(y_test, model1_test_pred)\n\nConfusion matrix:\n [[   601   6502]\n [  4818 108079]]\nOverall accuracy: 0.906 \nPrecision: 0.111 \nRecall 0.085 \nDICE: 0.096\n\n\nWhile we still have a good overall accuracy rate, our predictions of roads is substantially worse. We have only classified 8.3% of the road pixels correctly, and only 11.3% of our road predictions were actually roads.\n\n# Convert predictions to image\ntest_preds = model1_test_pred.reshape(small_ans_test.shape[0], small_ans_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\n\nskio.imshow(test_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans_test, ax = ax[1])\nax[1].set_title(\"Actual Solution\");\n\n\n\n\n\n\n\n\nOur model did NOT generalize well! This looks terrible!"
  },
  {
    "objectID": "code/rf.html#create-features",
    "href": "code/rf.html#create-features",
    "title": "Random Forest for Road Segmentation",
    "section": "Create Features",
    "text": "Create Features\nNear the beginning of this notebook, we wrote a function to calculate a number of features that may help with road identification. Here, we call our function and inspect some of those layers.\n\n# Create features\nsmall_rgb_layers = compute_features(small_rgb)\n\n\n# Inspect features\nfig, ax = plt.subplots(2, 2, figsize = (8, 8))\nskio.imshow(small_rgb_layers[:,:,8], ax = ax[0,0])\nax[0,0].set_title(\"Range of RGB\")\nskio.imshow(small_rgb_layers[:,:,3], cmap = \"gray\", ax = ax[0,1])\nax[0,1].set_title(\"Canny Edges Red\")\nskio.imshow(small_rgb_layers[:,:,12:15], ax = ax[1,0])\nax[1,0].set_title(\"Gaussian Blur RGB\")\nskio.imshow(small_rgb_layers[:,:,18:21], ax = ax[1,1])\nax[1,1].set_title(\"Log of Gaussian RGB\");\n\n\n\n\n\n\n\n\nOn the top left is the range of red, green and blue for each pixel. We chose this feature because roads are gray, and in the RGB color space, gray pixels have similar values of red, green and blue.\nOn the top right is canny edges for the red channel. We created this feature hoping to detect the edges of roads, but as you can see, it detects edges in many objects other than roads. We include canny edges in the red, green and blue channels.\nOn the bottom left is the original image after gaussian blurring with \\(\\sigma = 3\\). Our thought process here was that there might be some noise in the image leading random pixels to have the same R, G, and B values as roads. By blurring the image, we hoped to account for this by giving some weight to the values of nearby pixels. We include gaussian blur with \\(\\sigma = 1\\), \\(\\sigma = 3\\), and \\(\\sigma = 5\\) for red, green and blue, hoping that our model might learn from multiple blurring radii.\nOn the bottom right is our image after the log of gaussian filter has been applied to the red, green and blue channels. We hopes to pick up on the width/frequency of roads with this filter, so we included this filter for \\(\\sigma = 0.5\\), \\(\\sigma = 0.6\\), and \\(\\sigma = 0.8\\)."
  },
  {
    "objectID": "code/rf.html#train-model-1",
    "href": "code/rf.html#train-model-1",
    "title": "Random Forest for Road Segmentation",
    "section": "Train Model",
    "text": "Train Model\n\n# Train model\n\n# Flatten image\ntrain_small_rgb_layers = small_rgb_layers.reshape(small_rgb_layers.shape[0]*small_rgb_layers.shape[1], small_rgb_layers.shape[2])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel2 = RF.fit(train_small_rgb_layers, y_train)\n\n# Predictions on training data\nmodel2_pred = model2.predict(train_small_rgb_layers)\n\n# Confusion matrix\naccuracy_metrics(y_train, model2_pred)\n\nConfusion matrix:\n [[ 13022      1]\n [     0 106977]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nBefore adding the additional layers to our training data, our overall accuracy was 0.954, precision was 0.854, and recall was 0.69 on our training data. Now we have virtually perfect results! Let’s look at an image of the output.\n\n# Convert predictions to image\ntrain_preds = model2_pred.reshape(small_ans.shape[0], small_ans.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\n\nskio.imshow(train_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans, ax = ax[1])\nax[1].set_title(\"Actual Solution\")\n\nskio.imshow(train_preds==small_ans, ax = ax[2])\nax[2].set_title(\"Errors (look closely)\");\n\n\n\n\n\n\n\n\nYep, can’t even find the errors without looking closely at the difference between the two images. Let’s evaluate our results on the testing data!"
  },
  {
    "objectID": "code/rf.html#test-model-1",
    "href": "code/rf.html#test-model-1",
    "title": "Random Forest for Road Segmentation",
    "section": "Test Model",
    "text": "Test Model\n\n# Create additional features\nsmall_rgb_test_layers = compute_features(small_rgb_test)\n\n\n# Flatten image\ntest_small_rgb_layers = small_rgb_test_layers.reshape(small_rgb_test_layers.shape[0]*small_rgb_test_layers.shape[1], small_rgb_layers.shape[2])\n\n# Predictions on testing data\nmodel2_test_pred = model2.predict(test_small_rgb_layers)\n\n# Confusion matrix\naccuracy_metrics(y_test, model2_test_pred)\n\nConfusion matrix:\n [[   341   6762]\n [  1865 111032]]\nOverall accuracy: 0.928 \nPrecision: 0.155 \nRecall 0.048 \nDICE: 0.073\n\n\n\n# Convert predictions to image\ntest_preds = model2_test_pred.reshape(small_ans_test.shape[0], small_ans_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\n\nskio.imshow(test_preds, ax = ax[0], cmap = \"gray\")\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans_test, ax = ax[1])\nax[1].set_title(\"Actual Solution\")\n\nskio.imshow(test_preds==small_ans_test, ax = ax[2])\nax[2].set_title(\"Errors\");\n\n\n\n\n\n\n\n\nAdding these filters to our model had negligible impact on our results. It improved the accuracy from 0.906 to 0.927 and the precision from 0.113 to 0.147, but the recall dropped from 0.086 to 0.047. This means that of the pixels that actually represent roads, we are only correctly classifying 4.7% of them. With perfect results on our training data and pitiful results on our testing data, it appears that incorporating these features in our training data led to severe overfitting!"
  },
  {
    "objectID": "code/rf.html#train-rgb-model",
    "href": "code/rf.html#train-rgb-model",
    "title": "Random Forest for Road Segmentation",
    "section": "Train RGB Model",
    "text": "Train RGB Model\nFirst, let’s use this method on a model with just RGB layers.\n\n# Flatten training images\ntrain_rgb = rgb.reshape(rgb.shape[0]*rgb.shape[1], 3)\ny_train = ans.reshape(ans.shape[0]*ans.shape[1])\n\n# Subset training data by label\ny_train_true = y_train[y_train]\ny_train_false = y_train[~y_train]\ntrain_rgb_true = train_rgb[y_train]\ntrain_rgb_false = train_rgb[~y_train]\n\n# Sample indices of each label\ntrue_indices = sample_without_replacement(y_train_true.shape[0], 10000)\nfalse_indices = sample_without_replacement(y_train_false.shape[0], 10000)\n\n# Create modified training data\ny_train_mod = np.concatenate([y_train_true[true_indices[:5000]], y_train_false[false_indices[:5000]]])\ntrain_rgb_mod = np.concatenate([train_rgb_true[true_indices[:5000]], train_rgb_false[false_indices[:5000]]])\n\n# Create modified testing data\ny_test_mod = np.concatenate([y_train_true[true_indices[5000:]], y_train_false[false_indices[5000:]]])\ntest_rgb_mod = np.concatenate([train_rgb_true[true_indices[5000:]], train_rgb_false[false_indices[5000:]]])\n\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel3 = RF.fit(train_rgb_mod, y_train_mod)\n\n# Predictions on training data\nmodel3_pred = model3.predict(train_rgb_mod)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model3_pred)\n\nConfusion matrix:\n [[4962   38]\n [  86 4914]]\nOverall accuracy: 0.988 \nPrecision: 0.983 \nRecall 0.992 \nDICE: 0.988\n\n\nWhile this model does not have 100% training accuracy like the additional layers model, it has improved significantly over the original RGB model. Most notably, the training recall has improved from less than 70% to roughly 99%. Let’s see if we maintain this performance when we make predictions on our testing data."
  },
  {
    "objectID": "code/rf.html#test-rgb-model",
    "href": "code/rf.html#test-rgb-model",
    "title": "Random Forest for Road Segmentation",
    "section": "Test RGB Model",
    "text": "Test RGB Model\n\n# Predictions on testing data\nmodel3_test_pred = model3.predict(test_rgb_mod)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model3_test_pred)\n\nConfusion matrix:\n [[3810 1190]\n [1332 3668]]\nOverall accuracy: 0.748 \nPrecision: 0.741 \nRecall 0.762 \nDICE: 0.751\n\n\nOur results are encouraging! Our overall accuracy, precision, and recall are all approximatly 0.75. In the original RGB model, the overall accuracy was over 90%, while the precision and recall were roughly 10%. By balancing the amount of training data in each class, we were able to balance the different accuracy metrics, improving our predictions of roads at the expense of our predictions of non-roads. Perhaps if we incorporate our additional layers into the model, these balance improvements will translate to balanced and higher accuracy metrics."
  },
  {
    "objectID": "code/rf.html#test-rgb-model-on-new-image",
    "href": "code/rf.html#test-rgb-model-on-new-image",
    "title": "Random Forest for Road Segmentation",
    "section": "Test RGB Model on New Image",
    "text": "Test RGB Model on New Image\nWhile our results above are encouraging, our training and testing data were both drawn from the same image, so our model may have overtrained to this image. Let’s form predictions and compute accuracy metrics on a different image.\n\n# Flatten testing images\nflat_rgb_test = rgb_test.reshape(rgb_test.shape[0]*rgb_test.shape[1], 3)\ny_test = ans_test.reshape(ans_test.shape[0]*ans_test.shape[1])\n\n# Predictions on testing data\nmodel3_test_pred_2 = model3.predict(flat_rgb_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model3_test_pred_2)\n\nConfusion matrix:\n [[  98370   41895]\n [ 342878 1766857]]\nOverall accuracy: 0.829 \nPrecision: 0.223 \nRecall 0.701 \nDICE: 0.338\n\n\nSurprisingly, the overall accuracy is higher in the testing image than in the training image! The recall is still over 70%, indicating that we are capturing most pixels representing roads correctly. With a much lower precision, we must be predicting road pixels frequently where there are not actually roads.\nSince we are working with an entire image, we can inspect our results!\n\n# Convert predictions to image\ntest_preds = model3_test_pred_2.reshape(rgb_test.shape[0], rgb_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\nIt looks like we tend to label pixels as roads when they in reality represent other human features like buildings. We also exaggerate the width of some roads"
  },
  {
    "objectID": "code/rf.html#train-additional-layers-model",
    "href": "code/rf.html#train-additional-layers-model",
    "title": "Random Forest for Road Segmentation",
    "section": "Train Additional Layers Model",
    "text": "Train Additional Layers Model\n\n# Create additional features\ntrain_rgb_mod_layers = compute_features(rgb)\n\n# Flatten training image with extra layers\ntrain_rgb_2 = train_rgb_mod_layers.reshape(train_rgb_mod_layers.shape[0]*train_rgb_mod_layers.shape[1], train_rgb_mod_layers.shape[2])\n\n# Subset training data by label\ntrain_rgb_true_2 = train_rgb_2[y_train]\ntrain_rgb_false_2 = train_rgb_2[~y_train]\n\n# Create modified training data\ntrain_rgb_mod_2 = np.concatenate([train_rgb_true_2[true_indices[:5000]], train_rgb_false_2[false_indices[:5000]]])\n\n# Create modified testing data\ntest_rgb_mod_2 = np.concatenate([train_rgb_true_2[true_indices[5000:]], train_rgb_false_2[false_indices[5000:]]])\n\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel4 = RF.fit(train_rgb_mod_2, y_train_mod)\n\n# Predictions on training data\nmodel4_pred = model4.predict(train_rgb_mod_2)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model4_pred)\n\nConfusion matrix:\n [[5000    0]\n [   0 5000]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nOur training results are literally perfect. Does this translate to our testing data?"
  },
  {
    "objectID": "code/rf.html#test-additional-layers-model",
    "href": "code/rf.html#test-additional-layers-model",
    "title": "Random Forest for Road Segmentation",
    "section": "Test Additional Layers Model",
    "text": "Test Additional Layers Model\n\n# Predictions on testing data\nmodel4_test_pred = model4.predict(test_rgb_mod_2)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model4_test_pred)\n\nConfusion matrix:\n [[4178  822]\n [1133 3867]]\nOverall accuracy: 0.804 \nPrecision: 0.787 \nRecall 0.836 \nDICE: 0.81\n\n\nIt appears that there were some errors on our testing data. Going from the RGB model to the additional layers model, our overall accuracy improved from 0.755 to 0.812, the precision improved from 0.749 to 0.796, and the recall improved from 0.767 to 0.84. These are the most accurate road predictions yet!\nWhile our training and testing data contained none of the same pixels, they were both drawn from the same image, so it is possible that they were overtrained to our particular image of choice. Perhaps a more valid testing metric would involve testing our model on pixels from a different image. Let’s form predictions and compute accuracy metrics on the entirety of another image."
  },
  {
    "objectID": "code/rf.html#test-additional-layers-model-on-new-image",
    "href": "code/rf.html#test-additional-layers-model-on-new-image",
    "title": "Random Forest for Road Segmentation",
    "section": "Test Additional Layers Model on New Image",
    "text": "Test Additional Layers Model on New Image\n\n# Create additional features\ntest_rgb_layers_3 = compute_features(rgb_test)\n\n# Flatten testing images\ntest_rgb_3 = test_rgb_layers_3.reshape(test_rgb_layers_3.shape[0]*test_rgb_layers_3.shape[1], test_rgb_layers_3.shape[2])\n\n\n# Predictions on testing data\nmodel4_test_pred_2 = model4.predict(test_rgb_3)\n\n# Confusion matrix\naccuracy_metrics(y_test, model4_test_pred_2)\n\nConfusion matrix:\n [[  91162   49103]\n [ 253648 1856087]]\nOverall accuracy: 0.865 \nPrecision: 0.264 \nRecall 0.65 \nDICE: 0.376\n\n\nSimilar to the RGB model, the results on the testing image were largely similar to the results on the previous image, except for the precision dropping by over 50% The overall accuracy is over 85%, but the recall is now 65.9% and the precision is now 26.6%. While this is certainly not perfect, the precision and recall are still a substantial improvement over the models without sampling. However, the recall was actually slightly higher in the sampled RGB model, indicating that the RGB model generalized better in terms of predicting road pixels. Perhaps there are tactics we can use to combat overfitting.\nAlso, since we are now working with a complete image, we can once again inspect a full image illustrating our predictions versus the truth.\n\n# Convert predictions to image\ntest_preds = model4_test_pred_2.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\nOnce again, our model tends to incorrectly predict roads where there are other human features like buildings, and it exaggerates the width of some roads."
  },
  {
    "objectID": "code/rf.html#train-model-2",
    "href": "code/rf.html#train-model-2",
    "title": "Random Forest for Road Segmentation",
    "section": "Train Model",
    "text": "Train Model\nFirst, we apply Principal Component Analysis to our training data and plot the percent variance explained by each component. Note that PCA is only applicable for continuous features, so we cannot include binary features such as canny edges in this model.\n\n# Add layers to model\ntrain_pca_layers = compute_features(rgb, include_categorical = False)   \n\n# Flatten training image with extra layers\ntrain_pca_layers_flat = train_pca_layers.reshape(train_pca_layers.shape[0]*train_pca_layers.shape[1], train_pca_layers.shape[2])\n\n# Standardize the features\nscaler = StandardScaler()\ntrain_pca_layers_scaled = scaler.fit_transform(train_pca_layers_flat)\n\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=train_pca_layers.shape[2])\nlayers_pca = pca.fit_transform(train_pca_layers_scaled)\n\n# Explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Plotting the explained variance ratio\nplt.figure(figsize=(8, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\nApparently, over 50% of the variance in our data can be explained by the first component! The second component only accounts for about 11.5% of the variance in the data, and the numbers continue to drop after that.\n\nexplained_variance_ratio[0:5].sum()\n\n0.843420909342875\n\n\nApparently, the first 5 components account for over 84% of the variation in our data. Let’s try only retaining the first 5 components for our model and seeing whether our performance improves.\n\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=5)\nlayers_pca = pca.fit_transform(train_pca_layers_scaled)\n\n# Subset training data by label\nlayers_pca_true = layers_pca[y_train]\nlayers_pca_false = layers_pca[~y_train]\n\n# Create modified training data\nlayers_pca_mod_train = np.concatenate([layers_pca_true[true_indices[:5000]], layers_pca_false[false_indices[:5000]]])\n\n# Create modified testing data\nlayers_pca_mod_test = np.concatenate([layers_pca_true[true_indices[5000:]], layers_pca_false[false_indices[5000:]]])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel5 = RF.fit(layers_pca_mod_train, y_train_mod)\n\n# Predictions on training data\nmodel5_pred = model5.predict(layers_pca_mod_train)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model5_pred)\n\nConfusion matrix:\n [[5000    0]\n [   0 5000]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nPer usual, our model’s performance is perfect on the training data."
  },
  {
    "objectID": "code/rf.html#test-model-2",
    "href": "code/rf.html#test-model-2",
    "title": "Random Forest for Road Segmentation",
    "section": "Test Model",
    "text": "Test Model\n\n# Predictions on testing data\nmodel5_test_pred = model5.predict(layers_pca_mod_test)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model5_test_pred)\n\nConfusion matrix:\n [[3946 1054]\n [1477 3523]]\nOverall accuracy: 0.747 \nPrecision: 0.728 \nRecall 0.789 \nDICE: 0.757\n\n\nOn the testing data, all of our model’s performance metrics are lower than its non-PCA counterpart, although not by that much."
  },
  {
    "objectID": "code/rf.html#test-model-on-new-image",
    "href": "code/rf.html#test-model-on-new-image",
    "title": "Random Forest for Road Segmentation",
    "section": "Test Model on New Image",
    "text": "Test Model on New Image\n\n# Add layers to model\ntest_pca_layers = compute_features(rgb_test, include_categorical = False)\n\n# Flatten testing images\ntest_pca_layers_flat = test_pca_layers.reshape(test_pca_layers.shape[0]*test_pca_layers.shape[1], 22)\n\n# Standardize the features\ntest_pca_layers_scaled = scaler.fit_transform(test_pca_layers_flat)\n\n# Project onto principal components\nlayers_pca_test = pca.transform(test_pca_layers_scaled)\n\n\n# Predictions on testing data\nmodel5_test_pred_2 = model5.predict(layers_pca_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model5_test_pred_2)\n\nConfusion matrix:\n [[  69301   70964]\n [ 511205 1598530]]\nOverall accuracy: 0.741 \nPrecision: 0.119 \nRecall 0.494 \nDICE: 0.192\n\n\nAgain, on the testing image, all of our model’s performance metrics are lower than its non-PCA counterpart. In this scenario, it appears that the components explaining very little variation in the data were actually somewhat useful for predictions. Note that we tried this with a variety of number of retained components, and we found that the model’s performance improved as we increased the number of components.\nBelow, we inspect the image of our predictions.\n\n# Convert predictions to image\ntest_preds = model5_test_pred_2.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\nVisually, our predictions appear somewhat worse that those from our non-PCA additional layers model. There is overall a lot more noise in our predictions, and interestingly, we are not predicting a road in much of the massive highway."
  },
  {
    "objectID": "code/rf.html#training-sample-rgb",
    "href": "code/rf.html#training-sample-rgb",
    "title": "Random Forest for Road Segmentation",
    "section": "Training Sample RGB",
    "text": "Training Sample RGB\n\n# Store id's of images\nimgs = [\"10828735_15\", \"10228675_15\", \"10228705_15\", \"10228720_15\", \"10228735_15\", \"10528675_15\", \"10528750_15\", \"10978720_15\", \"11128825_15\", \"12028750_15\"]\n\n# Initialize arrays to store training data\ny_train = np.array([])\nrgb_train = np.zeros((0,3))\n\n# Sample from each image\nfor img in imgs:\n    rgb = skio.imread(\"../../data/MA_roads/tiff/train/\" + img + \".tiff\")\n    ans = skio.imread(\"../../data/MA_roads/tiff/train_labels/\" + img + \".tif\") &gt; 0\n    \n    # Flatten training images\n    rgb_flat = rgb.reshape(rgb.shape[0]*rgb.shape[1], 3)\n    ans_flat = ans.reshape(ans.shape[0]*ans.shape[1])\n    \n    # Subset training data by label\n    ans_true = ans_flat[ans_flat]\n    ans_false = ans_flat[~ans_flat]\n    rgb_true = rgb_flat[ans_flat]\n    rgb_false = rgb_flat[~ans_flat]\n    \n    # Sample indices of each label\n    true_indices = sample_without_replacement(ans_true.shape[0], 5000)\n    false_indices = sample_without_replacement(ans_false.shape[0], 5000)\n    \n    # Create modified training data\n    y_train = np.concatenate([y_train, ans_true[true_indices], ans_false[false_indices]])\n    rgb_train = np.concatenate([rgb_train, rgb_true[true_indices], rgb_false[false_indices]])"
  },
  {
    "objectID": "code/rf.html#train-rgb-model-1",
    "href": "code/rf.html#train-rgb-model-1",
    "title": "Random Forest for Road Segmentation",
    "section": "Train RGB Model",
    "text": "Train RGB Model\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel6 = RF.fit(rgb_train, y_train)\n\n# Predictions on training data\nmodel6_pred = model6.predict(rgb_train)\n\n# Confusion matrix\naccuracy_metrics(y_train, model6_pred)\n\nConfusion matrix:\n [[48147  1853]\n [ 2687 47313]]\nOverall accuracy: 0.955 \nPrecision: 0.947 \nRecall 0.963 \nDICE: 0.955"
  },
  {
    "objectID": "code/rf.html#test-rgb-model-on-new-image-1",
    "href": "code/rf.html#test-rgb-model-on-new-image-1",
    "title": "Random Forest for Road Segmentation",
    "section": "Test RGB Model on New Image",
    "text": "Test RGB Model on New Image\n\n# Predictions on testing data\nmodel6_test_pred = model6.predict(flat_rgb_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model6_test_pred)\n\nConfusion matrix:\n [[ 109933   30332]\n [ 377915 1731820]]\nOverall accuracy: 0.819 \nPrecision: 0.225 \nRecall 0.784 \nDICE: 0.35\n\n\n\n# Convert predictions to image\ntest_preds = model6_test_pred.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");"
  },
  {
    "objectID": "code/rf.html#training-sample-additional-layers",
    "href": "code/rf.html#training-sample-additional-layers",
    "title": "Random Forest for Road Segmentation",
    "section": "Training Sample Additional Layers",
    "text": "Training Sample Additional Layers\n\n# Store id's of images\nimgs = [\"10828735_15\", \"10228675_15\", \"10228705_15\", \"10228720_15\", \"10228735_15\", \"10528675_15\", \"10528750_15\", \"10978720_15\", \"11128825_15\", \"12028750_15\"]\n\n# Initialize arrays to store training data\ny_train = np.array([])\nrgb_train = np.zeros((0,27))\n\n# Sample from each image\nfor img in imgs:\n    rgb = skio.imread(\"../../data/MA_roads/tiff/train/\" + img + \".tiff\")\n    ans = skio.imread(\"../../data/MA_roads/tiff/train_labels/\" + img + \".tif\") &gt; 0\n\n    # Create additional layers\n    rgb_layers = compute_features(rgb)\n    \n    # Flatten training images\n    rgb_flat = rgb_layers.reshape(rgb_layers.shape[0]*rgb_layers.shape[1], rgb_layers.shape[2])\n    ans_flat = ans.reshape(ans.shape[0]*ans.shape[1])\n    \n    # Subset training data by label\n    ans_true = ans_flat[ans_flat]\n    ans_false = ans_flat[~ans_flat]\n    rgb_true = rgb_flat[ans_flat]\n    rgb_false = rgb_flat[~ans_flat]\n    \n    # Sample indices of each label\n    true_indices = sample_without_replacement(ans_true.shape[0], 5000)\n    false_indices = sample_without_replacement(ans_false.shape[0], 5000)\n    \n    # Create modified training data\n    y_train = np.concatenate([y_train, ans_true[true_indices], ans_false[false_indices]])\n    rgb_train = np.concatenate([rgb_train, rgb_true[true_indices], rgb_false[false_indices]])"
  },
  {
    "objectID": "code/rf.html#train-additional-layers-model-1",
    "href": "code/rf.html#train-additional-layers-model-1",
    "title": "Random Forest for Road Segmentation",
    "section": "Train Additional Layers Model",
    "text": "Train Additional Layers Model\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel7 = RF.fit(rgb_train, y_train)\n\n# Predictions on training data\nmodel7_pred = model7.predict(rgb_train)\n\n# Confusion matrix\naccuracy_metrics(y_train, model7_pred)\n\nConfusion matrix:\n [[49998     2]\n [    1 49999]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0"
  },
  {
    "objectID": "code/rf.html#test-additional-layers-model-on-new-image-1",
    "href": "code/rf.html#test-additional-layers-model-on-new-image-1",
    "title": "Random Forest for Road Segmentation",
    "section": "Test Additional Layers Model on New Image",
    "text": "Test Additional Layers Model on New Image\n\n# Predictions on testing data\nmodel7_test_pred = model7.predict(test_rgb_3)\n\n# Confusion matrix\naccuracy_metrics(y_test, model7_test_pred)\n\nConfusion matrix:\n [[ 115326   24939]\n [ 322235 1787500]]\nOverall accuracy: 0.846 \nPrecision: 0.264 \nRecall 0.822 \nDICE: 0.399\n\n\n\n# Convert predictions to image\ntest_preds = model7_test_pred.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");"
  },
  {
    "objectID": "pdal_tutorial/pdal_tutorial.html",
    "href": "pdal_tutorial/pdal_tutorial.html",
    "title": "",
    "section": "",
    "text": "From https://pdal.io/en/2.6.0/tutorial/iowa-entwine.html#install-pdal\nFirst, follow their instructions for installing pdal.\nThen run the code below (I copied their iowa.json and dem-colors.txt files)\n\n!pdal pipeline iowa.json --debug\n\n\n!gdaldem color-relief iowa.tif dem-colors.txt iowa-color.png\n\nPDAL is a C++ library for handling point cloud data. PDAL provides some Python support and understanding that more completely would require reading their documentation: https://pdal.io/en/2.7-maintenance/python.html#python"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Road Segmentation",
    "section": "",
    "text": "Semantic segmentation is a process that takes an input image and returns an output image of the same size where each pixel belongs to a particular class of objects, not distinguishing between objects within a class. For this study, the resulting image is binary, true meaning that a pixel is part of the class we are trying to segment and false meaning a pixel is not part of the class.\nThe purpose of this project was to compare different methods to use satellite data to segment roads. Three machine learning methodologies were used and each had increasing complexity and computational demands. These methods were K-Means Clustering, Random Forests, and Deep Learning using the UNet architecture. K-Means clustering is an unsupervised machine learning algorithm that partitions the data in k clusters, where k is determined by the user based on how many categories you want to cluster. It iteratively updates the centroids of each cluster using mean coordinates until convergence is achieved. Random forest classification is an ensemble learning method involving many decision trees. Each decision tree is trained on a random subset of observations and features, and predictions of the random forest are based on a plurality vote of the decision trees. The UNet architecture for deep neural networks was used as the final method. UNet consists of two convolutional neural networks put together: an encoder and a decoder. The encoder analyzes and extracts features and important information from the input image and the decoder takes this information and generates the binary segmentation image.\nEach method was implemented and trained, if necessary, and then segmentation metrics were tested to see how well each method was performing. The metrics used were precision, recall, and Dice Coefficient. In addition to comparing the performance of each method, we discuss the computational power, training data, and time needed in order to implement them. The dataset used was Massachusetts Road Dataset, which consists of 1110 training, 14 validation, and 49 testing satellite image and label pairs. Each image is 1500 by 1500 pixels at 1 meter resolution, resulting in each imaging capturing 2.25 square kilometers.\nThis report is a summary of our work, and does not include all code. In particular, training the U-Net model required substantial computational effort and we did not re-run the entire model in this notebook. For full documentation of our work, please visit our GitHub repository."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Machine Learning for Road Segmentation",
    "section": "",
    "text": "Semantic segmentation is a process that takes an input image and returns an output image of the same size where each pixel belongs to a particular class of objects, not distinguishing between objects within a class. For this study, the resulting image is binary, true meaning that a pixel is part of the class we are trying to segment and false meaning a pixel is not part of the class.\nThe purpose of this project was to compare different methods to use satellite data to segment roads. Three machine learning methodologies were used and each had increasing complexity and computational demands. These methods were K-Means Clustering, Random Forests, and Deep Learning using the UNet architecture. K-Means clustering is an unsupervised machine learning algorithm that partitions the data in k clusters, where k is determined by the user based on how many categories you want to cluster. It iteratively updates the centroids of each cluster using mean coordinates until convergence is achieved. Random forest classification is an ensemble learning method involving many decision trees. Each decision tree is trained on a random subset of observations and features, and predictions of the random forest are based on a plurality vote of the decision trees. The UNet architecture for deep neural networks was used as the final method. UNet consists of two convolutional neural networks put together: an encoder and a decoder. The encoder analyzes and extracts features and important information from the input image and the decoder takes this information and generates the binary segmentation image.\nEach method was implemented and trained, if necessary, and then segmentation metrics were tested to see how well each method was performing. The metrics used were precision, recall, and Dice Coefficient. In addition to comparing the performance of each method, we discuss the computational power, training data, and time needed in order to implement them. The dataset used was Massachusetts Road Dataset, which consists of 1110 training, 14 validation, and 49 testing satellite image and label pairs. Each image is 1500 by 1500 pixels at 1 meter resolution, resulting in each imaging capturing 2.25 square kilometers.\nThis report is a summary of our work, and does not include all code. In particular, training the U-Net model required substantial computational effort and we did not re-run the entire model in this notebook. For full documentation of our work, please visit our GitHub repository."
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Machine Learning for Road Segmentation",
    "section": "Methods",
    "text": "Methods\n\nPackages and Functions\nBefore thoroughly describing our work, we import necessary packages and define a few functions.\n\n\nCode\n# Import packages\nimport skimage.io as skio\nimport skimage.util as sku\nimport skimage.color as skol\nfrom skimage import filters, feature, transform\nimport skimage.morphology as skm\nimport skimage.draw as draw\nfrom skimage.color import rgb2gray\nfrom scipy.signal import convolve2d\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d\nimport numpy as np\nimport cv2\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.ndimage import gaussian_filter, gaussian_laplace\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import datasets\nimport tensorflow.keras.backend as K\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.random import sample_without_replacement\nfrom scipy.ndimage import gaussian_filter\nfrom scipy.ndimage import gaussian_laplace\nfrom scipy.ndimage import maximum_filter\nfrom scipy.ndimage import minimum_filter\nfrom scipy.ndimage import median_filter\n\n\n\n\nCode\n# Function to compute DICE\nsmooth=1\ndef dice_coef(y_true, y_pred):\n    intersection = np.sum(y_true * y_pred)\n    return (2. * intersection + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)\n        \n# Function to print several accuracy metrics\ndef accuracy_metrics(y_true, y_pred):\n    # Create confusion matrix\n    C = confusion_matrix(y_true, y_pred, labels=(True, False))\n\n    # Overall accuracy rate\n    acc = (C[0,0] + C[1,1])/C.sum()\n\n    # Recall\n    recall = (C[0,0])/(C[0,0] + C[1,0])\n    \n    # Precision\n    prec = (C[0,0])/(C[0,0] + C[0,1])\n\n    # DICE\n    dice = dice_coef(y_true, y_pred)\n\n\n    # Print results\n    print(\"Confusion matrix:\\n\", C)\n    print(\"Overall accuracy:\", np.round(acc, 3), \"\\nPrecision:\", np.round(recall, 3),\n            \"\\nRecall\", np.round(prec, 3), \"\\nDICE:\", np.round(dice, 3))\n\n# Function to create input layer\ndef classify_gray(image):\n\n    # Compute the standard deviation of the r, g, and b channels\n    std_dev = np.std(image, axis = 2)\n\n    # Define a threshold for classifying gray pixels\n    diff_threshold = 6 # Adjust as needed\n\n    # Classify pixels as gray or not gray based on the standard deviation\n    gray_mask = std_dev &lt; diff_threshold\n        \n    return gray_mask\n    \n# Function to compute layers for additional model features\ndef compute_features(img, include_categorical = True):    \n    # Range of values (gray pixels will have low range)\n    r = img.max(axis = 2) - img.min(axis = 2)\n\n    if include_categorical:\n        \n        # Canny edge detection\n        canny_edges_r = feature.canny(img[:,:,0], sigma=4)\n        canny_edges_g = feature.canny(img[:,:,1], sigma=4)\n        canny_edges_b = feature.canny(img[:,:,2], sigma=4)\n\n         # Calculation Canny gradient\n        image_gray = rgb2gray(img)\n        canny_edges = feature.canny(image_gray, sigma=3)\n    \n        # Create disk\n        disk = skm.disk(1)\n    \n        # Area closing for hough lines\n        closed_edges = skm.dilation(canny_edges, footprint = disk)\n        closed_edges = closed_edges * 255\n    \n        lines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n        hough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n    \n        # Draw the detected lines on the canvas\n        for line in lines:\n            p0, p1 = line\n            # Draw line segment\n            rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n            hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n    \n        #create gray mask\n        gray_mask = classify_gray(img)\n        gray_mask = gray_mask.reshape((img.shape[0], img.shape[1]))\n    \n\n        img = np.dstack([img, canny_edges_r, canny_edges_g, canny_edges_b, gray_mask, hough_lines])\n        \n    \n    # Gaussian blur sigma = 1\n    gaus_r_1 = gaussian_filter(img[:,:,0], sigma = 1)\n    gaus_g_1 = gaussian_filter(img[:,:,1], sigma = 1)\n    gaus_b_1 = gaussian_filter(img[:,:,2], sigma = 1)\n    \n    # Gaussian blur sigma = 3\n    gaus_r_3 = gaussian_filter(img[:,:,0], sigma = 3)\n    gaus_g_3 = gaussian_filter(img[:,:,1], sigma = 3)\n    gaus_b_3 = gaussian_filter(img[:,:,2], sigma = 3)\n\n    # Gaussian blur sigma = 5\n    gaus_r_5 = gaussian_filter(img[:,:,0], sigma = 5)\n    gaus_g_5 = gaussian_filter(img[:,:,1], sigma = 5)\n    gaus_b_5 = gaussian_filter(img[:,:,2], sigma = 5)\n    \n    # LoG blur sigma = .5\n    log_r_5 = gaussian_laplace(img[:,:,0], sigma = .5)\n    log_g_5 = gaussian_laplace(img[:,:,1], sigma = .5)\n    log_b_5 = gaussian_laplace(img[:,:,2], sigma = .5)\n    \n    # LoG blur sigma = .6\n    log_r_6 = gaussian_laplace(img[:,:,0], sigma = .6)\n    log_g_6 = gaussian_laplace(img[:,:,1], sigma = .6)\n    log_b_6 = gaussian_laplace(img[:,:,2], sigma = .6)\n    \n    # LoG blur sigma = .8\n    log_r_8 = gaussian_laplace(img[:,:,0], sigma = .8)\n    log_g_8 = gaussian_laplace(img[:,:,1], sigma = .8)\n    log_b_8 = gaussian_laplace(img[:,:,2], sigma = .8)\n    \n    # Add layers to model\n    return np.dstack([img, r,\n                     gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                     gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                     log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8])\n\ndef identify_road_cluster(clustered_image, image_label):\n\n    cluster_labels = np.unique(clustered_image)\n\n    best_recall = 0\n    best_cluster = -1\n\n    for i in cluster_labels:\n        cluster = (clustered_image==i)\n        C = confusion_matrix(image_label.ravel(), cluster.ravel(), labels=(True, False))\n\n        # # Overall accuracy rate\n        # acc = (C[0,0] + C[1,1])/C.sum()\n        # # Recall\n        recall = (C[0,0])/(C[0,0] + C[1,0])\n        # Precision\n        # prec = (C[0,0])/(C[0,0] + C[0,1])\n\n        if recall &gt; best_recall:\n            best_recall = recall\n            best_cluster = i\n\n    return best_cluster\n\n\n\n\nImport Images\nFirst, we import the images that we will use for most of our work. For the k-means and random forest sections, we use one image for training and another for testing, so we display those images here.\n\n\nCode\n# Read the data\nrgb = skio.imread(\"../data/MA_roads/tiff/train/10828735_15.tiff\")\nans = skio.imread(\"../data/MA_roads/tiff/train_labels/10828735_15.tif\") &gt; 0\n\nrgb_test = skio.imread(\"../data/MA_roads/tiff/train/21929005_15.tiff\")\nans_test = skio.imread(\"../data/MA_roads/tiff/train_labels/21929005_15.tif\") &gt; 0\n\n# Display training data and correct output\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(rgb, ax = ax[0])\nax[0].set_title(\"Training Data\")\nskio.imshow(ans, ax = ax[1])\nax[1].set_title(\"Training Solution\");\n\n# Display testing data and correct output\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Data\")\nskio.imshow(ans_test, ax = ax[1])\nax[1].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Image Filters\nIn our k-means and random forest sections, we will compute many features to help identify specific aspects of roads. In the previous section, we defined a function for creating this features. Let us take a closer look at a smaller area and then apply our filters to that area.\n\n\nCode\n# Create training subset of data\nsmall_rgb = rgb[0:400, 1200:, :]\nsmall_ans = ans[0:400, 1200:]\n\n# Create testing subset of data\nsmall_rgb_test = rgb[1200:, 0:400, :]\nsmall_ans_test = ans[1200:, 0:400]\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(small_rgb, ax = ax[0])\nax[0].set_title(\"Training Image\")\nskio.imshow(small_ans, ax = ax[1])\nax[1].set_title(\"Training Image\");\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(small_rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(small_ans_test, ax = ax[1])\nax[1].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let us create and inspect our features.\n\n\nCode\n# Create features\nsmall_rgb_layers = compute_features(small_rgb)\n\n# Inspect features\nfig, ax = plt.subplots(2, 3, figsize = (8, 8))\nskio.imshow(small_rgb_layers[:,:,8], ax = ax[0,0])\nax[0,0].set_title(\"Range of RGB\")\nskio.imshow(small_rgb_layers[:,:,3], cmap = \"gray\", ax = ax[0,1])\nax[0,1].set_title(\"Canny Edges Red\")\nskio.imshow(small_rgb_layers[:,:,6], cmap = \"gray\", ax = ax[0,2])\nax[0,2].set_title(\"Gray Mask\")\nskio.imshow(small_rgb_layers[:,:,7], cmap = \"gray\", ax = ax[1,0])\nax[1,0].set_title(\"Hough Transform\")\nskio.imshow(small_rgb_layers[:,:,12:15], ax = ax[1,1])\nax[1,1].set_title(\"Gaussian Blur RGB\")\nskio.imshow(small_rgb_layers[:,:,18:21], ax = ax[1,2])\nax[1,2].set_title(\"Log of Gaussian RGB\");\n\n\n\n\n\n\n\n\n\nOn the top left is the range of red, green and blue for each pixel. We chose this feature because roads are gray, and in the RGB color space, gray pixels have similar values of red, green and blue.\nOn the top middle is canny edges for the red channel. We created this feature hoping to detect the edges of roads, but as you can see, it detects edges in many objects other than roads. We include canny edges in the red, green and blue channels.\nOn the top right is the graymask created using standard deviation amongst the RGB channels and a threshold. It captures the roads, but also many other features.\nOn the bottom left is the hough transform image, which captures the long linear nature of roads but does not successfully detect every pixel.\nOn the bottom middle is the original image after gaussian blurring with \\(\\sigma = 3\\). Our thought process here was that there might be some noise in the image leading random pixels to have the same R, G, and B values as roads. By blurring the image, we hoped to account for this by giving some weight to the values of nearby pixels. We include gaussian blur with \\(\\sigma = 1\\), \\(\\sigma = 3\\), and \\(\\sigma = 5\\) for red, green and blue, hoping that our model might learn from multiple blurring radii.\nOn the bottom right is our image after the log of gaussian filter has been applied to the red, green and blue channels. We hopes to pick up on the width/frequency of roads with this filter, so we included this filter for \\(\\sigma = 0.5\\), \\(\\sigma = 0.6\\), and \\(\\sigma = 0.8\\).\n\n\nPrinciple Components Analysis\nPrincipal Component Analysis (PCA) was utilized to assess the possibility of cutting down on the number of features input into the various models. Currently there are about 27 filtered and calculated features extracted from each 1500x1500x3 RGB image. When scaling this process, the ability to cut down on preprocessing steps and decrease the size of the image input into the models would provide large increases in the speed and storage needed to run this process. This is where PCA has the potential to help, PCA is a commonly used tool for dimensionality reduction across data science. PCA is especially helpful when a dataset has multiple potentially correlated and redundant features and it is unclear which features are most important. In this situation, many of the features generated from our satellite images may provide very similar information or are highly correlated. PCA would allow us to use less layers while retaining the maximum amount of variance.\n\n\nK-Means Clustering\nK-Means Clustering is a basic unsupervised machine learning algorithm that groups data points into k clusters, the value of k can be chosen by the user. The centroids of each cluster are initialized randomly as three data points and all points are assigned to the cluster nearest to them. Then the mean coordinates of all the points in each cluster is calculated and this value is now the new centroid for that cluster. Then the process is repeated until a maximum number of interactions is reached or the centroids spot changing significantly between each interaction. Although there were only two categories in this study; road and background, k was chosen to be three. This is because it was found by (Maurya, et al) to keep the clusters containing the roads from including other non-road features.\nK-Means clustering was run on features extracted from the full 1500x1500 images, and two sets of features were used. First was all features discussed in the previous section and the second was the top 5 principle components accessed by PCA. We will compare the results of these two approaches to K-Means clustering and then compare the results of K-Means in general to the results from our other models.\n\n\nRandom Forest\nRandom forest is a highly flexible supervised machine learning algorithm that can perform both regression and classificiation and can take both continuous and categorical data as input. Random forest is an ensemble learning method that works by training a specified number of decision trees on the training data. Each decision tree is trained using a random subset of training data and features, and essentially is composed of a root node and a number of internal/decision nodes. Each data point is passed to the root of the decision tree, and at each decision node, either travels to the node’s left or right child depending on the value of one feature of the data point. The data point is passed through the tree until it reaches a terminal node, at which point it is classified or given a predicted value.\nIn the random forest model, classification predictions are created based on a majority vote of the decision trees, while regression predictions are created based on averaging the output of the decision trees. For our work, we consider each pixel as an observation and we use the random forest model to classify pixels as either road or non-road. Due to computational constraints, we did not train a model using the entirety of multiple images – we don’t even train a model on a single image. Each image has \\(2,250,000\\) pixels, and we ran this part of our analysis on our personal computers. Instead, we created our first model by training on the RGB channels of a subset of one image. In the second model, we trained on the RBG channels plus all of the additional layers mentioned above. We noticed that our model tended to produce far more accurate predictions on non-road pixels than road pixels, so we then sampled an image for an equal number of pixels of each class and trained both RGB and additional layer models on that input. Then, we attempted to prevent overfitting by fitting a random forest on the additional layers input reduced to 5 features via PCA. Finally, we trained both RGB and additional layers models on sampled data from multiple images. In the results section, we compare the performance of all of our models.\n\n\nU-Net\nA U-Net model is a convolutional neural network architecture which is designed for segmentation. The architecture is characterized by it’s symmetrical “U-shaped” design composed of an encoder path and a decoder path. The encoder path takes as input the original image and consists of multiple iterations of convolutional layers followed by max pooling layers. This path is designed to take the context and identify key features and reduce the spatial dimensions between each layer. The decoder path is unique to U-Net neural networks, as it attempts to construct a mask using the features learned in the encoder path and regaining the spatial dimensions lost. It consists of a series of up-sampling layers (often using transposed convolutions or interpolation) followed by concatenation with feature maps from the corresponding contracting path. Throughout this process, a skip connection is made between each layer of the two paths to preserve the high resolution between each layer. The final layer of the U-Net typically uses a 1x1 convolution followed by an activation function (e.g., sigmoid or softmax) to produce the final segmentation mask or output. This layer condenses the information learned by the network into the desired output format, such as a binary mask for semantic segmentation tasks.\nOur U-Net model is comprised of 10 layers: 5 encoding layers and 5 decoding layers. We were limited by resources like time and memory, so instead of fitting the model with 1500 x 1500 pixel images, we used 128 x128 pixels. We also divided each 1500 x 1500 into 128 x 128 images, increasing the training size tremendously. Images and masks without roads were removed from the dataset. The model is optimized using the Adam algorithm with a learning rate of 0.01. Our loss is calculated using the Binary Cross-Entropy function. Like previous models, we measured the strength of our model using the dice coefficient."
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "Machine Learning for Road Segmentation",
    "section": "Results",
    "text": "Results\n\nPCA\nRead in our PCA example image and label and crop them\n\n\nCode\npca_img = skio.imread(\"../data/MA_roads/tiff/train/10528735_15.tiff\")\npca_img_label = skio.imread(\"../data/MA_roads/tiff/train_labels/10528735_15.tif\")\n\nx1, x2 = 1200, 1300\ny1, y2 = 300, 375\npca_img_cropped = pca_img[y1:y2, x1:x2, :]\npca_img_label_cropped = pca_img_label[y1:y2, x1:x2]\n\nfig, axes = plt.subplots(1,2, figsize = (10,15))\n\naxes[0].imshow(pca_img_cropped)\naxes[1].imshow(pca_img_label_cropped, cmap='gray')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThis image will be used to demostrate what is being done with PCA and to assess how well the dimension reduction and variance maximization of PCA group of road points and background points into distnguishable clusters. A much smaller image is used so that the data points don’t just appear as a large mass of the size of graph that is able to be shown. For this example, we picked a straight forward patch with a perpedicular road intersection.\nGet all filters of the image\n\n\nCode\npca_filters = compute_features(pca_img_cropped)\n\n\nShow top 5 principle components\n\n\nCode\npca_layers = pca_filters.reshape(pca_filters.shape[0] * pca_filters.shape[1], pca_filters.shape[2])\n# Standardize the features\nscaler = StandardScaler()\npca_layers_scaled = scaler.fit_transform(pca_layers)\n\n# Initialize PCA and fit the scaled data\npca_5 = PCA(n_components=5)\n# layers_pca = pca_10.fit_transform(pca_layers_scaled)\npca_5_comps = pca_5.fit_transform(pca_layers_scaled)\n\n# Explained variance ratio\nexplained_variance_ratio_5 = pca_5.explained_variance_ratio_\n\n# Plotting the explained variance ratio\nplt.figure(figsize=(6, 4))\nplt.bar(range(1, len(explained_variance_ratio_5) + 1), explained_variance_ratio_5, alpha=0.5, align='center')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\n\nThis graph shows the percent of variance explained by the top 5 principle components, it can be interpretted that since the top component explains about 45% of variance, the second component explains about 15%, and so on. Therefore, in reducing our dimensions to just 5 we still maintain around 70-80% of the variance that our previous 27 features had.\nIn order to visualize the new dimensinos that PCA can output, we will use the top 3 components to plot our road and background points in 3D space\n\n\nCode\n# Initialize PCA and fit the scaled data\npca_3 = PCA(n_components=3)\n# layers_pca = pca_10.fit_transform(pca_layers_scaled)\npca_3_comps = pca_3.fit_transform(pca_layers_scaled)\n\n\nDisplay labelled data points in 3D\n\n\nCode\nfig = plt.figure(1, figsize=(6, 6))\nplt.clf()\n\nax = fig.add_subplot(111, projection=\"3d\", elev=15, azim=90)\nax.set_position([0, 0, 0.95, 1])\n\nX = pca_3_comps\ny = pca_img_label_cropped.ravel()\n\nfor name, label in [(\"Background\", 0), (\"Road\", 255)]:\n    ax.text3D(\n        X[y == label, 0].mean(),\n        X[y == label, 1].mean() + 1.5,\n        X[y == label, 2].mean(),\n        name,\n        horizontalalignment=\"center\",\n        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n    )\n# Reorder the labels to have colors matching the cluster results\n# y = np.choose(y, [0, 255]).astype(float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='Accent', edgecolor=\"k\")\n\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThere is some clear distinction between the road and backround points but also still a lot of overlap. This shows that the problem of segmenting roads will be difficult but possible.\n\n\nK-Means Clustering\nLoad in test image, this image will be used to test k-Means and future methods\n\n\nCode\ntest_image = skio.imread(\"../data/MA_roads/tiff/train/21929005_15.tiff\")\ntest_image_label = skio.imread(\"../data/MA_roads/tiff/train_labels/21929005_15.tif\")\ntest_image_label_bool = (test_image_label==255)\n\n\n\n\nCode\nfig, axes = plt.subplots(1,2, figsize = (10,15))\n\naxes[0].imshow(test_image)\naxes[1].imshow(test_image_label, cmap='gray')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nGet all feature layers and PCA top-5 layers for input into K-Means\n\n\nCode\ntest_filters = compute_features(test_image)\ntest_layers = test_filters.reshape(test_filters.shape[0] * test_filters.shape[1], test_filters.shape[2])\n\n# Standardize the features\ntest_layers_scaled = scaler.fit_transform(test_layers)\n\n# Initialize PCA and fit the scaled data\ntest_pca_5 = PCA(n_components=5)\n# layers_pca = pca_10.fit_transform(pca_layers_scaled)\ntest_pca_5_layers = test_pca_5.fit_transform(test_layers_scaled)\n\n\nRun K-Means on all features\n\n\nCode\nkmeans_all_layers = KMeans(n_clusters=3, verbose=1).fit(test_layers)\n\n\nInitialization complete\nIteration 0, inertia 156575021617.0.\nIteration 1, inertia 115025038683.5734.\nIteration 2, inertia 112581278537.2893.\nIteration 3, inertia 112197579571.15659.\nIteration 4, inertia 112101897492.05617.\nIteration 5, inertia 112071542003.54475.\nIteration 6, inertia 112060482467.27419.\nIteration 7, inertia 112056114456.59085.\nIteration 8, inertia 112054313068.1214.\nIteration 9, inertia 112053524278.15723.\nIteration 10, inertia 112053178792.97005.\nConverged at iteration 10: center shift 0.13958334970474479 within tolerance 0.2913024911899612.\n\n\nReshape our segmented image, identify which layer is the layer with the roads, and print metrics\n\n\nCode\nsegmented_image_all_layers = kmeans_all_layers.labels_.reshape((test_image.shape[0], test_image.shape[1]))\nroad_cluster_num = identify_road_cluster(segmented_image_all_layers, test_image_label_bool)\nroad_cluster = (segmented_image_all_layers==road_cluster_num)\naccuracy_metrics(test_image_label_bool.ravel(), road_cluster.ravel())\n\n\nConfusion matrix:\n [[  96616   43649]\n [ 888145 1221590]]\nOverall accuracy: 0.586 \nPrecision: 0.098 \nRecall 0.689 \nDICE: 0.172\n\n\n\n\nCode\n# Show cluster and original image\nfig, axes = plt.subplots(1, 2, figsize=(10, 10), sharex=True, sharey=True)\n\naxes[0].imshow(road_cluster, cmap='gray')\naxes[0].set_title('Road Cluster')\n\naxes[1].imshow(test_image)\naxes[1].set_title('Original Image')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nRun K-Means on the PCA filters\n\n\nCode\nkmeans_pca = KMeans(n_clusters=3, verbose=1).fit(test_pca_5_layers)\n\n\nInitialization complete\nIteration 0, inertia 28462864.121999323.\nIteration 1, inertia 22772599.926497176.\nIteration 2, inertia 22365365.74884916.\nIteration 3, inertia 22305926.751492783.\nIteration 4, inertia 22285522.619277447.\nIteration 5, inertia 22274295.569714464.\nIteration 6, inertia 22267485.489484854.\nIteration 7, inertia 22263139.295963943.\nIteration 8, inertia 22260413.28828707.\nIteration 9, inertia 22258742.78713832.\nIteration 10, inertia 22257703.835843332.\nIteration 11, inertia 22257070.42794912.\nConverged at iteration 11: center shift 0.0003099553591653962 within tolerance 0.00041374644497123064.\n\n\n\n\nCode\nsegmented_image_pca = kmeans_pca.labels_.reshape((test_image.shape[0], test_image.shape[1]))\npca_road_cluster_num = identify_road_cluster(segmented_image_pca, test_image_label_bool)\npca_road_cluster = (segmented_image_pca==pca_road_cluster_num)\naccuracy_metrics(test_image_label_bool.ravel(), pca_road_cluster.ravel())\n\n\nConfusion matrix:\n [[  93505   46760]\n [ 414199 1695536]]\nOverall accuracy: 0.795 \nPrecision: 0.184 \nRecall 0.667 \nDICE: 0.289\n\n\n\n\nCode\n# Show cluster and original image\nfig, axes = plt.subplots(1, 2, figsize=(10, 10), sharex=True, sharey=True)\n\naxes[0].imshow(pca_road_cluster, cmap='gray')\naxes[0].set_title('Road Cluster')\n\naxes[1].imshow(test_image)\naxes[1].set_title('Original Image')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nA clear visual difference can be seen with the K-Means clustering run on the PCA top-5 components. The resulting road clusters are more uniform, while the results with using all features were quite grainy. This observation impacts the metrics as well, with PCA layers having similar recall to all layers but better precision and therefore DICE coefficient.\n\n\nRandom Forest Results\n\nInitial RGB Model\nRecall that our initial random forest model uses a small subset of our training image, as displayed earlier. First, we train our model.\n\n\nCode\n# Flatten images\ntrain_small_rgb = small_rgb.reshape(small_rgb.shape[0]*small_rgb.shape[1], 3)\ny_train = small_ans.reshape(small_ans.shape[0]*small_ans.shape[1])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel1 = RF.fit(train_small_rgb, y_train)\n\n# Predictions on training data\nmodel1_pred = model1.predict(train_small_rgb)\n\n# Confusion matrix\naccuracy_metrics(y_train, model1_pred)\n\n\nConfusion matrix:\n [[  8986   4037]\n [  1545 105432]]\nOverall accuracy: 0.953 \nPrecision: 0.853 \nRecall 0.69 \nDICE: 0.763\n\n\nWhile we have a really good overall accuracy rate, we are correctly predicting only 69% of the actual road pixels. With a precision of 0.85, about 85% of our road predictions are actually roads.\n\n\nCode\n# Convert predictions to image\ntrain_preds = model1_pred.reshape(small_ans.shape[0], small_ans.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\n\nskio.imshow(train_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans, ax = ax[1])\nax[1].set_title(\"Actual Solution\");\n\n\n\n\n\n\n\n\n\nVisually, our solution looks alright, but it obviously has room for improvement. Let’s see what our results look like on the testing data.\n\n\nCode\n# Flatten images\ntest_small_rgb = small_rgb_test.reshape(small_rgb_test.shape[0]*small_rgb_test.shape[1], 3)\ny_test = small_ans_test.reshape(small_ans_test.shape[0]*small_ans_test.shape[1])\n\n# Predictions on testing data\nmodel1_test_pred = model1.predict(test_small_rgb)\n\n# Confusion matrix\naccuracy_metrics(y_test, model1_test_pred)\n\n\nConfusion matrix:\n [[   605   6498]\n [  4762 108135]]\nOverall accuracy: 0.906 \nPrecision: 0.113 \nRecall 0.085 \nDICE: 0.097\n\n\nWhile we still have a good overall accuracy rate, our predictions of roads is substantially worse. We have only classified 8.5% of the road pixels correctly, and only 11.3% of our road predictions were actually roads.\n\n\nCode\n# Convert predictions to image\ntest_preds = model1_test_pred.reshape(small_ans_test.shape[0], small_ans_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\n\nskio.imshow(test_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans_test, ax = ax[1])\nax[1].set_title(\"Actual Solution\");\n\n\n\n\n\n\n\n\n\nOur model did NOT generalize well! This looks terrible!\n\n\nInitial Additional Layers Model\nNow, let’s try training on the same region, but incorporating all of the features described above.\n\n\nCode\n# Train model\n\n# Flatten image\ntrain_small_rgb_layers = small_rgb_layers.reshape(small_rgb_layers.shape[0]*small_rgb_layers.shape[1], small_rgb_layers.shape[2])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel2 = RF.fit(train_small_rgb_layers, y_train)\n\n# Predictions on training data\nmodel2_pred = model2.predict(train_small_rgb_layers)\n\n# Confusion matrix\naccuracy_metrics(y_train, model2_pred)\n\n\nConfusion matrix:\n [[ 13023      0]\n [     0 106977]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nNow we have virtually perfect results! Let’s look at an image of the output.\n\n\nCode\n# Convert predictions to image\ntrain_preds = model2_pred.reshape(small_ans.shape[0], small_ans.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\n\nskio.imshow(train_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans, ax = ax[1])\nax[1].set_title(\"Actual Solution\")\n\nskio.imshow(train_preds==small_ans, ax = ax[2])\nax[2].set_title(\"Errors (look closely)\");\n\n\n\n\n\n\n\n\n\nYep, can’t even find the errors without looking closely at the difference between the two images. Let’s evaluate our results on the testing data!\n\n\nCode\n# Create additional features\nsmall_rgb_test_layers = compute_features(small_rgb_test)\n\n# Flatten image\ntest_small_rgb_layers = small_rgb_test_layers.reshape(small_rgb_test_layers.shape[0]*small_rgb_test_layers.shape[1], small_rgb_layers.shape[2])\n\n# Predictions on testing data\nmodel2_test_pred = model2.predict(test_small_rgb_layers)\n\n# Confusion matrix\naccuracy_metrics(y_test, model2_test_pred)\n\n\nConfusion matrix:\n [[   321   6782]\n [  1839 111058]]\nOverall accuracy: 0.928 \nPrecision: 0.149 \nRecall 0.045 \nDICE: 0.069\n\n\n\n\nCode\n# Convert predictions to image\ntest_preds = model2_test_pred.reshape(small_ans_test.shape[0], small_ans_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\n\nskio.imshow(test_preds, ax = ax[0], cmap = \"gray\")\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans_test, ax = ax[1])\nax[1].set_title(\"Actual Solution\")\n\nskio.imshow(test_preds==small_ans_test, ax = ax[2])\nax[2].set_title(\"Errors\");\n\n\n\n\n\n\n\n\n\nAdding these filters to our model had negligible impact on our results. It improved the accuracy from 0.906 to 0.928 and the precision from 0.113 to 0.149, but the recall dropped from 0.085 to 0.045. This means that of the pixels that actually represent roads, we are only correctly classifying 4.7% of them. With perfect results on our training data and pitiful results on our testing data, it appears that incorporating these features in our training data led to severe overfitting! Visually, our results look slightly less random, even though the performance metrics are worse.\n\n\nSampling for Overfitting: RGB\nWe have fed a substantial amount of data, which ought to contain some useful information regarding roads, to our model In our training solution, this data was in fact useful, leading to virtually 100% accuracy. However, on the testing data for both the RGB model and the model with additional layers, our model correctly predicted less than 10% of our roads. Perhaps this means that our model is overfit to our training data. Since the vast majority of pixels in our training data represent non-roads, perhaps our model is overfit to the particularities of the non-road pixels in our training data. One way to address this issue is to randomly select an equal number of pixels of both classes, and then train the model on those pixels. Let’s try randomly picking 5000 road pixels and 5000 non-road pixels for our training data and 5000 of each for our testing data and evaluating our model’s performance.\nFirst, let’s use this method on a model with just RGB layers.\n\n\nCode\n# Flatten training images\ntrain_rgb = rgb.reshape(rgb.shape[0]*rgb.shape[1], 3)\ny_train = ans.reshape(ans.shape[0]*ans.shape[1])\n\n# Subset training data by label\ny_train_true = y_train[y_train]\ny_train_false = y_train[~y_train]\ntrain_rgb_true = train_rgb[y_train]\ntrain_rgb_false = train_rgb[~y_train]\n\n# Sample indices of each label\ntrue_indices = sample_without_replacement(y_train_true.shape[0], 10000)\nfalse_indices = sample_without_replacement(y_train_false.shape[0], 10000)\n\n# Create modified training data\ny_train_mod = np.concatenate([y_train_true[true_indices[:5000]], y_train_false[false_indices[:5000]]])\ntrain_rgb_mod = np.concatenate([train_rgb_true[true_indices[:5000]], train_rgb_false[false_indices[:5000]]])\n\n# Create modified testing data\ny_test_mod = np.concatenate([y_train_true[true_indices[5000:]], y_train_false[false_indices[5000:]]])\ntest_rgb_mod = np.concatenate([train_rgb_true[true_indices[5000:]], train_rgb_false[false_indices[5000:]]])\n\n\n\n\nCode\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel3 = RF.fit(train_rgb_mod, y_train_mod)\n\n# Predictions on training data\nmodel3_pred = model3.predict(train_rgb_mod)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model3_pred)\n\n\nConfusion matrix:\n [[4960   40]\n [  73 4927]]\nOverall accuracy: 0.989 \nPrecision: 0.985 \nRecall 0.992 \nDICE: 0.989\n\n\nWhile this model does not have 100% training accuracy like the additional layers model, it has improved significantly over the original RGB model. Most notably, the training recall has improved from less than 70% to roughly 99%. Let’s see if we maintain this performance when we make predictions on our testing data.\n\n\nCode\n# Predictions on testing data\nmodel3_test_pred = model3.predict(test_rgb_mod)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model3_test_pred)\n\n\nConfusion matrix:\n [[3794 1206]\n [1267 3733]]\nOverall accuracy: 0.753 \nPrecision: 0.75 \nRecall 0.759 \nDICE: 0.754\n\n\nOur results are encouraging! Our overall accuracy, precision, and recall are all approximatly 0.75. In the original RGB model, the overall accuracy was over 90%, while the precision and recall were roughly 10%. By balancing the amount of training data in each class, we were able to balance the different accuracy metrics, improving our predictions of roads at the expense of our predictions of non-roads. Perhaps if we incorporate our additional layers into the model, these balance improvements will translate to balanced and higher accuracy metrics.\nWhile our results above are encouraging, our training and testing data were both drawn from the same image, so our model may have overtrained to this image. Let’s form predictions and compute accuracy metrics on a different image.\n\n\nCode\n# Flatten testing images\nflat_rgb_test = rgb_test.reshape(rgb_test.shape[0]*rgb_test.shape[1], 3)\ny_test = ans_test.reshape(ans_test.shape[0]*ans_test.shape[1])\n\n# Predictions on testing data\nmodel3_test_pred_2 = model3.predict(flat_rgb_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model3_test_pred_2)\n\n\nConfusion matrix:\n [[ 100346   39919]\n [ 344802 1764933]]\nOverall accuracy: 0.829 \nPrecision: 0.225 \nRecall 0.715 \nDICE: 0.343\n\n\nSurprisingly, the overall accuracy is higher in the testing image than in the training image! The recall is still over 70%, indicating that we are capturing most pixels representing roads correctly. With a much lower precision, we must be predicting road pixels frequently where there are not actually roads.\nSince we are working with an entire image, we can inspect our results!\n\n\nCode\n# Convert predictions to image\ntest_preds = model3_test_pred_2.reshape(rgb_test.shape[0], rgb_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\nIt looks like we tend to label pixels as roads when they in reality represent other human features like buildings. We also exaggerate the width of some roads.\n\n\nSampling for Overfitting: Additional Layers\nNow let’s try the same sampling technique but after producing all of our features.\n\n\nCode\n# Create additional features\ntrain_rgb_mod_layers = compute_features(rgb)\n\n# Flatten training image with extra layers\ntrain_rgb_2 = train_rgb_mod_layers.reshape(train_rgb_mod_layers.shape[0]*train_rgb_mod_layers.shape[1], train_rgb_mod_layers.shape[2])\n\n# Subset training data by label\ntrain_rgb_true_2 = train_rgb_2[y_train]\ntrain_rgb_false_2 = train_rgb_2[~y_train]\n\n# Create modified training data\ntrain_rgb_mod_2 = np.concatenate([train_rgb_true_2[true_indices[:5000]], train_rgb_false_2[false_indices[:5000]]])\n\n# Create modified testing data\ntest_rgb_mod_2 = np.concatenate([train_rgb_true_2[true_indices[5000:]], train_rgb_false_2[false_indices[5000:]]])\n\n\n\n\nCode\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel4 = RF.fit(train_rgb_mod_2, y_train_mod)\n\n# Predictions on training data\nmodel4_pred = model4.predict(train_rgb_mod_2)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model4_pred)\n\n\nConfusion matrix:\n [[5000    0]\n [   0 5000]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nOur training results are literally perfect. Does this translate to our testing data?\n\n\nCode\n# Predictions on testing data\nmodel4_test_pred = model4.predict(test_rgb_mod_2)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model4_test_pred)\n\n\nConfusion matrix:\n [[4102  898]\n [1062 3938]]\nOverall accuracy: 0.804 \nPrecision: 0.794 \nRecall 0.82 \nDICE: 0.807\n\n\nIt appears that there were some errors on our testing data. Going from the RGB model to the additional layers model, our overall accuracy improved from 0.755 to 0.812, the precision improved from 0.749 to 0.796, and the recall improved from 0.767 to 0.84. These are the most accurate road predictions yet!\nWhile our training and testing data contained none of the same pixels, they were both drawn from the same image, so it is possible that they were overtrained to our particular image of choice. Perhaps a more valid testing metric would involve testing our model on pixels from a different image. Let’s form predictions and compute accuracy metrics on the entirety of another image.\n\n\nCode\n# Create additional features\ntest_rgb_layers_3 = compute_features(rgb_test)\n\n# Flatten testing images\ntest_rgb_3 = test_rgb_layers_3.reshape(test_rgb_layers_3.shape[0]*test_rgb_layers_3.shape[1], test_rgb_layers_3.shape[2])\n\n\n\n\nCode\n# Predictions on testing data\nmodel4_test_pred_2 = model4.predict(test_rgb_3)\n\n# Confusion matrix\naccuracy_metrics(y_test, model4_test_pred_2)\n\n\nConfusion matrix:\n [[  92296   47969]\n [ 249036 1860699]]\nOverall accuracy: 0.868 \nPrecision: 0.27 \nRecall 0.658 \nDICE: 0.383\n\n\nSimilar to the RGB model, the results on the testing image were largely similar to the results on the previous image, except for the precision dropping by over 50% The overall accuracy is over 85%, but the recall is now 65.9% and the precision is now 26.6%. While this is certainly not perfect, the precision and recall are still a substantial improvement over the models without sampling. However, the recall was actually slightly higher in the sampled RGB model, indicating that the RGB model generalized better in terms of predicting road pixels. Perhaps there are tactics we can use to combat overfitting.\nAlso, since we are now working with a complete image, we can once again inspect a full image illustrating our predictions versus the truth.\n\n\nCode\n# Convert predictions to image\ntest_preds = model4_test_pred_2.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\nOnce again, our model tends to incorrectly predict roads where there are other human features like buildings, and it exaggerates the width of some roads.\n\n\nPCA for Overfitting\nIt appears that our additional layers model with sampled training data is overfit to our training data, as we have excellent performance on the training data but subpar performance on the new image. Perhaps our model suffers from overfitting because we have constructed so many features, many of which are similar to one another. To help combat this issue, we fit our model again below, but after applying Principal Component Analysis and selecting the most important components.\nFirst, we apply Principal Component Analysis to our training data and plot the percent variance explained by each component. Note that PCA is only applicable for continuous features, so we cannot include binary features such as canny edges in this model.\n\n\nCode\n# Add layers to model\ntrain_pca_layers = compute_features(rgb, include_categorical = False)   \n\n# Flatten training image with extra layers\ntrain_pca_layers_flat = train_pca_layers.reshape(train_pca_layers.shape[0]*train_pca_layers.shape[1], train_pca_layers.shape[2])\n\n# Standardize the features\nscaler = StandardScaler()\ntrain_pca_layers_scaled = scaler.fit_transform(train_pca_layers_flat)\n\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=train_pca_layers.shape[2])\nlayers_pca = pca.fit_transform(train_pca_layers_scaled)\n\n# Explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Plotting the explained variance ratio\nplt.figure(figsize=(8, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\n\nApparently, over 50% of the variance in our data can be explained by the first component! The second component only accounts for about 11.5% of the variance in the data, and the numbers continue to drop after that.\n\n\nCode\nexplained_variance_ratio[0:5].sum()\n\n\n0.843420909342875\n\n\nThe first 5 components account for over 84% of the variation in our data. Let’s try only retaining the first 5 components for our model and seeing whether our performance improves.\n\n\nCode\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=5)\nlayers_pca = pca.fit_transform(train_pca_layers_scaled)\n\n# Subset training data by label\nlayers_pca_true = layers_pca[y_train]\nlayers_pca_false = layers_pca[~y_train]\n\n# Create modified training data\nlayers_pca_mod_train = np.concatenate([layers_pca_true[true_indices[:5000]], layers_pca_false[false_indices[:5000]]])\n\n# Create modified testing data\nlayers_pca_mod_test = np.concatenate([layers_pca_true[true_indices[5000:]], layers_pca_false[false_indices[5000:]]])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel5 = RF.fit(layers_pca_mod_train, y_train_mod)\n\n# Predictions on training data\nmodel5_pred = model5.predict(layers_pca_mod_train)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model5_pred)\n\n\nConfusion matrix:\n [[4999    1]\n [   0 5000]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nPer usual with our additional layers, our model’s performance is virtually perfect on the training data. Let’s take a look at the testing performance.\n\n\nCode\n# Predictions on testing data\nmodel5_test_pred = model5.predict(layers_pca_mod_test)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model5_test_pred)\n\n\nConfusion matrix:\n [[3820 1180]\n [1379 3621]]\nOverall accuracy: 0.744 \nPrecision: 0.735 \nRecall 0.764 \nDICE: 0.749\n\n\nOn the testing data, all of our model’s performance metrics are lower than its non-PCA counterpart, although not by that much. Let’s test on an entirely new image.\n\n\nCode\n# Add layers to model\ntest_pca_layers = compute_features(rgb_test, include_categorical = False)\n\n# Flatten testing images\ntest_pca_layers_flat = test_pca_layers.reshape(test_pca_layers.shape[0]*test_pca_layers.shape[1], 22)\n\n# Standardize the features\ntest_pca_layers_scaled = scaler.fit_transform(test_pca_layers_flat)\n\n# Project onto principal components\nlayers_pca_test = pca.transform(test_pca_layers_scaled)\n\n\n\n\nCode\n# Predictions on testing data\nmodel5_test_pred_2 = model5.predict(layers_pca_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model5_test_pred_2)\n\n\nConfusion matrix:\n [[  75268   64997]\n [ 536637 1573098]]\nOverall accuracy: 0.733 \nPrecision: 0.123 \nRecall 0.537 \nDICE: 0.2\n\n\nAgain, on the testing image, all of our model’s performance metrics are lower than its non-PCA counterpart. In this scenario, it appears that the components explaining very little variation in the data were actually somewhat useful for predictions. Note that we tried this with a variety of number of retained components, and we found that the model’s performance improved as we increased the number of components.\nBelow, we inspect the image of our predictions.\n\n\nCode\n# Convert predictions to image\ntest_preds = model5_test_pred_2.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\nVisually, our predictions appear somewhat worse that those from our non-PCA additional layers model. There is overall a lot more noise in our predictions, and interestingly, we are not predicting a road in much of the massive highway.\n\n\nSampling from Multiple Images: RGB\nPCA did not help us generalize to new testing data, but perhaps there is another approach we could take. Earlier, we created our training data by sampling from a single image. Perhaps we could sample from multiple images, mitigating bias from working with a single image. Below, we select 10 images and sample from them to train our model.\n\n\nCode\n# Store id's of images\nimgs = [\"10828735_15\", \"10228675_15\", \"10228705_15\", \"10228720_15\", \"10228735_15\", \"10528675_15\", \"10528750_15\", \"10978720_15\", \"11128825_15\", \"12028750_15\"]\n\n# Initialize arrays to store training data\ny_train = np.array([])\nrgb_train = np.zeros((0,3))\n\n# Sample from each image\nfor img in imgs:\n    rgb = skio.imread(\"../data/MA_roads/tiff/train/\" + img + \".tiff\")\n    ans = skio.imread(\"../data/MA_roads/tiff/train_labels/\" + img + \".tif\") &gt; 0\n    \n    # Flatten training images\n    rgb_flat = rgb.reshape(rgb.shape[0]*rgb.shape[1], 3)\n    ans_flat = ans.reshape(ans.shape[0]*ans.shape[1])\n    \n    # Subset training data by label\n    ans_true = ans_flat[ans_flat]\n    ans_false = ans_flat[~ans_flat]\n    rgb_true = rgb_flat[ans_flat]\n    rgb_false = rgb_flat[~ans_flat]\n    \n    # Sample indices of each label\n    true_indices = sample_without_replacement(ans_true.shape[0], 5000)\n    false_indices = sample_without_replacement(ans_false.shape[0], 5000)\n    \n    # Create modified training data\n    y_train = np.concatenate([y_train, ans_true[true_indices], ans_false[false_indices]])\n    rgb_train = np.concatenate([rgb_train, rgb_true[true_indices], rgb_false[false_indices]])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel6 = RF.fit(rgb_train, y_train)\n\n# Predictions on training data\nmodel6_pred = model6.predict(rgb_train)\n\n# Confusion matrix\naccuracy_metrics(y_train, model6_pred)\n\n\nConfusion matrix:\n [[48194  1806]\n [ 2559 47441]]\nOverall accuracy: 0.956 \nPrecision: 0.95 \nRecall 0.964 \nDICE: 0.957\n\n\nThese metrics are actually somewhat lower than the training metrics for the RGB model sampled from a single image. But our true question is whether the model generalizes better to the testing data – more specifically, to our new testing image.\n\n\nCode\n# Predictions on testing data\nmodel6_test_pred = model6.predict(flat_rgb_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model6_test_pred)\n\n\nConfusion matrix:\n [[ 110650   29615]\n [ 384361 1725374]]\nOverall accuracy: 0.816 \nPrecision: 0.224 \nRecall 0.789 \nDICE: 0.348\n\n\nIn the RGB model trained on sampling data from one image, our results were as follows.\n\nOverall accuracy: 0.829\nPrecision: 0.225\nRecall 0.715\nDICE: 0.343\n\nThis model has slightly decreased in all metrics except for the recall. Considering the additional computational power required to train this model, it does not appear to be advantageous over the other model.\n\n\nCode\n# Convert predictions to image\ntest_preds = model6_test_pred.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\nThe output image visually looks quite similar to that when trained on data sampled from a single image.\n\n\nSampling from Multiple Images: Additional Layers\nFirst, we train our model.\n\n\nCode\n# Store id's of images\nimgs = [\"10828735_15\", \"10228675_15\", \"10228705_15\", \"10228720_15\", \"10228735_15\", \"10528675_15\", \"10528750_15\", \"10978720_15\", \"11128825_15\", \"12028750_15\"]\n\n# Initialize arrays to store training data\ny_train = np.array([])\nrgb_train = np.zeros((0,27))\n\n# Sample from each image\nfor img in imgs:\n    rgb = skio.imread(\"../data/MA_roads/tiff/train/\" + img + \".tiff\")\n    ans = skio.imread(\"../data/MA_roads/tiff/train_labels/\" + img + \".tif\") &gt; 0\n\n    # Create additional layers\n    rgb_layers = compute_features(rgb)\n    \n    # Flatten training images\n    rgb_flat = rgb_layers.reshape(rgb_layers.shape[0]*rgb_layers.shape[1], rgb_layers.shape[2])\n    ans_flat = ans.reshape(ans.shape[0]*ans.shape[1])\n    \n    # Subset training data by label\n    ans_true = ans_flat[ans_flat]\n    ans_false = ans_flat[~ans_flat]\n    rgb_true = rgb_flat[ans_flat]\n    rgb_false = rgb_flat[~ans_flat]\n    \n    # Sample indices of each label\n    true_indices = sample_without_replacement(ans_true.shape[0], 5000)\n    false_indices = sample_without_replacement(ans_false.shape[0], 5000)\n    \n    # Create modified training data\n    y_train = np.concatenate([y_train, ans_true[true_indices], ans_false[false_indices]])\n    rgb_train = np.concatenate([rgb_train, rgb_true[true_indices], rgb_false[false_indices]])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel7 = RF.fit(rgb_train, y_train)\n\n# Predictions on training data\nmodel7_pred = model7.predict(rgb_train)\n\n# Confusion matrix\naccuracy_metrics(y_train, model7_pred)\n\n\nConfusion matrix:\n [[49999     1]\n [    1 49999]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nPer usual when working with all of our layers, we have virtually perfect results on our training data. Next, we test the model on our usual testing image.\n\n\nCode\n# Predictions on testing data\nmodel7_test_pred = model7.predict(test_rgb_3)\n\n# Confusion matrix\naccuracy_metrics(y_test, model7_test_pred)\n\n\nConfusion matrix:\n [[ 115371   24894]\n [ 321743 1787992]]\nOverall accuracy: 0.846 \nPrecision: 0.264 \nRecall 0.823 \nDICE: 0.4\n\n\nThis is the best performance we have seen so far. The biggest difference between this model and the additional layers model with training data sampled from a single image is that the recall has increased from 0.658 to 0.823.\nFinally, we visually inspect our output.\n\n\nCode\n# Convert predictions to image\ntest_preds = model7_test_pred.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\nThis is definitely the cleanest output we have seen so far, which is reflective of our model’s high recall value. The model continues to erroneously label human features that are not roads as roads, and it has labelled virtually the entire highway as road instead of just its centerlines. Differentiating between the centerline and the road surface seems to be a very difficult problem.\n\n\nComparison of Random Forest Results\nWe fit a lot of random forest models, so we summarized their accuracy metrics in the following table. Note that our testing metrics listed here are the results when we tested on an entirely new image.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nRGB\nMore Layers\nRGB Sampled\nMore Layers Sampled\nPCA Sampled\nRGB Multiple Images\nMore Layers Multiple Images\n\n\n\n\nTraining Accuracy\n0.953\n1.0\n0.989\n1.0\n1.0\n0.956\n1.0\n\n\nTraining Precision\n0.853\n1.0\n0.985\n1.0\n1.0\n0.95\n1.0\n\n\nTraining Recall\n0.69\n1.0\n0.992\n1.0\n1.0\n0.964\n1.0\n\n\nTraining Dice\n0.763\n1.0\n0.989\n1.0\n1.0\n0.957\n1.0\n\n\nTesting Accuracy\n0.906\n0.928\n0.829\n0.868\n0.733\n0.816\n0.846\n\n\nTesting Precision\n0.113\n0.149\n0.225\n0.27\n0.123\n0.224\n0.264\n\n\nTesting Recall\n0.085\n0.045\n0.715\n0.658\n0.537\n0.789\n0.823\n\n\nTesting Dice\n0.097\n0.069\n0.343\n0.383\n0.2\n0.348\n0.4\n\n\n\nFor a quick summary of each model’s performance, we look at the testing Dice score. We noticed a major bump in performance when we started sampling equal portions of training data, with the dice coefficient of both the RGB and additional layers models jumping from less than 0.1 to above 0.3. We attempted to account for overfitting by introducing PCA, but this cut our Dice coefficient in half. Finally, creating our training data by sampling from multiple images resulted in the highest performance, particularly in the additional layers model, where the Dice coefficient jumped to 0.4. Our final model was our best performing random forest model by every metric except testing accuracy.\n\n\n\nU-Net Results\nBecause of the high computational demans of the U-Net model, we did not re-run the model in this notebook. For full documentation of our code, please see our GitHub repository.\nWe managed to get solid predictions for the small 128 x 128 images. The precision got up to 0.445, the recall went to 0.938, and the dice coefficient was 0.604. We wanted to compare our results with the previous models, so we ran our predictions on the test set (also divided into 128 x 128 pixel images), and reconstructed the 128 x 128 masks to get 1408 x 1408 masks which resemble the original image. These final predictions were less accurate than the original 128 x 128 masks with a precision metric of 0.06, a recall of 0.197, and a dice coefficient of 0.092. Individually the predictions made by the U-Net were pretty accurate, but when reconstructed to the greater original image size, the cumulative prediction must have lost enough information to lower its accuracy metrics.\n\n\nOverall Result Comparison\nWhich method worked best: k-means, random forest, or U-Net? We include the following table to facilitate comparison of results. We trained many different models in the preceding sections, but we only report the best performing model for each method in the table below.\n\n\n\nModel\nK-Means\nRandom Forest\nU-Net\n\n\n\n\nTesting Precision\n0.187\n0.264\n0.06\n\n\nTesting Recall\n0.661\n0.823\n0.197\n\n\nTesting Dice\n0.291\n0.4\n0.092\n\n\n\nEssentially, with increasing complexity of the algorithm and computational requirements came increased performance. K-Means clustering required the least computational power but also achieved the worst results, achieving a Dice coefficient of 0.291 after we reduced the number of features using PCA. Random forest required more computational power, especially when we began sampling training data from multiple images, achieving an improved Dice score of 0.4. Finally, our U-Net model achieved even better results when we considered small subsets of our testing images with a dice score of 0.604, but poor results when applied to the entire image with a Dice score of 0.092. The U-Net models in the literature and Kaggle datasets have achieved by far the best results. Unfortunately, achieving such high performance with U-Net requires substantial computational power for model training. Overall, we conclude that U-Net can achieve the best performance when appropriate resources are available. In the absence of high performance computing capabilities, random forest may suffice."
  },
  {
    "objectID": "index.html#discussion",
    "href": "index.html#discussion",
    "title": "Machine Learning for Road Segmentation",
    "section": "Discussion",
    "text": "Discussion\nWe had mixed results with the effectiveness of PCA. For K-Means Clustering, using the top-5 components helped the model perform better but for Random Forest, using PCA did not improve the results. PCA was not applicable to deep neural networks so we cannot make a firm conclusion on whether or not PCA is an important tool for segmenting road with satellite images.\nADD MORE\nAlthogh U-Net was very expensive computationally and took the most time to run, we conclude that it is the best approach of the three we tested. This methodology will often be applied to very large datasets and will had high computational requirements just because of the amount of data. Also high precision and recall are very important if the benefits from segmenting roads is to be maximized."
  },
  {
    "objectID": "index.html#accessibility",
    "href": "index.html#accessibility",
    "title": "Machine Learning for Road Segmentation",
    "section": "Accessibility",
    "text": "Accessibility\nDue the high requirements to run these models, especially U-Net, and the large amount of data needed, these methods are not very accessible to many individuals. Luckily this is not a task that many individuals would look to perform but more a task that would be performed by large organization like governments. Considering accessibility at this scale brings in the question of access to large datasets and high performance computers, as many countries may not have access to such resources and therefore would not be able to implement these methods on a large scale.\nThere are some positive accessibility points with these algorithms as well, being that no one has to physically be in the place where you hope to segment roads. This means that in areas with difficult to access roads, one can still segment the road network. Some examples of when this could be extremely helpful are climate disasters and war time aid. With climate disasters, roads may be destroyed like during the floods in Vermont last summer. Knowing what roads were destroyed without having to go out into the field where danger from flood is still high could be extremely helpful for disaster response. In the example of war, especially in places that are being heavily bombed, it may be hard to get humanitarian aid workers into certain areas due to the destruction of roads. These algorithms could help assess with roads are still open without having to put anyone on the field and in danger.\nThere are some other accessibility points such as ability to interpret the image segmentation. In an area where the visual interpretation is essential, making sure color blind inclusive colors are used is very important."
  },
  {
    "objectID": "index.html#ethical-considerations",
    "href": "index.html#ethical-considerations",
    "title": "Machine Learning for Road Segmentation",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\nAs mentioned above, some countries and organizations may not have the resources to train large models like U-Net. Resources like data centers and HPCs are essential and many places may not have access to them. Looking deeper, if a country like the US trains a UNet model for segmenting roads on a dataset including road in the US, that model could be used by other countries but it may be much less accurate if there road infrastructure looks different than that in the US. Furthermore, if large labeled datasets of roads only exist with roads from developed countries, this would hinder any country that may have less built infrastructure as the training data will not align with the data being input into the model.\nFurther ethical considerations around data across fields is the exploitative nature in which many labeled datasets are created. Large tech companies often out-source their data labeling overseas and pay people very little for the data that makes all this possible. They make huge profits off of the models that are only possible with this labeled data and the associated labor.\nGoing into remote sensing and the high resolution of satellite images, privacy concerns may be raised. These satellite images will contain sections consisting of private problems and invasion of personal privacy should always be a consideration when using remote sensing on inhabited areas."
  },
  {
    "objectID": "index.html#reflection",
    "href": "index.html#reflection",
    "title": "Machine Learning for Road Segmentation",
    "section": "Reflection",
    "text": "Reflection\n\nSchedule and Obstacles\nWe each stayed on schedule for our tasks in general, but some tasks were much more difficult than expected and we ended up deciding we would save them for future work. Some tasks just ended up being more time consuming, like training the U-Net. Lots of time was spent waiting for a model to train just have an error or horrible predictions. Looking back, we should have learned how to connect to Middlebury’s ADA cluster to train our model. We ended up ditching LiDAR data because it was more difficult to process than we realized and would put an even heavily burden on the amount of training data we were using. We also felt like we already had a plethora of topics to discuss and compare, so it wasn’t necessary to add even another aspect to the project.\n\n\nFuture Work\nFuture work we are interested in pursuing is of course integrating LiDAR data into each of our models and comparing the metrics on how well roads were segmented. We are very curious to see if this additional data would help our models and are interested in learning about LiDAR data. Another path of future work is performing more data engineering and augmentation on our training data for input into U-Net. We know from reading and seeing models trained on Kaggle that very accurate segmentations are possible and we had a lot of room for improvement with our model."
  }
]